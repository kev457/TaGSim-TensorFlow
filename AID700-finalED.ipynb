{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59692463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers,losses,layers\n",
    "import pickle\n",
    "from glob import glob\n",
    "from os.path import basename\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1725e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'AIDS700nef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7dd1a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_generated_graphs(dataset_name, file_name='generated_graph_500'):\n",
    "\n",
    "    dir = './dataset/' + dataset_name + '/' + file_name\n",
    "    g = open(dir, 'rb')\n",
    "    generated_graphs = pickle.load(g)\n",
    "    g.close()\n",
    "    return generated_graphs\n",
    "\n",
    "\n",
    "\n",
    "training_pairs = load_generated_graphs(dataset, file_name='generated_graph_pairs')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1131c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_nicely(l):\n",
    "    def tryint(s):\n",
    "        try:\n",
    "            return int(s)\n",
    "        except:\n",
    "            return s\n",
    "\n",
    "    import re\n",
    "    def alphanum_key(s):\n",
    "        return [tryint(c) for c in re.split('([0-9]+)', s)]\n",
    "\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "def load_graphs(dataset_name, train_or_test='train'):\n",
    "    graphs = []\n",
    "    dir = './dataset/' + dataset_name + '/' + train_or_test\n",
    "    for file in sorted_nicely(glob(dir + '/*.gexf')):\n",
    "        gid = int(basename(file).split('.')[0])\n",
    "        g = nx.read_gexf(file)\n",
    "        g.graph['gid'] = gid\n",
    "        graphs.append(g)\n",
    "        if not nx.is_connected(g):\n",
    "            raise RuntimeError('{} not connected'.format(gid))\n",
    "    return graphs\n",
    "\n",
    "training_graphs = load_graphs(dataset, train_or_test='train')\n",
    "testing_graphs = load_graphs(dataset, train_or_test='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d7313f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_graphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtraining_graphs\u001b[49m[\u001b[38;5;241m1\u001b[39m],testing_graphs[\u001b[38;5;241m1\u001b[39m],training_pairs[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_graphs' is not defined"
     ]
    }
   ],
   "source": [
    "print(training_graphs[1],testing_graphs[1],training_pairs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f92d0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorNetworkModule(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.setup_weights()\n",
    "\n",
    "    def setup_weights(self):\n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "        self.weight_matrix = initializer(shape = (self.input_dim, self.input_dim, 16), dtype = tf.float32)\n",
    "\n",
    "        self.weight_matrix_block = initializer(shape= (16, 2*self.input_dim), dtype = tf.float32)\n",
    "\n",
    "        self.bias = initializer(shape = (16, 1), dtype = tf.float32)\n",
    "\n",
    "\n",
    "    def call(self, embedding_1, embedding_2):\n",
    "\n",
    "    \n",
    "        \n",
    "        scoring = tf.matmul(tf.transpose(embedding_1, perm=[1,0]), self.weight_matrix.reshape(self.input_dim, -1))\n",
    "        scoring = scoring.reshape(self.input_dim, 16)\n",
    "        scoring = tf.matmul(tf.transpose(scoring, perm=[1,0]), embedding_2)\n",
    "        combined_representation = tf.concat((embedding_1,embedding_2), axis=0)\n",
    "        block_scoring = tf.matmul(self.weight_matrix_block, combined_representation)\n",
    "        scores = tf.nn.relu(scoring + block_scoring + self.bias)\n",
    "        scores = tf.transpose(scores, perm=[1,0])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd70dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAggregationLayer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,in_features=10, out_features=10):\n",
    "        super(GraphAggregationLayer, self).__init__()\n",
    "\n",
    "    def call(self, input, adj):\n",
    "        h_prime = tf.matmul(adj, input)\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e276c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    g = open('./dataset/' + dataset + '/global_labels', 'rb')\n",
    "    global_labels = pickle.load(g)\n",
    "    g.close()\n",
    "    number_of_labels = len(global_labels)\n",
    "    self.input_dim = 2* number_of_labels\n",
    "    \n",
    "    self.gal1 = GraphAggregationLayer()\n",
    "    self.gal2 = GraphAggregationLayer()\n",
    "    self.tensor_network_nc = TensorNetworkModule( 2*number_of_labels)\n",
    "    self.tensor_network_in = TensorNetworkModule(2*number_of_labels)\n",
    "    self.tensor_network_ie = TensorNetworkModule(2*number_of_labels)\n",
    "    \n",
    "    self.input_nc = tf.keras.Input(shape=(16,))\n",
    "    self.fully_connected_first_nc = tf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.fully_connected_second_nc = tf.keras.layers.Dense(8, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.fully_connected_third_nc = tf.keras.layers.Dense(4, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.scoring_layer_nc = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    self.scoring_layer_nc = keras.Sequential([\n",
    "        self.input_nc, \n",
    "        self.fully_connected_first_nc,\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        self.fully_connected_second_nc,\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        self.fully_connected_third_nc,\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        self.scoring_layer_nc])\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.input_in = tf.keras.Input(shape=(16,) )\n",
    "    self.fully_connected_first_in = tf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.fully_connected_second_in = tf.keras.layers.Dense(8, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.fully_connected_third_in = tf.keras.layers.Dense(4, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.scoring_layer_in = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    self.scoring_layer_in = keras.Sequential([\n",
    "        self.input_in, ##feature_count = tensor_neurons = 16\n",
    "        self.fully_connected_first_in,\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        self.fully_connected_second_in,\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        self.fully_connected_third_in,\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        self.scoring_layer_in])\n",
    "    \n",
    "    self.input_ie = tf.keras.Input(shape=(16,))\n",
    "    self.fully_connected_first_ie = tf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.fully_connected_second_ie = tf.keras.layers.Dense(8, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.fully_connected_third_ie = tf.keras.layers.Dense(4, activation=\"relu\", kernel_regularizer='l2')\n",
    "    self.scoring_layer_ie = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    self.scoring_layer_ie = keras.Sequential([\n",
    "        self.input_ie, ##feature_count = tensor_neurons = 16\n",
    "        self.fully_connected_first_ie,\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        self.fully_connected_second_ie,\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        self.fully_connected_third_ie,\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        self.scoring_layer_ie])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "  def gal_pass(self, edge_index, features):\n",
    "\n",
    "    hidden1 = self.gal1(features, edge_index)\n",
    "    hidden2 = self.gal2(hidden1, edge_index)\n",
    "    \n",
    "    return hidden1, hidden2\n",
    "\n",
    "  def call(self, data):\n",
    "    \n",
    "    adj_1 = tf.constant(np.array(data[\"edge_index_1\"].todense()),dtype=tf.float32)\n",
    "    adj_2 = tf.constant(np.array(data[\"edge_index_2\"].todense()),dtype=tf.float32)\n",
    "\n",
    "    features_1, features_2 = data[\"features_1\"], data[\"features_2\"]\n",
    "    \n",
    "    #GAL \n",
    "    graph1_hidden1, graph1_hidden2 = self.gal_pass(adj_1, features_1)#\n",
    "    graph2_hidden1, graph2_hidden2 = self.gal_pass(adj_2, features_2)#\n",
    "    \n",
    "\n",
    "\n",
    "    graph1_01concat = tf.concat([features_1, graph1_hidden1], axis=1)\n",
    "    graph2_01concat = tf.concat([features_2, graph2_hidden1], axis=1)\n",
    "    graph1_12concat = tf.concat([graph1_hidden1, graph1_hidden2], axis=1)\n",
    "    graph2_12concat = tf.concat([graph2_hidden1, graph2_hidden2], axis=1)\n",
    "\n",
    "    graph1_01pooled = tf.expand_dims(tf.reduce_sum(graph1_01concat,axis=0),1)\n",
    "    graph1_12pooled = tf.expand_dims(tf.reduce_sum(graph1_12concat,axis=0),1)\n",
    "    graph2_01pooled = tf.expand_dims(tf.reduce_sum(graph2_01concat,axis=0),1)\n",
    "    graph2_12pooled = tf.expand_dims(tf.reduce_sum(graph2_12concat,axis=0),1)\n",
    "    \n",
    "    \n",
    "\n",
    "    scores_nc = self.tensor_network_nc(graph1_01pooled, graph2_01pooled)\n",
    "    scores_nc = self.scoring_layer_nc(scores_nc)\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    scores_in = self.tensor_network_in(graph1_01pooled, graph2_01pooled)\n",
    "    scores_in = self.scoring_layer_in(scores_in)\n",
    "\n",
    "    scores_ie = self.tensor_network_ie(graph1_12pooled, graph2_12pooled)\n",
    "\n",
    "\n",
    "    \n",
    "    scores_ie = self.scoring_layer_ie(scores_ie)\n",
    "    \n",
    "   #\n",
    "    #print(\"score_ie: \\n\", scores_ie)\n",
    "    return(tf.concat([scores_nc, scores_in, scores_ie], axis=1))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cffb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "def transfer_to_torch(data, type_specified=True):\n",
    "        g = open('./dataset/' + dataset + '/global_labels', 'rb')\n",
    "        global_labels = pickle.load(g)\n",
    "        g.close()\n",
    "        new_data = dict()\n",
    "        graph1, graph2 = data['graph_pair'][0], data['graph_pair'][1]\n",
    "        nodes1, nodes2 = list(graph1.nodes()), list(graph2.nodes())\n",
    "\n",
    "        features_1, features_2 = [], []\n",
    "\n",
    "        for n in graph1.nodes():\n",
    "            features_1.append([1.0 if global_labels[graph1.nodes()[n]['type']] == i else 0.0 for i in global_labels.values()])\n",
    "\n",
    "        for n in graph2.nodes():\n",
    "            features_2.append([1.0 if global_labels[graph2.nodes()[n]['type']] == i else 0.0 for i in global_labels.values()])\n",
    "\n",
    "        features_1, features_2 = tf.constant(np.array(features_1),dtype = tf.float32), tf.constant(np.array(features_2),dtype = tf.float32)\n",
    "\n",
    "        new_data[\"edge_index_1\"], new_data[\"edge_index_2\"] = nx.adjacency_matrix(graph1), nx.adjacency_matrix(graph2)\n",
    "        new_data[\"features_1\"], new_data[\"features_2\"] = features_1, features_2\n",
    "\n",
    "        if(type_specified):\n",
    "            norm_ged = [data['ged'][key] / (0.5 * (graph1.number_of_nodes() + graph2.number_of_nodes())) for key in ['nc', 'in', 'ie']]\n",
    "            norm_ged = np.array(norm_ged)\n",
    "            new_data[\"target\"] = tf.cast(tf.convert_to_tensor(np.exp(-norm_ged)).reshape(1, -1), tf.float32)\n",
    "            \n",
    "            norm_gt_ged = (data['ged']['nc'] + data['ged']['in'] + data['ged']['ie']) / (0.5 * (graph1.number_of_nodes() + graph2.number_of_nodes()))\n",
    "            new_data[\"gt_ged\"] = tf.cast(tf.convert_to_tensor(np.exp(-norm_gt_ged).reshape(1, 1)).reshape(1, -1),tf.float32)\n",
    "        else:\n",
    "            norm_gt_ged = data['ged'] / (0.5 * (graph1.number_of_nodes() + graph2.number_of_nodes()))\n",
    "            new_data[\"gt_ged\"] = tf.cast(tf.convert_to_tensor(np.exp(-norm_gt_ged).reshape(1, 1)).reshape(1, -1),tf.float32)\n",
    "\n",
    "        return new_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c73c9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\AppData\\Local\\Temp\\ipykernel_13156\\157768131.py:21: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  new_data[\"edge_index_1\"], new_data[\"edge_index_2\"] = nx.adjacency_matrix(graph1), nx.adjacency_matrix(graph2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 loss:  0.12144497036933899\n",
      "Iteration:  1 loss:  0.10983861982822418\n",
      "Iteration:  2 loss:  0.09103536605834961\n",
      "Iteration:  3 loss:  0.09026333689689636\n",
      "Iteration:  4 loss:  0.07439111173152924\n",
      "Iteration:  5 loss:  0.07091686874628067\n",
      "Iteration:  6 loss:  0.06940209865570068\n",
      "Iteration:  7 loss:  0.062353745102882385\n",
      "Iteration:  8 loss:  0.04948916286230087\n",
      "Iteration:  9 loss:  0.05520903691649437\n",
      "Iteration:  10 loss:  0.05045954883098602\n",
      "Iteration:  11 loss:  0.042805589735507965\n",
      "Iteration:  12 loss:  0.046508584171533585\n",
      "Iteration:  13 loss:  0.041993074119091034\n",
      "Iteration:  14 loss:  0.036387309432029724\n",
      "Iteration:  15 loss:  0.03842842951416969\n",
      "Iteration:  16 loss:  0.03589348867535591\n",
      "Iteration:  17 loss:  0.03868696838617325\n",
      "Iteration:  18 loss:  0.035091232508420944\n",
      "Iteration:  19 loss:  0.03642066568136215\n",
      "Iteration:  20 loss:  0.029510146006941795\n",
      "Iteration:  21 loss:  0.03133748099207878\n",
      "Iteration:  22 loss:  0.027959346771240234\n",
      "Iteration:  23 loss:  0.027882052585482597\n",
      "Iteration:  24 loss:  0.02473396621644497\n",
      "Iteration:  25 loss:  0.027310345321893692\n",
      "Iteration:  26 loss:  0.02488134056329727\n",
      "Iteration:  27 loss:  0.027058100327849388\n",
      "Iteration:  28 loss:  0.025782449170947075\n",
      "Iteration:  29 loss:  0.025193430483341217\n",
      "Iteration:  30 loss:  0.025186363607645035\n",
      "Iteration:  31 loss:  0.021072931587696075\n",
      "Iteration:  32 loss:  0.024840865284204483\n",
      "Iteration:  33 loss:  0.023578235879540443\n",
      "Iteration:  34 loss:  0.02025492861866951\n",
      "Iteration:  35 loss:  0.015530114993453026\n",
      "Iteration:  36 loss:  0.018983066082000732\n",
      "Iteration:  37 loss:  0.016268491744995117\n",
      "Iteration:  38 loss:  0.0159298125654459\n",
      "Iteration:  39 loss:  0.01600494049489498\n",
      "Iteration:  40 loss:  0.01794738508760929\n",
      "Iteration:  41 loss:  0.01916414499282837\n",
      "Iteration:  42 loss:  0.01651402749121189\n",
      "Iteration:  43 loss:  0.018377086147665977\n",
      "Iteration:  44 loss:  0.03310370817780495\n",
      "Iteration:  45 loss:  0.030828045681118965\n",
      "Iteration:  46 loss:  0.03187422826886177\n",
      "Iteration:  47 loss:  0.030233172699809074\n",
      "Iteration:  48 loss:  0.031822290271520615\n",
      "Iteration:  49 loss:  0.02893843874335289\n",
      "Iteration:  50 loss:  0.026854053139686584\n",
      "Iteration:  51 loss:  0.028116008266806602\n",
      "Iteration:  52 loss:  0.02141263149678707\n",
      "Iteration:  53 loss:  0.019793851301074028\n",
      "Iteration:  54 loss:  0.019275624305009842\n",
      "Iteration:  55 loss:  0.01725398376584053\n",
      "Iteration:  56 loss:  0.018910085782408714\n",
      "Iteration:  57 loss:  0.017917707562446594\n",
      "Iteration:  58 loss:  0.015179905109107494\n",
      "Iteration:  59 loss:  0.015210173092782497\n",
      "Iteration:  60 loss:  0.01797890104353428\n",
      "Iteration:  61 loss:  0.015235483646392822\n",
      "Iteration:  62 loss:  0.016720827668905258\n",
      "Iteration:  63 loss:  0.01597832515835762\n",
      "Iteration:  64 loss:  0.017161311581730843\n",
      "Iteration:  65 loss:  0.016948996111750603\n",
      "Iteration:  66 loss:  0.018629759550094604\n",
      "Iteration:  67 loss:  0.016387948766350746\n",
      "Iteration:  68 loss:  0.014660567045211792\n",
      "Iteration:  69 loss:  0.013878916390240192\n",
      "Iteration:  70 loss:  0.01764615811407566\n",
      "Iteration:  71 loss:  0.01776581071317196\n",
      "Iteration:  72 loss:  0.017022164538502693\n",
      "Iteration:  73 loss:  0.01790527068078518\n",
      "Iteration:  74 loss:  0.016111163422465324\n",
      "Iteration:  75 loss:  0.01612374745309353\n",
      "Iteration:  76 loss:  0.018225787207484245\n",
      "Iteration:  77 loss:  0.016158146783709526\n",
      "Iteration:  78 loss:  0.016779430210590363\n",
      "Iteration:  79 loss:  0.014874100685119629\n",
      "Iteration:  80 loss:  0.01383738312870264\n",
      "Iteration:  81 loss:  0.014282838441431522\n",
      "Iteration:  82 loss:  0.014775064773857594\n",
      "Iteration:  83 loss:  0.01297455932945013\n",
      "Iteration:  84 loss:  0.014684502966701984\n",
      "Iteration:  85 loss:  0.01599719747900963\n",
      "Iteration:  86 loss:  0.013559520244598389\n",
      "Iteration:  87 loss:  0.012099318206310272\n",
      "Iteration:  88 loss:  0.0102236932143569\n",
      "Iteration:  89 loss:  0.011581451632082462\n",
      "Iteration:  90 loss:  0.011607685126364231\n",
      "Iteration:  91 loss:  0.011010290123522282\n",
      "Iteration:  92 loss:  0.01077449880540371\n",
      "Iteration:  93 loss:  0.010199532844126225\n",
      "Iteration:  94 loss:  0.009538721293210983\n",
      "Iteration:  95 loss:  0.011466841213405132\n",
      "Iteration:  96 loss:  0.009946266189217567\n",
      "Iteration:  97 loss:  0.009363684803247452\n",
      "Iteration:  98 loss:  0.010249192826449871\n",
      "Iteration:  99 loss:  0.009591411799192429\n",
      "Iteration:  100 loss:  0.00938370544463396\n",
      "Iteration:  101 loss:  0.009833975695073605\n",
      "Iteration:  102 loss:  0.009717200882732868\n",
      "Iteration:  103 loss:  0.009449439123272896\n",
      "Iteration:  104 loss:  0.008867384865880013\n",
      "Iteration:  105 loss:  0.013591532595455647\n",
      "Iteration:  106 loss:  0.011977306567132473\n",
      "Iteration:  107 loss:  0.010863750241696835\n",
      "Iteration:  108 loss:  0.010958431288599968\n",
      "Iteration:  109 loss:  0.009890813380479813\n",
      "Iteration:  110 loss:  0.011720618233084679\n",
      "Iteration:  111 loss:  0.011223088018596172\n",
      "Iteration:  112 loss:  0.011714553460478783\n",
      "Iteration:  113 loss:  0.010504765436053276\n",
      "Iteration:  114 loss:  0.01105747651308775\n",
      "Iteration:  115 loss:  0.010859406553208828\n",
      "Iteration:  116 loss:  0.009568068198859692\n",
      "Iteration:  117 loss:  0.011075892485678196\n",
      "Iteration:  118 loss:  0.008693099953234196\n",
      "Iteration:  119 loss:  0.00909325759857893\n",
      "Iteration:  120 loss:  0.010706128552556038\n",
      "Iteration:  121 loss:  0.009414074011147022\n",
      "Iteration:  122 loss:  0.010420270264148712\n",
      "Iteration:  123 loss:  0.010219675488770008\n",
      "Iteration:  124 loss:  0.009508692659437656\n",
      "Iteration:  125 loss:  0.00801000650972128\n",
      "Iteration:  126 loss:  0.008114161901175976\n",
      "Iteration:  127 loss:  0.009539155289530754\n",
      "Iteration:  128 loss:  0.008427886292338371\n",
      "Iteration:  129 loss:  0.008688574656844139\n",
      "Iteration:  130 loss:  0.00801067054271698\n",
      "Iteration:  131 loss:  0.011560347862541676\n",
      "Iteration:  132 loss:  0.012702884152531624\n",
      "Iteration:  133 loss:  0.013366959989070892\n",
      "Iteration:  134 loss:  0.009489449672400951\n",
      "Iteration:  135 loss:  0.010725707747042179\n",
      "Iteration:  136 loss:  0.011116961017251015\n",
      "Iteration:  137 loss:  0.009894565679132938\n",
      "Iteration:  138 loss:  0.011127328500151634\n",
      "Iteration:  139 loss:  0.010827100835740566\n",
      "Iteration:  140 loss:  0.010418210178613663\n",
      "Iteration:  141 loss:  0.010114244185388088\n",
      "Iteration:  142 loss:  0.010054297745227814\n",
      "Iteration:  143 loss:  0.007563358172774315\n",
      "Iteration:  144 loss:  0.009294864721596241\n",
      "Iteration:  145 loss:  0.008946491405367851\n",
      "Iteration:  146 loss:  0.008326161652803421\n",
      "Iteration:  147 loss:  0.008585888892412186\n",
      "Iteration:  148 loss:  0.008610012009739876\n",
      "Iteration:  149 loss:  0.012489881366491318\n",
      "Iteration:  150 loss:  0.012402643449604511\n",
      "Iteration:  151 loss:  0.011272182688117027\n",
      "Iteration:  152 loss:  0.01044470351189375\n",
      "Iteration:  153 loss:  0.010914313606917858\n",
      "Iteration:  154 loss:  0.010751111432909966\n",
      "Iteration:  155 loss:  0.012214639224112034\n",
      "Iteration:  156 loss:  0.008910835720598698\n",
      "Iteration:  157 loss:  0.008444908075034618\n",
      "Iteration:  158 loss:  0.007669457234442234\n",
      "Iteration:  159 loss:  0.007769319694489241\n",
      "Iteration:  160 loss:  0.008091265335679054\n",
      "Iteration:  161 loss:  0.006555612664669752\n",
      "Iteration:  162 loss:  0.005993700586259365\n",
      "Iteration:  163 loss:  0.00691830413416028\n",
      "Iteration:  164 loss:  0.007313792128115892\n",
      "Iteration:  165 loss:  0.005924203898757696\n",
      "Iteration:  166 loss:  0.009271606802940369\n",
      "Iteration:  167 loss:  0.009373610839247704\n",
      "Iteration:  168 loss:  0.008795569650828838\n",
      "Iteration:  169 loss:  0.008646373637020588\n",
      "Iteration:  170 loss:  0.007469847798347473\n",
      "Iteration:  171 loss:  0.010613237507641315\n",
      "Iteration:  172 loss:  0.008700958453118801\n",
      "Iteration:  173 loss:  0.008759071119129658\n",
      "Iteration:  174 loss:  0.009872674942016602\n",
      "Iteration:  175 loss:  0.007340466137975454\n",
      "Iteration:  176 loss:  0.009228074923157692\n",
      "Iteration:  177 loss:  0.006904582027345896\n",
      "Iteration:  178 loss:  0.007377223111689091\n",
      "Iteration:  179 loss:  0.007995170541107655\n",
      "Iteration:  180 loss:  0.007323756348341703\n",
      "Iteration:  181 loss:  0.006926488596946001\n",
      "Iteration:  182 loss:  0.008078276179730892\n",
      "Iteration:  183 loss:  0.007315929047763348\n",
      "Iteration:  184 loss:  0.010116941295564175\n",
      "Iteration:  185 loss:  0.010149521753191948\n",
      "Iteration:  186 loss:  0.009323574602603912\n",
      "Iteration:  187 loss:  0.007669772487133741\n",
      "Iteration:  188 loss:  0.00838556233793497\n",
      "Iteration:  189 loss:  0.0073564313352108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  190 loss:  0.009572543203830719\n",
      "Iteration:  191 loss:  0.008479790762066841\n",
      "Iteration:  192 loss:  0.00688283471390605\n",
      "Iteration:  193 loss:  0.007844479754567146\n",
      "Iteration:  194 loss:  0.008376275189220905\n",
      "Iteration:  195 loss:  0.007976631633937359\n",
      "Iteration:  196 loss:  0.007318259682506323\n",
      "Iteration:  197 loss:  0.007003404665738344\n",
      "Iteration:  198 loss:  0.007884702645242214\n",
      "Iteration:  199 loss:  0.007305547129362822\n",
      "Iteration:  200 loss:  0.007403187919408083\n",
      "Iteration:  201 loss:  0.007741170469671488\n",
      "Iteration:  202 loss:  0.00784634705632925\n",
      "Iteration:  203 loss:  0.006823928561061621\n",
      "Iteration:  204 loss:  0.007095615845173597\n",
      "Iteration:  205 loss:  0.007300660014152527\n",
      "Iteration:  206 loss:  0.00727043254300952\n",
      "Iteration:  207 loss:  0.00763570936396718\n",
      "Iteration:  208 loss:  0.007775216829031706\n",
      "Iteration:  209 loss:  0.00908636674284935\n",
      "Iteration:  210 loss:  0.02615322358906269\n",
      "Iteration:  211 loss:  0.028548603877425194\n",
      "Iteration:  212 loss:  0.026143433526158333\n",
      "Iteration:  213 loss:  0.02552608586847782\n",
      "Iteration:  214 loss:  0.025075748562812805\n",
      "Iteration:  215 loss:  0.022241517901420593\n",
      "Iteration:  216 loss:  0.023813098669052124\n",
      "Iteration:  217 loss:  0.019768813624978065\n",
      "Iteration:  218 loss:  0.017890527844429016\n",
      "Iteration:  219 loss:  0.007957011461257935\n",
      "Iteration:  220 loss:  0.007666387595236301\n",
      "Iteration:  221 loss:  0.008151908405125141\n",
      "Iteration:  222 loss:  0.00754270376637578\n",
      "Iteration:  223 loss:  0.006993690971285105\n",
      "Iteration:  224 loss:  0.007640487980097532\n",
      "Iteration:  225 loss:  0.006839171517640352\n",
      "Iteration:  226 loss:  0.007111715152859688\n",
      "Iteration:  227 loss:  0.008579922839999199\n",
      "Iteration:  228 loss:  0.0088425911962986\n",
      "Iteration:  229 loss:  0.009428550489246845\n",
      "Iteration:  230 loss:  0.007263509556651115\n",
      "Iteration:  231 loss:  0.009468569420278072\n",
      "Iteration:  232 loss:  0.010368837043642998\n",
      "Iteration:  233 loss:  0.010245065204799175\n",
      "Iteration:  234 loss:  0.008814607746899128\n",
      "Iteration:  235 loss:  0.007286155130714178\n",
      "Iteration:  236 loss:  0.008534979075193405\n",
      "Iteration:  237 loss:  0.007948050275444984\n",
      "Iteration:  238 loss:  0.007175107952207327\n",
      "Iteration:  239 loss:  0.007958211936056614\n",
      "Iteration:  240 loss:  0.00808523129671812\n",
      "Iteration:  241 loss:  0.006246792618185282\n",
      "Iteration:  242 loss:  0.007340677548199892\n",
      "Iteration:  243 loss:  0.006486093159765005\n",
      "Iteration:  244 loss:  0.00845353677868843\n",
      "Iteration:  245 loss:  0.0058908346109092236\n",
      "Iteration:  246 loss:  0.005987097974866629\n",
      "Iteration:  247 loss:  0.005574283190071583\n",
      "Iteration:  248 loss:  0.005549087189137936\n",
      "Iteration:  249 loss:  0.0062778363935649395\n",
      "Iteration:  250 loss:  0.005842340644448996\n",
      "Iteration:  251 loss:  0.005886480212211609\n",
      "Iteration:  252 loss:  0.007306318264454603\n",
      "Iteration:  253 loss:  0.0065383752807974815\n",
      "Iteration:  254 loss:  0.011317398399114609\n",
      "Iteration:  255 loss:  0.011860785074532032\n",
      "Iteration:  256 loss:  0.008886287920176983\n",
      "Iteration:  257 loss:  0.009757589548826218\n",
      "Iteration:  258 loss:  0.009318770840764046\n",
      "Iteration:  259 loss:  0.008891528472304344\n",
      "Iteration:  260 loss:  0.008892558515071869\n",
      "Iteration:  261 loss:  0.008900552988052368\n",
      "Iteration:  262 loss:  0.007580632343888283\n",
      "Iteration:  263 loss:  0.006345424335449934\n",
      "Iteration:  264 loss:  0.006568500306457281\n",
      "Iteration:  265 loss:  0.006719903089106083\n",
      "Iteration:  266 loss:  0.006535963620990515\n",
      "Iteration:  267 loss:  0.006585505325347185\n",
      "Iteration:  268 loss:  0.005987637676298618\n",
      "Iteration:  269 loss:  0.006395325995981693\n",
      "Iteration:  270 loss:  0.006082900799810886\n",
      "Iteration:  271 loss:  0.0063748531974852085\n",
      "Iteration:  272 loss:  0.006072664167732\n",
      "Iteration:  273 loss:  0.00604009535163641\n",
      "Iteration:  274 loss:  0.006149857770651579\n",
      "Iteration:  275 loss:  0.006685204338282347\n",
      "Iteration:  276 loss:  0.005460270680487156\n",
      "Iteration:  277 loss:  0.006663240492343903\n",
      "Iteration:  278 loss:  0.005505467299371958\n",
      "Iteration:  279 loss:  0.0062436978332698345\n",
      "Iteration:  280 loss:  0.008715405128896236\n",
      "Iteration:  281 loss:  0.009416485205292702\n",
      "Iteration:  282 loss:  0.008756071329116821\n",
      "Iteration:  283 loss:  0.009390835650265217\n",
      "Iteration:  284 loss:  0.009552262723445892\n",
      "Iteration:  285 loss:  0.00930531695485115\n",
      "Iteration:  286 loss:  0.00964611116796732\n",
      "Iteration:  287 loss:  0.009346219711005688\n",
      "Iteration:  288 loss:  0.008314172737300396\n",
      "Iteration:  289 loss:  0.005064214579761028\n",
      "Iteration:  290 loss:  0.005515605676919222\n",
      "Iteration:  291 loss:  0.005966345779597759\n",
      "Iteration:  292 loss:  0.006439731922000647\n",
      "Iteration:  293 loss:  0.005544541869312525\n",
      "Iteration:  294 loss:  0.005786528345197439\n",
      "Iteration:  295 loss:  0.005562305450439453\n",
      "Iteration:  296 loss:  0.0048013716004788876\n",
      "Iteration:  297 loss:  0.006532322149723768\n",
      "Iteration:  298 loss:  0.008371178060770035\n",
      "Iteration:  299 loss:  0.007470383774489164\n",
      "Iteration:  300 loss:  0.008684989996254444\n",
      "Iteration:  301 loss:  0.008395818993449211\n",
      "Iteration:  302 loss:  0.00673068268224597\n",
      "Iteration:  303 loss:  0.007743048947304487\n",
      "Iteration:  304 loss:  0.007635399699211121\n",
      "Iteration:  305 loss:  0.006238482426851988\n",
      "Iteration:  306 loss:  0.008346525952219963\n",
      "Iteration:  307 loss:  0.0069679380394518375\n",
      "Iteration:  308 loss:  0.006571589969098568\n",
      "Iteration:  309 loss:  0.006069904658943415\n",
      "Iteration:  310 loss:  0.00645294738933444\n",
      "Iteration:  311 loss:  0.006462352350354195\n",
      "Iteration:  312 loss:  0.0069694663397967815\n",
      "Iteration:  313 loss:  0.005957042332738638\n",
      "Iteration:  314 loss:  0.005861305166035891\n",
      "Iteration:  315 loss:  0.005158538930118084\n",
      "Iteration:  316 loss:  0.005595086608082056\n",
      "Iteration:  317 loss:  0.006150807719677687\n",
      "Iteration:  318 loss:  0.005201713647693396\n",
      "Iteration:  319 loss:  0.005619416944682598\n",
      "Iteration:  320 loss:  0.005206383299082518\n",
      "Iteration:  321 loss:  0.005034953355789185\n",
      "Iteration:  322 loss:  0.004811801481992006\n",
      "Iteration:  323 loss:  0.005833490751683712\n",
      "Iteration:  324 loss:  0.007189716678112745\n",
      "Iteration:  325 loss:  0.006327331531792879\n",
      "Iteration:  326 loss:  0.006800578907132149\n",
      "Iteration:  327 loss:  0.007383150048553944\n",
      "Iteration:  328 loss:  0.007047579623758793\n",
      "Iteration:  329 loss:  0.0073488266207277775\n",
      "Iteration:  330 loss:  0.005949045065790415\n",
      "Iteration:  331 loss:  0.006761542055755854\n",
      "Iteration:  332 loss:  0.005960620474070311\n",
      "Iteration:  333 loss:  0.0067297909408807755\n",
      "Iteration:  334 loss:  0.006561561021953821\n",
      "Iteration:  335 loss:  0.0065560294315218925\n",
      "Iteration:  336 loss:  0.00634218193590641\n",
      "Iteration:  337 loss:  0.006006339564919472\n",
      "Iteration:  338 loss:  0.00611136993393302\n",
      "Iteration:  339 loss:  0.005547077860683203\n",
      "Iteration:  340 loss:  0.006438947282731533\n",
      "Iteration:  341 loss:  0.005584523547440767\n",
      "Iteration:  342 loss:  0.006122041493654251\n",
      "Iteration:  343 loss:  0.005743758287280798\n",
      "Iteration:  344 loss:  0.006217716261744499\n",
      "Iteration:  345 loss:  0.0059856814332306385\n",
      "Iteration:  346 loss:  0.0058662001974880695\n",
      "Iteration:  347 loss:  0.00664228992536664\n",
      "Iteration:  348 loss:  0.0055715953931212425\n",
      "Iteration:  349 loss:  0.005914153531193733\n",
      "Iteration:  350 loss:  0.007485863287001848\n",
      "Iteration:  351 loss:  0.007354957051575184\n",
      "Iteration:  352 loss:  0.0071075246669352055\n",
      "Iteration:  353 loss:  0.008270329795777798\n",
      "Iteration:  354 loss:  0.006316031329333782\n",
      "Iteration:  355 loss:  0.007413601502776146\n",
      "Iteration:  356 loss:  0.007450778968632221\n",
      "Iteration:  357 loss:  0.0075456006452441216\n",
      "Iteration:  358 loss:  0.008339814841747284\n",
      "Iteration:  359 loss:  0.011386621743440628\n",
      "Iteration:  360 loss:  0.010622347705066204\n",
      "Iteration:  361 loss:  0.010555769316852093\n",
      "Iteration:  362 loss:  0.009390612132847309\n",
      "Iteration:  363 loss:  0.010701091960072517\n",
      "Iteration:  364 loss:  0.010535676032304764\n",
      "Iteration:  365 loss:  0.009941325522959232\n",
      "Iteration:  366 loss:  0.008882029913365841\n",
      "Iteration:  367 loss:  0.007694052532315254\n",
      "Iteration:  368 loss:  0.004671289585530758\n",
      "Iteration:  369 loss:  0.00520741194486618\n",
      "Iteration:  370 loss:  0.005571987945586443\n",
      "Iteration:  371 loss:  0.005552475340664387\n",
      "Iteration:  372 loss:  0.005871173460036516\n",
      "Iteration:  373 loss:  0.006422936450690031\n",
      "Iteration:  374 loss:  0.005565119441598654\n",
      "Iteration:  375 loss:  0.0059041790664196014\n",
      "Iteration:  376 loss:  0.00848152581602335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  377 loss:  0.007622045930474997\n",
      "Iteration:  378 loss:  0.008835313841700554\n",
      "Iteration:  379 loss:  0.00814378447830677\n",
      "Iteration:  380 loss:  0.00782507099211216\n",
      "Iteration:  381 loss:  0.008464409969747066\n",
      "Iteration:  382 loss:  0.008098236285150051\n",
      "Iteration:  383 loss:  0.007555454503744841\n",
      "Iteration:  384 loss:  0.007881629280745983\n",
      "Iteration:  385 loss:  0.008044405840337276\n",
      "Iteration:  386 loss:  0.007260564249008894\n",
      "Iteration:  387 loss:  0.007807706017047167\n",
      "Iteration:  388 loss:  0.008063594810664654\n",
      "Iteration:  389 loss:  0.008382942527532578\n",
      "Iteration:  390 loss:  0.007294681388884783\n",
      "Iteration:  391 loss:  0.006957044824957848\n",
      "Iteration:  392 loss:  0.007379575166851282\n",
      "Iteration:  393 loss:  0.006249243393540382\n",
      "Iteration:  394 loss:  0.0052285403944551945\n",
      "Iteration:  395 loss:  0.004912988748401403\n",
      "Iteration:  396 loss:  0.005730575416237116\n",
      "Iteration:  397 loss:  0.006592223886400461\n",
      "Iteration:  398 loss:  0.00692874938249588\n",
      "Iteration:  399 loss:  0.005397865083068609\n",
      "Iteration:  400 loss:  0.005511314142495394\n",
      "Iteration:  401 loss:  0.00618932768702507\n",
      "Iteration:  402 loss:  0.006805220618844032\n",
      "Iteration:  403 loss:  0.00691527035087347\n",
      "Iteration:  404 loss:  0.006529239006340504\n",
      "Iteration:  405 loss:  0.006675455253571272\n",
      "Iteration:  406 loss:  0.005425525363534689\n",
      "Iteration:  407 loss:  0.005843339022248983\n",
      "Iteration:  408 loss:  0.005868115462362766\n",
      "Iteration:  409 loss:  0.005006395746022463\n",
      "Iteration:  410 loss:  0.006142088212072849\n",
      "Iteration:  411 loss:  0.006039690691977739\n",
      "Iteration:  412 loss:  0.005887695122510195\n",
      "Iteration:  413 loss:  0.005188141018152237\n",
      "Iteration:  414 loss:  0.005577907897531986\n",
      "Iteration:  415 loss:  0.005265377927571535\n",
      "Iteration:  416 loss:  0.005617207381874323\n",
      "Iteration:  417 loss:  0.0055824872106313705\n",
      "Iteration:  418 loss:  0.004445671569555998\n",
      "Iteration:  419 loss:  0.0050149960443377495\n",
      "Iteration:  420 loss:  0.01217719167470932\n",
      "Iteration:  421 loss:  0.010700451210141182\n",
      "Iteration:  422 loss:  0.009588334709405899\n",
      "Iteration:  423 loss:  0.010308858938515186\n",
      "Iteration:  424 loss:  0.011494510807096958\n",
      "Iteration:  425 loss:  0.011426062323153019\n",
      "Iteration:  426 loss:  0.010054155252873898\n",
      "Iteration:  427 loss:  0.010498597286641598\n",
      "Iteration:  428 loss:  0.008888877928256989\n",
      "Iteration:  429 loss:  0.007158455904573202\n",
      "Iteration:  430 loss:  0.006695474497973919\n",
      "Iteration:  431 loss:  0.006242064293473959\n",
      "Iteration:  432 loss:  0.006770729552954435\n",
      "Iteration:  433 loss:  0.005807872395962477\n",
      "Iteration:  434 loss:  0.0060999272391200066\n",
      "Iteration:  435 loss:  0.006493626162409782\n",
      "Iteration:  436 loss:  0.006263804621994495\n",
      "Iteration:  437 loss:  0.006174707785248756\n",
      "Iteration:  438 loss:  0.0065519558265805244\n",
      "Iteration:  439 loss:  0.006818214897066355\n",
      "Iteration:  440 loss:  0.007461349945515394\n",
      "Iteration:  441 loss:  0.005549794062972069\n",
      "Iteration:  442 loss:  0.0061869388446211815\n",
      "Iteration:  443 loss:  0.005925232078880072\n",
      "Iteration:  444 loss:  0.00522522022947669\n",
      "Iteration:  445 loss:  0.0055092633701860905\n",
      "Iteration:  446 loss:  0.005217640195041895\n",
      "Iteration:  447 loss:  0.004499432630836964\n",
      "Iteration:  448 loss:  0.004798784386366606\n",
      "Iteration:  449 loss:  0.004052796401083469\n",
      "Iteration:  450 loss:  0.004558134358376265\n",
      "Iteration:  451 loss:  0.005329787265509367\n",
      "Iteration:  452 loss:  0.004817160312086344\n",
      "Iteration:  453 loss:  0.004928884096443653\n",
      "Iteration:  454 loss:  0.0050827497616410255\n",
      "Iteration:  455 loss:  0.004991607740521431\n",
      "Iteration:  456 loss:  0.004444363992661238\n",
      "Iteration:  457 loss:  0.005130788777023554\n",
      "Iteration:  458 loss:  0.004805175121873617\n",
      "Iteration:  459 loss:  0.00506626907736063\n",
      "Iteration:  460 loss:  0.004427099134773016\n",
      "Iteration:  461 loss:  0.005426811520010233\n",
      "Iteration:  462 loss:  0.00532873859629035\n",
      "Iteration:  463 loss:  0.0047197118401527405\n",
      "Iteration:  464 loss:  0.005140424706041813\n",
      "Iteration:  465 loss:  0.006258329842239618\n",
      "Iteration:  466 loss:  0.00624573789536953\n",
      "Iteration:  467 loss:  0.007195799145847559\n",
      "Iteration:  468 loss:  0.005038203671574593\n",
      "Iteration:  469 loss:  0.006177632138133049\n",
      "Iteration:  470 loss:  0.006104861851781607\n",
      "Iteration:  471 loss:  0.005319381132721901\n",
      "Iteration:  472 loss:  0.006179332733154297\n",
      "Iteration:  473 loss:  0.0055353655479848385\n",
      "Iteration:  474 loss:  0.004713030997663736\n",
      "Iteration:  475 loss:  0.005763472523540258\n",
      "Iteration:  476 loss:  0.005395385902374983\n",
      "Iteration:  477 loss:  0.005085485056042671\n",
      "Iteration:  478 loss:  0.004979344084858894\n",
      "Iteration:  479 loss:  0.00511371623724699\n",
      "Iteration:  480 loss:  0.005231338553130627\n",
      "Iteration:  481 loss:  0.0066702584736049175\n",
      "Iteration:  482 loss:  0.004800157155841589\n",
      "Iteration:  483 loss:  0.004850624594837427\n",
      "Iteration:  484 loss:  0.005146844312548637\n",
      "Iteration:  485 loss:  0.0043709659948945045\n",
      "Iteration:  486 loss:  0.0061500840820372105\n",
      "Iteration:  487 loss:  0.0044920723885297775\n",
      "Iteration:  488 loss:  0.005087169352918863\n",
      "Iteration:  489 loss:  0.004308529198169708\n",
      "Iteration:  490 loss:  0.0051070344634354115\n",
      "Iteration:  491 loss:  0.004460649099200964\n",
      "Iteration:  492 loss:  0.005412381142377853\n",
      "Iteration:  493 loss:  0.005278364755213261\n",
      "Iteration:  494 loss:  0.0050882515497505665\n",
      "Iteration:  495 loss:  0.005539883393794298\n",
      "Iteration:  496 loss:  0.004231818486005068\n",
      "Iteration:  497 loss:  0.004313887096941471\n",
      "Iteration:  498 loss:  0.0063977171666920185\n",
      "Iteration:  499 loss:  0.00956189539283514\n",
      "Iteration:  500 loss:  0.009863151237368584\n",
      "Iteration:  501 loss:  0.009891397319734097\n",
      "Iteration:  502 loss:  0.010781032964587212\n",
      "Iteration:  503 loss:  0.00980650819838047\n",
      "Iteration:  504 loss:  0.008542390540242195\n",
      "Iteration:  505 loss:  0.009348420426249504\n",
      "Iteration:  506 loss:  0.009133460000157356\n",
      "Iteration:  507 loss:  0.006672281306236982\n",
      "Iteration:  508 loss:  0.00545406062155962\n",
      "Iteration:  509 loss:  0.00625569187104702\n",
      "Iteration:  510 loss:  0.005544608924537897\n",
      "Iteration:  511 loss:  0.005181158427149057\n",
      "Iteration:  512 loss:  0.005516804754734039\n",
      "Iteration:  513 loss:  0.00528750428929925\n",
      "Iteration:  514 loss:  0.005230877548456192\n",
      "Iteration:  515 loss:  0.005130428355187178\n",
      "Iteration:  516 loss:  0.01802230067551136\n",
      "Iteration:  517 loss:  0.022457845509052277\n",
      "Iteration:  518 loss:  0.02061714045703411\n",
      "Iteration:  519 loss:  0.019954515621066093\n",
      "Iteration:  520 loss:  0.020625608041882515\n",
      "Iteration:  521 loss:  0.018298495560884476\n",
      "Iteration:  522 loss:  0.019700292497873306\n",
      "Iteration:  523 loss:  0.01781376078724861\n",
      "Iteration:  524 loss:  0.01635739393532276\n",
      "Iteration:  525 loss:  0.016557877883315086\n",
      "Iteration:  526 loss:  0.01532144658267498\n",
      "Iteration:  527 loss:  0.014725281856954098\n",
      "Iteration:  528 loss:  0.016794780269265175\n",
      "Iteration:  529 loss:  0.01397830992937088\n",
      "Iteration:  530 loss:  0.014340318739414215\n",
      "Iteration:  531 loss:  0.013146718963980675\n",
      "Iteration:  532 loss:  0.012819558382034302\n",
      "Iteration:  533 loss:  0.010119459591805935\n",
      "Iteration:  534 loss:  0.0095775555819273\n",
      "Iteration:  535 loss:  0.010320059023797512\n",
      "Iteration:  536 loss:  0.01094638928771019\n",
      "Iteration:  537 loss:  0.008421477861702442\n",
      "Iteration:  538 loss:  0.009774632751941681\n",
      "Iteration:  539 loss:  0.008705005049705505\n",
      "Iteration:  540 loss:  0.010403132997453213\n",
      "Iteration:  541 loss:  0.010493193753063679\n",
      "Iteration:  542 loss:  0.00773948710411787\n",
      "Iteration:  543 loss:  0.006567290984094143\n",
      "Iteration:  544 loss:  0.006037591956555843\n",
      "Iteration:  545 loss:  0.005618349649012089\n",
      "Iteration:  546 loss:  0.006155910436064005\n",
      "Iteration:  547 loss:  0.005710984580218792\n",
      "Iteration:  548 loss:  0.006322520785033703\n",
      "Iteration:  549 loss:  0.006472333334386349\n",
      "Iteration:  550 loss:  0.005987591575831175\n",
      "Iteration:  551 loss:  0.010098143480718136\n",
      "Iteration:  552 loss:  0.010906499810516834\n",
      "Iteration:  553 loss:  0.008766626939177513\n",
      "Iteration:  554 loss:  0.0096952635794878\n",
      "Iteration:  555 loss:  0.009068164974451065\n",
      "Iteration:  556 loss:  0.009888489730656147\n",
      "Iteration:  557 loss:  0.009943501092493534\n",
      "Iteration:  558 loss:  0.008773788809776306\n",
      "Iteration:  559 loss:  0.008883591741323471\n",
      "Iteration:  560 loss:  0.007123150862753391\n",
      "Iteration:  561 loss:  0.007375754881650209\n",
      "Iteration:  562 loss:  0.007078643422573805\n",
      "Iteration:  563 loss:  0.007407957687973976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  564 loss:  0.00664640823379159\n",
      "Iteration:  565 loss:  0.006703627295792103\n",
      "Iteration:  566 loss:  0.007076142355799675\n",
      "Iteration:  567 loss:  0.006108908914029598\n",
      "Iteration:  568 loss:  0.006003437098115683\n",
      "Iteration:  569 loss:  0.005308340769261122\n",
      "Iteration:  570 loss:  0.005717694293707609\n",
      "Iteration:  571 loss:  0.005448104813694954\n",
      "Iteration:  572 loss:  0.0060028922744095325\n",
      "Iteration:  573 loss:  0.004908631555736065\n",
      "Iteration:  574 loss:  0.004886074457317591\n",
      "Iteration:  575 loss:  0.005675213411450386\n",
      "Iteration:  576 loss:  0.006057743448764086\n",
      "Iteration:  577 loss:  0.0074386452324688435\n",
      "Iteration:  578 loss:  0.007510538212954998\n",
      "Iteration:  579 loss:  0.006653910037130117\n",
      "Iteration:  580 loss:  0.006333376746624708\n",
      "Iteration:  581 loss:  0.005418118089437485\n",
      "Iteration:  582 loss:  0.00590846873819828\n",
      "Iteration:  583 loss:  0.005616751965135336\n",
      "Iteration:  584 loss:  0.0062012625858187675\n",
      "Iteration:  585 loss:  0.005434509366750717\n",
      "Iteration:  586 loss:  0.008248723112046719\n",
      "Iteration:  587 loss:  0.008250098675489426\n",
      "Iteration:  588 loss:  0.006830963771790266\n",
      "Iteration:  589 loss:  0.008107006549835205\n",
      "Iteration:  590 loss:  0.007653036620467901\n",
      "Iteration:  591 loss:  0.007930203340947628\n",
      "Iteration:  592 loss:  0.007725611794739962\n",
      "Iteration:  593 loss:  0.0072774444706737995\n",
      "Iteration:  594 loss:  0.006971113849431276\n",
      "Iteration:  595 loss:  0.005244756117463112\n",
      "Iteration:  596 loss:  0.005201376508921385\n",
      "Iteration:  597 loss:  0.005061005242168903\n",
      "Iteration:  598 loss:  0.006227169185876846\n",
      "Iteration:  599 loss:  0.005658263806253672\n",
      "Iteration:  600 loss:  0.005246555432677269\n",
      "Iteration:  601 loss:  0.006020521279424429\n",
      "Iteration:  602 loss:  0.005374479573220015\n",
      "Iteration:  603 loss:  0.007464998867362738\n",
      "Iteration:  604 loss:  0.0054083834402263165\n",
      "Iteration:  605 loss:  0.009557290934026241\n",
      "Iteration:  606 loss:  0.006283486727625132\n",
      "Iteration:  607 loss:  0.007538917474448681\n",
      "Iteration:  608 loss:  0.007743875961750746\n",
      "Iteration:  609 loss:  0.008656813763082027\n",
      "Iteration:  610 loss:  0.007461890112608671\n",
      "Iteration:  611 loss:  0.008013591170310974\n",
      "Iteration:  612 loss:  0.007708502002060413\n",
      "Iteration:  613 loss:  0.007164486218243837\n",
      "Iteration:  614 loss:  0.008036247454583645\n",
      "Iteration:  615 loss:  0.009524852968752384\n",
      "Iteration:  616 loss:  0.009104696102440357\n",
      "Iteration:  617 loss:  0.00852837972342968\n",
      "Iteration:  618 loss:  0.00840767938643694\n",
      "Iteration:  619 loss:  0.00870379526168108\n",
      "Iteration:  620 loss:  0.008983097970485687\n",
      "Iteration:  621 loss:  0.006339311599731445\n",
      "Iteration:  622 loss:  0.005549116991460323\n",
      "Iteration:  623 loss:  0.004228501580655575\n",
      "Iteration:  624 loss:  0.0051167793571949005\n",
      "Iteration:  625 loss:  0.004769753664731979\n",
      "Iteration:  626 loss:  0.005163451191037893\n",
      "Iteration:  627 loss:  0.005575032904744148\n",
      "Iteration:  628 loss:  0.005692454054951668\n",
      "Iteration:  629 loss:  0.005331298802047968\n",
      "Iteration:  630 loss:  0.005115166772156954\n",
      "Iteration:  631 loss:  0.005358295980840921\n",
      "Iteration:  632 loss:  0.004614115227013826\n",
      "Iteration:  633 loss:  0.005041966680437326\n",
      "Iteration:  634 loss:  0.00502436887472868\n",
      "Iteration:  635 loss:  0.004545955918729305\n",
      "Iteration:  636 loss:  0.005611542146652937\n",
      "Iteration:  637 loss:  0.004983086138963699\n",
      "Iteration:  638 loss:  0.0057462966069579124\n",
      "Iteration:  639 loss:  0.004989712964743376\n",
      "Iteration:  640 loss:  0.0051191928796470165\n",
      "Iteration:  641 loss:  0.005559760611504316\n",
      "Iteration:  642 loss:  0.0057970574125647545\n",
      "Iteration:  643 loss:  0.005364459007978439\n",
      "Iteration:  644 loss:  0.005779490806162357\n",
      "Iteration:  645 loss:  0.005215554963797331\n",
      "Iteration:  646 loss:  0.005569117609411478\n",
      "Iteration:  647 loss:  0.005180774722248316\n",
      "Iteration:  648 loss:  0.005122623406350613\n",
      "Iteration:  649 loss:  0.00477876141667366\n",
      "Iteration:  650 loss:  0.005057666450738907\n",
      "Iteration:  651 loss:  0.005410010926425457\n",
      "Iteration:  652 loss:  0.005586146842688322\n",
      "Iteration:  653 loss:  0.004904759582132101\n",
      "Iteration:  654 loss:  0.0053902617655694485\n",
      "Iteration:  655 loss:  0.004949343856424093\n",
      "Iteration:  656 loss:  0.006300574634224176\n",
      "Iteration:  657 loss:  0.006995209027081728\n",
      "Iteration:  658 loss:  0.006842873524874449\n",
      "Iteration:  659 loss:  0.006414119619876146\n",
      "Iteration:  660 loss:  0.006897156126797199\n",
      "Iteration:  661 loss:  0.006680125370621681\n",
      "Iteration:  662 loss:  0.0072983140125870705\n",
      "Iteration:  663 loss:  0.00586175499483943\n",
      "Iteration:  664 loss:  0.006685925181955099\n",
      "Iteration:  665 loss:  0.0053145382553339005\n",
      "Iteration:  666 loss:  0.005141535308212042\n",
      "Iteration:  667 loss:  0.005166294053196907\n",
      "Iteration:  668 loss:  0.0051459078676998615\n",
      "Iteration:  669 loss:  0.004740358795970678\n",
      "Iteration:  670 loss:  0.004688497632741928\n",
      "Iteration:  671 loss:  0.004026102367788553\n",
      "Iteration:  672 loss:  0.004479216877371073\n",
      "Iteration:  673 loss:  0.0052131060510873795\n",
      "Iteration:  674 loss:  0.006043360568583012\n",
      "Iteration:  675 loss:  0.006377521436661482\n",
      "Iteration:  676 loss:  0.0051496694795787334\n",
      "Iteration:  677 loss:  0.006603216752409935\n",
      "Iteration:  678 loss:  0.005925329867750406\n",
      "Iteration:  679 loss:  0.005500071682035923\n",
      "Iteration:  680 loss:  0.005369023885577917\n",
      "Iteration:  681 loss:  0.006649835035204887\n",
      "Iteration:  682 loss:  0.00652729719877243\n",
      "Iteration:  683 loss:  0.005770709831267595\n",
      "Iteration:  684 loss:  0.005754683166742325\n",
      "Iteration:  685 loss:  0.005676617380231619\n",
      "Iteration:  686 loss:  0.006828984245657921\n",
      "Iteration:  687 loss:  0.006051256321370602\n",
      "Iteration:  688 loss:  0.006605906877666712\n",
      "Iteration:  689 loss:  0.006487940903753042\n",
      "Iteration:  690 loss:  0.006327803712338209\n",
      "Iteration:  691 loss:  0.004906577058136463\n",
      "Iteration:  692 loss:  0.004408119712024927\n",
      "Iteration:  693 loss:  0.005131774581968784\n",
      "Iteration:  694 loss:  0.004870615899562836\n",
      "Iteration:  695 loss:  0.0043577514588832855\n",
      "Iteration:  696 loss:  0.005297596100717783\n",
      "Iteration:  697 loss:  0.0037262814585119486\n",
      "Iteration:  698 loss:  0.004624648950994015\n",
      "Iteration:  699 loss:  0.00454714335501194\n",
      "Iteration:  700 loss:  0.006210589315742254\n",
      "Iteration:  701 loss:  0.005891581531614065\n",
      "Iteration:  702 loss:  0.0059718298725783825\n",
      "Iteration:  703 loss:  0.00605312455445528\n",
      "Iteration:  704 loss:  0.005318719428032637\n",
      "Iteration:  705 loss:  0.006129647605121136\n",
      "Iteration:  706 loss:  0.004619602579623461\n",
      "Iteration:  707 loss:  0.00505044125020504\n",
      "Iteration:  708 loss:  0.006107656750828028\n",
      "Iteration:  709 loss:  0.005849266890436411\n",
      "Iteration:  710 loss:  0.005227009765803814\n",
      "Iteration:  711 loss:  0.0052018421702086926\n",
      "Iteration:  712 loss:  0.005765811540186405\n",
      "Iteration:  713 loss:  0.0051265740767121315\n",
      "Iteration:  714 loss:  0.005387178156524897\n",
      "Iteration:  715 loss:  0.005313773639500141\n",
      "Iteration:  716 loss:  0.0045071812346577644\n",
      "Iteration:  717 loss:  0.005430057179182768\n",
      "Iteration:  718 loss:  0.005359007976949215\n",
      "Iteration:  719 loss:  0.005838451441377401\n",
      "Iteration:  720 loss:  0.005302279256284237\n",
      "Iteration:  721 loss:  0.005887285806238651\n",
      "Iteration:  722 loss:  0.005833782721310854\n",
      "Iteration:  723 loss:  0.005779929459095001\n",
      "Iteration:  724 loss:  0.006491431966423988\n",
      "Iteration:  725 loss:  0.005903583951294422\n",
      "Iteration:  726 loss:  0.0060943313874304295\n",
      "Iteration:  727 loss:  0.006334401201456785\n",
      "Iteration:  728 loss:  0.005479230545461178\n",
      "Iteration:  729 loss:  0.004656944889575243\n",
      "Iteration:  730 loss:  0.006194602232426405\n",
      "Iteration:  731 loss:  0.004916224163025618\n",
      "Iteration:  732 loss:  0.006203575991094112\n",
      "Iteration:  733 loss:  0.005002305842936039\n",
      "Iteration:  734 loss:  0.005239949095994234\n",
      "Iteration:  735 loss:  0.017672287300229073\n",
      "Iteration:  736 loss:  0.017309673130512238\n",
      "Iteration:  737 loss:  0.018011849373579025\n",
      "Iteration:  738 loss:  0.01925436221063137\n",
      "Iteration:  739 loss:  0.018709171563386917\n",
      "Iteration:  740 loss:  0.018220342695713043\n",
      "Iteration:  741 loss:  0.017058437690138817\n",
      "Iteration:  742 loss:  0.01583697646856308\n",
      "Iteration:  743 loss:  0.012872015126049519\n",
      "Iteration:  744 loss:  0.00561755895614624\n",
      "Iteration:  745 loss:  0.006721725221723318\n",
      "Iteration:  746 loss:  0.005624496843665838\n",
      "Iteration:  747 loss:  0.00671355752274394\n",
      "Iteration:  748 loss:  0.006609187461435795\n",
      "Iteration:  749 loss:  0.006739300675690174\n",
      "Iteration:  750 loss:  0.006148881744593382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  751 loss:  0.0064310431480407715\n",
      "Iteration:  752 loss:  0.0074643101543188095\n",
      "Iteration:  753 loss:  0.006012361031025648\n",
      "Iteration:  754 loss:  0.006156399846076965\n",
      "Iteration:  755 loss:  0.006814029533416033\n",
      "Iteration:  756 loss:  0.006984691135585308\n",
      "Iteration:  757 loss:  0.006608949508517981\n",
      "Iteration:  758 loss:  0.0063782949000597\n",
      "Iteration:  759 loss:  0.007944285869598389\n",
      "Iteration:  760 loss:  0.006583525333553553\n",
      "Iteration:  761 loss:  0.005062232259660959\n",
      "Iteration:  762 loss:  0.004249205347150564\n",
      "Iteration:  763 loss:  0.0055887275375425816\n",
      "Iteration:  764 loss:  0.005423125345259905\n",
      "Iteration:  765 loss:  0.004259287379682064\n",
      "Iteration:  766 loss:  0.0051483530551195145\n",
      "Iteration:  767 loss:  0.0046602291986346245\n",
      "Iteration:  768 loss:  0.005016377195715904\n",
      "Iteration:  769 loss:  0.005392947234213352\n",
      "Iteration:  770 loss:  0.006710364483296871\n",
      "Iteration:  771 loss:  0.006775441113859415\n",
      "Iteration:  772 loss:  0.006038160528987646\n",
      "Iteration:  773 loss:  0.00523794861510396\n",
      "Iteration:  774 loss:  0.005646820180118084\n",
      "Iteration:  775 loss:  0.006146311294287443\n",
      "Iteration:  776 loss:  0.006425237748771906\n",
      "Iteration:  777 loss:  0.005535149946808815\n",
      "Iteration:  778 loss:  0.005454842001199722\n",
      "Iteration:  779 loss:  0.004893026314675808\n",
      "Iteration:  780 loss:  0.005768220406025648\n",
      "Iteration:  781 loss:  0.005691587924957275\n",
      "Iteration:  782 loss:  0.005240718834102154\n",
      "Iteration:  783 loss:  0.005376193206757307\n",
      "Iteration:  784 loss:  0.004785885568708181\n",
      "Iteration:  785 loss:  0.005933754146099091\n",
      "Iteration:  786 loss:  0.005656534340232611\n",
      "Iteration:  787 loss:  0.005881716962903738\n",
      "Iteration:  788 loss:  0.005059391260147095\n",
      "Iteration:  789 loss:  0.00471915677189827\n",
      "Iteration:  790 loss:  0.005883350037038326\n",
      "Iteration:  791 loss:  0.005296166054904461\n",
      "Iteration:  792 loss:  0.004840364679694176\n",
      "Iteration:  793 loss:  0.005102065391838551\n",
      "Iteration:  794 loss:  0.005345368757843971\n",
      "Iteration:  795 loss:  0.005035315174609423\n",
      "Iteration:  796 loss:  0.007311237044632435\n",
      "Iteration:  797 loss:  0.007940981537103653\n",
      "Iteration:  798 loss:  0.009198597632348537\n",
      "Iteration:  799 loss:  0.008393955416977406\n",
      "Iteration:  800 loss:  0.008475841023027897\n",
      "Iteration:  801 loss:  0.008086616173386574\n",
      "Iteration:  802 loss:  0.008292099460959435\n",
      "Iteration:  803 loss:  0.007900933735072613\n",
      "Iteration:  804 loss:  0.0067587606608867645\n",
      "Iteration:  805 loss:  0.004230898804962635\n",
      "Iteration:  806 loss:  0.004568264354020357\n",
      "Iteration:  807 loss:  0.004395412281155586\n",
      "Iteration:  808 loss:  0.0048763747327029705\n",
      "Iteration:  809 loss:  0.004687068983912468\n",
      "Iteration:  810 loss:  0.004454144276678562\n",
      "Iteration:  811 loss:  0.004786788020282984\n",
      "Iteration:  812 loss:  0.004360098857432604\n",
      "Iteration:  813 loss:  0.004060691222548485\n",
      "Iteration:  814 loss:  0.005963783711194992\n",
      "Iteration:  815 loss:  0.005335720255970955\n",
      "Iteration:  816 loss:  0.0054539949633181095\n",
      "Iteration:  817 loss:  0.005888194777071476\n",
      "Iteration:  818 loss:  0.0064416625536978245\n",
      "Iteration:  819 loss:  0.005736779887229204\n",
      "Iteration:  820 loss:  0.005466453265398741\n",
      "Iteration:  821 loss:  0.005342561285942793\n",
      "Iteration:  822 loss:  0.006081745959818363\n",
      "Iteration:  823 loss:  0.005473238416016102\n",
      "Iteration:  824 loss:  0.006006058305501938\n",
      "Iteration:  825 loss:  0.005829330999404192\n",
      "Iteration:  826 loss:  0.0052722059190273285\n",
      "Iteration:  827 loss:  0.005735608283430338\n",
      "Iteration:  828 loss:  0.0045710220001637936\n",
      "Iteration:  829 loss:  0.005780742969363928\n",
      "Iteration:  830 loss:  0.004812557715922594\n",
      "Iteration:  831 loss:  0.010396272875368595\n",
      "Iteration:  832 loss:  0.011801550164818764\n",
      "Iteration:  833 loss:  0.010707992129027843\n",
      "Iteration:  834 loss:  0.011181119829416275\n",
      "Iteration:  835 loss:  0.010431692004203796\n",
      "Iteration:  836 loss:  0.010653526522219181\n",
      "Iteration:  837 loss:  0.011411916464567184\n",
      "Iteration:  838 loss:  0.011158746667206287\n",
      "Iteration:  839 loss:  0.010942144319415092\n",
      "Iteration:  840 loss:  0.00407502381131053\n",
      "Iteration:  841 loss:  0.005067037418484688\n",
      "Iteration:  842 loss:  0.00519908219575882\n",
      "Iteration:  843 loss:  0.0044332887046039104\n",
      "Iteration:  844 loss:  0.004326445050537586\n",
      "Iteration:  845 loss:  0.0050566005520522594\n",
      "Iteration:  846 loss:  0.004007242154330015\n",
      "Iteration:  847 loss:  0.0053566196002066135\n",
      "Iteration:  848 loss:  0.0046167802065610886\n",
      "Iteration:  849 loss:  0.006248379126191139\n",
      "Iteration:  850 loss:  0.00616647070273757\n",
      "Iteration:  851 loss:  0.0067130993120372295\n",
      "Iteration:  852 loss:  0.005729286465793848\n",
      "Iteration:  853 loss:  0.006365110632032156\n",
      "Iteration:  854 loss:  0.005718905013054609\n",
      "Iteration:  855 loss:  0.006197348237037659\n",
      "Iteration:  856 loss:  0.007003859616816044\n",
      "Iteration:  857 loss:  0.006157558411359787\n",
      "Iteration:  858 loss:  0.005396340973675251\n",
      "Iteration:  859 loss:  0.005860748700797558\n",
      "Iteration:  860 loss:  0.005968945566564798\n",
      "Iteration:  861 loss:  0.005477013997733593\n",
      "Iteration:  862 loss:  0.005975446198135614\n",
      "Iteration:  863 loss:  0.006710845045745373\n",
      "Iteration:  864 loss:  0.006102135870605707\n",
      "Iteration:  865 loss:  0.005731536075472832\n",
      "Iteration:  866 loss:  0.005070757120847702\n",
      "Iteration:  867 loss:  0.004446030128747225\n",
      "Iteration:  868 loss:  0.005273540038615465\n",
      "Iteration:  869 loss:  0.005051081068813801\n",
      "Iteration:  870 loss:  0.0051032258197665215\n",
      "Iteration:  871 loss:  0.005112826358526945\n",
      "Iteration:  872 loss:  0.004948101472109556\n",
      "Iteration:  873 loss:  0.004353624302893877\n",
      "Iteration:  874 loss:  0.00477664964273572\n",
      "Iteration:  875 loss:  0.005396944936364889\n",
      "Iteration:  876 loss:  0.005249342415481806\n",
      "Iteration:  877 loss:  0.005488497670739889\n",
      "Iteration:  878 loss:  0.005451319273561239\n",
      "Iteration:  879 loss:  0.004904778674244881\n",
      "Iteration:  880 loss:  0.005022958852350712\n",
      "Iteration:  881 loss:  0.005473707336932421\n",
      "Iteration:  882 loss:  0.005427924916148186\n",
      "Iteration:  883 loss:  0.005803717765957117\n",
      "Iteration:  884 loss:  0.0069822268560528755\n",
      "Iteration:  885 loss:  0.006290270481258631\n",
      "Iteration:  886 loss:  0.005339339841157198\n",
      "Iteration:  887 loss:  0.005609940737485886\n",
      "Iteration:  888 loss:  0.005523499567061663\n",
      "Iteration:  889 loss:  0.00641654571518302\n",
      "Iteration:  890 loss:  0.0058133299462497234\n",
      "Iteration:  891 loss:  0.006341136991977692\n",
      "Iteration:  892 loss:  0.006226804107427597\n",
      "Iteration:  893 loss:  0.005740380845963955\n",
      "Iteration:  894 loss:  0.006601918954402208\n",
      "Iteration:  895 loss:  0.006158564239740372\n",
      "Iteration:  896 loss:  0.004988676402717829\n",
      "Iteration:  897 loss:  0.005301695317029953\n",
      "Iteration:  898 loss:  0.005385594442486763\n",
      "Iteration:  899 loss:  0.004783397540450096\n",
      "Iteration:  900 loss:  0.0051881386898458\n",
      "Iteration:  901 loss:  0.005421274341642857\n",
      "Iteration:  902 loss:  0.005596564617007971\n",
      "Iteration:  903 loss:  0.00528133288025856\n",
      "Iteration:  904 loss:  0.005993309430778027\n",
      "Iteration:  905 loss:  0.005836955737322569\n",
      "Iteration:  906 loss:  0.005217427853494883\n",
      "Iteration:  907 loss:  0.005169778596609831\n",
      "Iteration:  908 loss:  0.005287306848913431\n",
      "Iteration:  909 loss:  0.005008579697459936\n",
      "Iteration:  910 loss:  0.004967055283486843\n",
      "Iteration:  911 loss:  0.005057120695710182\n",
      "Iteration:  912 loss:  0.005337096285074949\n",
      "Iteration:  913 loss:  0.004811912775039673\n",
      "Iteration:  914 loss:  0.005821090191602707\n",
      "Iteration:  915 loss:  0.006169918924570084\n",
      "Iteration:  916 loss:  0.0047749606892466545\n",
      "Iteration:  917 loss:  0.00471494859084487\n",
      "Iteration:  918 loss:  0.004451252054423094\n",
      "Iteration:  919 loss:  0.0058209397830069065\n",
      "Iteration:  920 loss:  0.005472707096487284\n",
      "Iteration:  921 loss:  0.004940893966704607\n",
      "Iteration:  922 loss:  0.005239549558609724\n",
      "Iteration:  923 loss:  0.005434261169284582\n",
      "Iteration:  924 loss:  0.00521483737975359\n",
      "Iteration:  925 loss:  0.005686171352863312\n",
      "Iteration:  926 loss:  0.00468495162203908\n",
      "Iteration:  927 loss:  0.013064239174127579\n",
      "Iteration:  928 loss:  0.02102874033153057\n",
      "Iteration:  929 loss:  0.02225605584681034\n",
      "Iteration:  930 loss:  0.01906641758978367\n",
      "Iteration:  931 loss:  0.02108480967581272\n",
      "Iteration:  932 loss:  0.021080128848552704\n",
      "Iteration:  933 loss:  0.022056251764297485\n",
      "Iteration:  934 loss:  0.017737602815032005\n",
      "Iteration:  935 loss:  0.015428583137691021\n",
      "Iteration:  936 loss:  0.008329041302204132\n",
      "Iteration:  937 loss:  0.004568967502564192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  938 loss:  0.004412137903273106\n",
      "Iteration:  939 loss:  0.004806401673704386\n",
      "Iteration:  940 loss:  0.004204676486551762\n",
      "Iteration:  941 loss:  0.004761693067848682\n",
      "Iteration:  942 loss:  0.004480896983295679\n",
      "Iteration:  943 loss:  0.003998224623501301\n",
      "Iteration:  944 loss:  0.004507186356931925\n",
      "Iteration:  945 loss:  0.004436403047293425\n",
      "Iteration:  946 loss:  0.004604442976415157\n",
      "Iteration:  947 loss:  0.004003114998340607\n",
      "Iteration:  948 loss:  0.005334876943379641\n",
      "Iteration:  949 loss:  0.005418349988758564\n",
      "Iteration:  950 loss:  0.005008318927139044\n",
      "Iteration:  951 loss:  0.004334296099841595\n",
      "Iteration:  952 loss:  0.00404643127694726\n",
      "Iteration:  953 loss:  0.005022620782256126\n",
      "Iteration:  954 loss:  0.005431867204606533\n",
      "Iteration:  955 loss:  0.006269180215895176\n",
      "Iteration:  956 loss:  0.005056276917457581\n",
      "Iteration:  957 loss:  0.005284030456095934\n",
      "Iteration:  958 loss:  0.005613848567008972\n",
      "Iteration:  959 loss:  0.0053832875564694405\n",
      "Iteration:  960 loss:  0.004778094589710236\n",
      "Iteration:  961 loss:  0.0047643776051700115\n",
      "Iteration:  962 loss:  0.005246962886303663\n",
      "Iteration:  963 loss:  0.005156286526471376\n",
      "Iteration:  964 loss:  0.005085823591798544\n",
      "Iteration:  965 loss:  0.005827122833579779\n",
      "Iteration:  966 loss:  0.005012139678001404\n",
      "Iteration:  967 loss:  0.005563550163060427\n",
      "Iteration:  968 loss:  0.004419858567416668\n",
      "Iteration:  969 loss:  0.00542314862832427\n",
      "Iteration:  970 loss:  0.005449457559734583\n",
      "Iteration:  971 loss:  0.005232110153883696\n",
      "Iteration:  972 loss:  0.004884520545601845\n",
      "Iteration:  973 loss:  0.004865699447691441\n",
      "Iteration:  974 loss:  0.005059801507741213\n",
      "Iteration:  975 loss:  0.0048269182443618774\n",
      "Iteration:  976 loss:  0.004529019817709923\n",
      "Iteration:  977 loss:  0.005456015467643738\n",
      "Iteration:  978 loss:  0.005326428450644016\n",
      "Iteration:  979 loss:  0.004467172082513571\n",
      "Iteration:  980 loss:  0.00511186383664608\n",
      "Iteration:  981 loss:  0.005256858188658953\n",
      "Iteration:  982 loss:  0.005262558814138174\n",
      "Iteration:  983 loss:  0.005452831741422415\n",
      "Iteration:  984 loss:  0.005338734481483698\n",
      "Iteration:  985 loss:  0.004538508597761393\n",
      "Iteration:  986 loss:  0.005068932194262743\n",
      "Iteration:  987 loss:  0.004754211753606796\n",
      "Iteration:  988 loss:  0.005278696771711111\n",
      "Iteration:  989 loss:  0.004868725780397654\n",
      "Iteration:  990 loss:  0.005203947424888611\n",
      "Iteration:  991 loss:  0.0048459372483193874\n",
      "Iteration:  992 loss:  0.005540637299418449\n",
      "Iteration:  993 loss:  0.005653958301991224\n",
      "Iteration:  994 loss:  0.005461914464831352\n",
      "Iteration:  995 loss:  0.005245971493422985\n",
      "Iteration:  996 loss:  0.005004895385354757\n",
      "Iteration:  997 loss:  0.005637729540467262\n",
      "Iteration:  998 loss:  0.0059041897766292095\n",
      "Iteration:  999 loss:  0.005670404061675072\n",
      "Iteration:  1000 loss:  0.006189651787281036\n",
      "Iteration:  1001 loss:  0.006497303023934364\n",
      "Iteration:  1002 loss:  0.0052949171513319016\n",
      "Iteration:  1003 loss:  0.006024116184562445\n",
      "Iteration:  1004 loss:  0.0064256866462528706\n",
      "Iteration:  1005 loss:  0.006040685344487429\n",
      "Iteration:  1006 loss:  0.0047797528095543385\n",
      "Iteration:  1007 loss:  0.004710710607469082\n",
      "Iteration:  1008 loss:  0.00473030935972929\n",
      "Iteration:  1009 loss:  0.005055358167737722\n",
      "Iteration:  1010 loss:  0.004665077198296785\n",
      "Iteration:  1011 loss:  0.004997247830033302\n",
      "Iteration:  1012 loss:  0.0046128518879413605\n",
      "Iteration:  1013 loss:  0.004651044495403767\n",
      "Iteration:  1014 loss:  0.00402388721704483\n",
      "Iteration:  1015 loss:  0.005731645040214062\n",
      "Iteration:  1016 loss:  0.005294253118336201\n",
      "Iteration:  1017 loss:  0.005282468628138304\n",
      "Iteration:  1018 loss:  0.005306197330355644\n",
      "Iteration:  1019 loss:  0.005397789645940065\n",
      "Iteration:  1020 loss:  0.006195985712110996\n",
      "Iteration:  1021 loss:  0.00503334729000926\n",
      "Iteration:  1022 loss:  0.004896464291960001\n",
      "Iteration:  1023 loss:  0.004524885211139917\n",
      "Iteration:  1024 loss:  0.005058786831796169\n",
      "Iteration:  1025 loss:  0.004195219837129116\n",
      "Iteration:  1026 loss:  0.0055349706672132015\n",
      "Iteration:  1027 loss:  0.004701629746705294\n",
      "Iteration:  1028 loss:  0.004543928895145655\n",
      "Iteration:  1029 loss:  0.004430658183991909\n",
      "Iteration:  1030 loss:  0.004909947514533997\n",
      "Iteration:  1031 loss:  0.0047957696951925755\n",
      "Iteration:  1032 loss:  0.004459125455468893\n",
      "Iteration:  1033 loss:  0.004707156680524349\n",
      "Iteration:  1034 loss:  0.004174272529780865\n",
      "Iteration:  1035 loss:  0.004258590284734964\n",
      "Iteration:  1036 loss:  0.00486320536583662\n",
      "Iteration:  1037 loss:  0.004657542333006859\n",
      "Iteration:  1038 loss:  0.004524747841060162\n",
      "Iteration:  1039 loss:  0.004994030576199293\n",
      "Iteration:  1040 loss:  0.004777468275278807\n",
      "Iteration:  1041 loss:  0.004357259254902601\n",
      "Iteration:  1042 loss:  0.0048333462327718735\n",
      "Iteration:  1043 loss:  0.004400057718157768\n",
      "Iteration:  1044 loss:  0.0038074639160186052\n",
      "Iteration:  1045 loss:  0.004409448243677616\n",
      "Iteration:  1046 loss:  0.0046058353036642075\n",
      "Iteration:  1047 loss:  0.004241171758621931\n",
      "Iteration:  1048 loss:  0.0044221035204827785\n",
      "Iteration:  1049 loss:  0.004174963571131229\n",
      "Iteration:  1050 loss:  0.005639492534101009\n",
      "Iteration:  1051 loss:  0.005543871317058802\n",
      "Iteration:  1052 loss:  0.005272488109767437\n",
      "Iteration:  1053 loss:  0.00510761421173811\n",
      "Iteration:  1054 loss:  0.004927932284772396\n",
      "Iteration:  1055 loss:  0.005031056236475706\n",
      "Iteration:  1056 loss:  0.005886604078114033\n",
      "Iteration:  1057 loss:  0.005198659375309944\n",
      "Iteration:  1058 loss:  0.005521784070879221\n",
      "Iteration:  1059 loss:  0.006619940046221018\n",
      "Iteration:  1060 loss:  0.005498990416526794\n",
      "Iteration:  1061 loss:  0.0049572293646633625\n",
      "Iteration:  1062 loss:  0.005209113005548716\n",
      "Iteration:  1063 loss:  0.005144719500094652\n",
      "Iteration:  1064 loss:  0.005717230495065451\n",
      "Iteration:  1065 loss:  0.005023632198572159\n",
      "Iteration:  1066 loss:  0.005492924712598324\n",
      "Iteration:  1067 loss:  0.00613154424354434\n",
      "Iteration:  1068 loss:  0.007609175052493811\n",
      "Iteration:  1069 loss:  0.007676420267671347\n",
      "Iteration:  1070 loss:  0.008111845701932907\n",
      "Iteration:  1071 loss:  0.007746358402073383\n",
      "Iteration:  1072 loss:  0.007330570835620165\n",
      "Iteration:  1073 loss:  0.007536170072853565\n",
      "Iteration:  1074 loss:  0.007457385770976543\n",
      "Iteration:  1075 loss:  0.00779911782592535\n",
      "Iteration:  1076 loss:  0.006391268223524094\n",
      "Iteration:  1077 loss:  0.005306988954544067\n",
      "Iteration:  1078 loss:  0.005527048837393522\n",
      "Iteration:  1079 loss:  0.005084804724901915\n",
      "Iteration:  1080 loss:  0.005927870981395245\n",
      "Iteration:  1081 loss:  0.005246841814368963\n",
      "Iteration:  1082 loss:  0.00542922830209136\n",
      "Iteration:  1083 loss:  0.004878395237028599\n",
      "Iteration:  1084 loss:  0.005427778698503971\n",
      "Iteration:  1085 loss:  0.004176871385425329\n",
      "Iteration:  1086 loss:  0.0045358166098594666\n",
      "Iteration:  1087 loss:  0.004434109199792147\n",
      "Iteration:  1088 loss:  0.004268130753189325\n",
      "Iteration:  1089 loss:  0.003906723577529192\n",
      "Iteration:  1090 loss:  0.00425731623545289\n",
      "Iteration:  1091 loss:  0.004545486532151699\n",
      "Iteration:  1092 loss:  0.004356177058070898\n",
      "Iteration:  1093 loss:  0.00480975117534399\n",
      "Iteration:  1094 loss:  0.004506233613938093\n",
      "Iteration:  1095 loss:  0.004042749758809805\n",
      "Iteration:  1096 loss:  0.004198669455945492\n",
      "Iteration:  1097 loss:  0.0042543960735201836\n",
      "Iteration:  1098 loss:  0.0043480657041072845\n",
      "Iteration:  1099 loss:  0.004656829871237278\n",
      "Iteration:  1100 loss:  0.004284266848117113\n",
      "Iteration:  1101 loss:  0.004137819167226553\n",
      "Iteration:  1102 loss:  0.004360243212431669\n",
      "Iteration:  1103 loss:  0.00628122640773654\n",
      "Iteration:  1104 loss:  0.005594540853053331\n",
      "Iteration:  1105 loss:  0.004813842009752989\n",
      "Iteration:  1106 loss:  0.005584283731877804\n",
      "Iteration:  1107 loss:  0.0056240432895720005\n",
      "Iteration:  1108 loss:  0.005748117808252573\n",
      "Iteration:  1109 loss:  0.005232441239058971\n",
      "Iteration:  1110 loss:  0.005434451159089804\n",
      "Iteration:  1111 loss:  0.004992131609469652\n",
      "Iteration:  1112 loss:  0.006083805579692125\n",
      "Iteration:  1113 loss:  0.00543335871770978\n",
      "Iteration:  1114 loss:  0.005331660620868206\n",
      "Iteration:  1115 loss:  0.005720216315239668\n",
      "Iteration:  1116 loss:  0.005707697942852974\n",
      "Iteration:  1117 loss:  0.005477621220052242\n",
      "Iteration:  1118 loss:  0.0054627154022455215\n",
      "Iteration:  1119 loss:  0.004809459671378136\n",
      "Iteration:  1120 loss:  0.00536163616925478\n",
      "Iteration:  1121 loss:  0.005721220746636391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1122 loss:  0.005827504675835371\n",
      "Iteration:  1123 loss:  0.005372158717364073\n",
      "Iteration:  1124 loss:  0.004702461417764425\n",
      "Iteration:  1125 loss:  0.005832904949784279\n",
      "Iteration:  1126 loss:  0.004942663945257664\n",
      "Iteration:  1127 loss:  0.0051267193630337715\n",
      "Iteration:  1128 loss:  0.00498018367215991\n",
      "Iteration:  1129 loss:  0.005160836037248373\n",
      "Iteration:  1130 loss:  0.004546227399259806\n",
      "Iteration:  1131 loss:  0.0051965597085654736\n",
      "Iteration:  1132 loss:  0.0045736064203083515\n",
      "Iteration:  1133 loss:  0.0047906991094350815\n",
      "Iteration:  1134 loss:  0.004620113410055637\n",
      "Iteration:  1135 loss:  0.004311799071729183\n",
      "Iteration:  1136 loss:  0.0048425025306642056\n",
      "Iteration:  1137 loss:  0.004353114869445562\n",
      "Iteration:  1138 loss:  0.005107648205012083\n",
      "Iteration:  1139 loss:  0.004843330010771751\n",
      "Iteration:  1140 loss:  0.004959953483194113\n",
      "Iteration:  1141 loss:  0.004664942622184753\n",
      "Iteration:  1142 loss:  0.004275650717318058\n",
      "Iteration:  1143 loss:  0.003901863005012274\n",
      "Iteration:  1144 loss:  0.004480364266782999\n",
      "Iteration:  1145 loss:  0.004765443503856659\n",
      "Iteration:  1146 loss:  0.0050408560782670975\n",
      "Iteration:  1147 loss:  0.005401368252933025\n",
      "Iteration:  1148 loss:  0.005333391018211842\n",
      "Iteration:  1149 loss:  0.004749105777591467\n",
      "Iteration:  1150 loss:  0.00522638252004981\n",
      "Iteration:  1151 loss:  0.005831596441566944\n",
      "Iteration:  1152 loss:  0.005059961229562759\n",
      "Iteration:  1153 loss:  0.004958218429237604\n",
      "Iteration:  1154 loss:  0.0051657939329743385\n",
      "Iteration:  1155 loss:  0.004857806023210287\n",
      "Iteration:  1156 loss:  0.00446079159155488\n",
      "Iteration:  1157 loss:  0.004851134028285742\n",
      "Iteration:  1158 loss:  0.00472702132537961\n",
      "Iteration:  1159 loss:  0.004285567440092564\n",
      "Iteration:  1160 loss:  0.004258321598172188\n",
      "Iteration:  1161 loss:  0.004807494580745697\n",
      "Iteration:  1162 loss:  0.004138479474931955\n",
      "Iteration:  1163 loss:  0.0039962404407560825\n",
      "Iteration:  1164 loss:  0.0043053762055933475\n",
      "Iteration:  1165 loss:  0.004606150556355715\n",
      "Iteration:  1166 loss:  0.0048014456406235695\n",
      "Iteration:  1167 loss:  0.004416061565279961\n",
      "Iteration:  1168 loss:  0.0037076270673424006\n",
      "Iteration:  1169 loss:  0.004650701768696308\n",
      "Iteration:  1170 loss:  0.0044179195538163185\n",
      "Iteration:  1171 loss:  0.004480721428990364\n",
      "Iteration:  1172 loss:  0.005122839473187923\n",
      "Iteration:  1173 loss:  0.004174843430519104\n",
      "Iteration:  1174 loss:  0.004330710973590612\n",
      "Iteration:  1175 loss:  0.004357820842415094\n",
      "Iteration:  1176 loss:  0.004788755439221859\n",
      "Iteration:  1177 loss:  0.00502143707126379\n",
      "Iteration:  1178 loss:  0.004729116801172495\n",
      "Iteration:  1179 loss:  0.004383919294923544\n",
      "Iteration:  1180 loss:  0.0046186139807105064\n",
      "Iteration:  1181 loss:  0.006319019943475723\n",
      "Iteration:  1182 loss:  0.008023020811378956\n",
      "Iteration:  1183 loss:  0.006478094030171633\n",
      "Iteration:  1184 loss:  0.006984473671764135\n",
      "Iteration:  1185 loss:  0.005903729237616062\n",
      "Iteration:  1186 loss:  0.006346283946186304\n",
      "Iteration:  1187 loss:  0.0068819038569927216\n",
      "Iteration:  1188 loss:  0.006988517940044403\n",
      "Iteration:  1189 loss:  0.00604987982660532\n",
      "Iteration:  1190 loss:  0.005437958519905806\n",
      "Iteration:  1191 loss:  0.004842653404921293\n",
      "Iteration:  1192 loss:  0.005043912213295698\n",
      "Iteration:  1193 loss:  0.005066125188022852\n",
      "Iteration:  1194 loss:  0.005659261252731085\n",
      "Iteration:  1195 loss:  0.005802614148706198\n",
      "Iteration:  1196 loss:  0.005167316645383835\n",
      "Iteration:  1197 loss:  0.005508861038833857\n",
      "Iteration:  1198 loss:  0.004785566125065088\n",
      "Iteration:  1199 loss:  0.004712230060249567\n",
      "Iteration:  1200 loss:  0.004879757761955261\n",
      "Iteration:  1201 loss:  0.004740351345390081\n",
      "Iteration:  1202 loss:  0.005240307655185461\n",
      "Iteration:  1203 loss:  0.004454546608030796\n",
      "Iteration:  1204 loss:  0.004135957453399897\n",
      "Iteration:  1205 loss:  0.0046133240684866905\n",
      "Iteration:  1206 loss:  0.004654633812606335\n",
      "Iteration:  1207 loss:  0.005308692809194326\n",
      "Iteration:  1208 loss:  0.006735701579600573\n",
      "Iteration:  1209 loss:  0.006096540484577417\n",
      "Iteration:  1210 loss:  0.0065296790562570095\n",
      "Iteration:  1211 loss:  0.007205132395029068\n",
      "Iteration:  1212 loss:  0.006040183361619711\n",
      "Iteration:  1213 loss:  0.005928993225097656\n",
      "Iteration:  1214 loss:  0.0067352778278291225\n",
      "Iteration:  1215 loss:  0.00568298390135169\n",
      "Iteration:  1216 loss:  0.006412934977561235\n",
      "Iteration:  1217 loss:  0.004713807720690966\n",
      "Iteration:  1218 loss:  0.005716106854379177\n",
      "Iteration:  1219 loss:  0.004945515654981136\n",
      "Iteration:  1220 loss:  0.004197045229375362\n",
      "Iteration:  1221 loss:  0.00526409363374114\n",
      "Iteration:  1222 loss:  0.005695970728993416\n",
      "Iteration:  1223 loss:  0.0058073061518371105\n",
      "Iteration:  1224 loss:  0.005053915549069643\n",
      "Iteration:  1225 loss:  0.005756204016506672\n",
      "Iteration:  1226 loss:  0.007272622082382441\n",
      "Iteration:  1227 loss:  0.005607874598354101\n",
      "Iteration:  1228 loss:  0.006381595972925425\n",
      "Iteration:  1229 loss:  0.006608802359551191\n",
      "Iteration:  1230 loss:  0.006409392226487398\n",
      "Iteration:  1231 loss:  0.005351286847144365\n",
      "Iteration:  1232 loss:  0.005986794363707304\n",
      "Iteration:  1233 loss:  0.005910682026296854\n",
      "Iteration:  1234 loss:  0.005380452610552311\n",
      "Iteration:  1235 loss:  0.006308235228061676\n",
      "Iteration:  1236 loss:  0.005998487584292889\n",
      "Iteration:  1237 loss:  0.004662951920181513\n",
      "Iteration:  1238 loss:  0.00527622364461422\n",
      "Iteration:  1239 loss:  0.005170222371816635\n",
      "Iteration:  1240 loss:  0.004975215531885624\n",
      "Iteration:  1241 loss:  0.004703681915998459\n",
      "Iteration:  1242 loss:  0.005776531528681517\n",
      "Iteration:  1243 loss:  0.004900628700852394\n",
      "Iteration:  1244 loss:  0.0052153985016047955\n",
      "Iteration:  1245 loss:  0.005351628642529249\n",
      "Iteration:  1246 loss:  0.005303886719048023\n",
      "Iteration:  1247 loss:  0.003798210760578513\n",
      "Iteration:  1248 loss:  0.004457567818462849\n",
      "Iteration:  1249 loss:  0.005706107709556818\n",
      "Iteration:  1250 loss:  0.005047373473644257\n",
      "Iteration:  1251 loss:  0.005325989332050085\n",
      "Iteration:  1252 loss:  0.004893039353191853\n",
      "Iteration:  1253 loss:  0.004985825624316931\n",
      "Iteration:  1254 loss:  0.006165884435176849\n",
      "Iteration:  1255 loss:  0.004064952023327351\n",
      "Iteration:  1256 loss:  0.004750054795295\n",
      "Iteration:  1257 loss:  0.004843574482947588\n",
      "Iteration:  1258 loss:  0.00534073356539011\n",
      "Iteration:  1259 loss:  0.004274727776646614\n",
      "Iteration:  1260 loss:  0.009343988262116909\n",
      "Iteration:  1261 loss:  0.009547102265059948\n",
      "Iteration:  1262 loss:  0.007759084925055504\n",
      "Iteration:  1263 loss:  0.008122842758893967\n",
      "Iteration:  1264 loss:  0.00884153414517641\n",
      "Iteration:  1265 loss:  0.009164159186184406\n",
      "Iteration:  1266 loss:  0.007330453488975763\n",
      "Iteration:  1267 loss:  0.007646198384463787\n",
      "Iteration:  1268 loss:  0.007328165229409933\n",
      "Iteration:  1269 loss:  0.004417656920850277\n",
      "Iteration:  1270 loss:  0.005114179104566574\n",
      "Iteration:  1271 loss:  0.004180226940661669\n",
      "Iteration:  1272 loss:  0.004280195105820894\n",
      "Iteration:  1273 loss:  0.004584751557558775\n",
      "Iteration:  1274 loss:  0.00382795138284564\n",
      "Iteration:  1275 loss:  0.004387022461742163\n",
      "Iteration:  1276 loss:  0.0046987999230623245\n",
      "Iteration:  1277 loss:  0.004831986501812935\n",
      "Iteration:  1278 loss:  0.004753770772367716\n",
      "Iteration:  1279 loss:  0.005496142897754908\n",
      "Iteration:  1280 loss:  0.0054356553591787815\n",
      "Iteration:  1281 loss:  0.005632652435451746\n",
      "Iteration:  1282 loss:  0.005629316903650761\n",
      "Iteration:  1283 loss:  0.004937099292874336\n",
      "Iteration:  1284 loss:  0.005578066688030958\n",
      "Iteration:  1285 loss:  0.005104281939566135\n",
      "Iteration:  1286 loss:  0.005627944599837065\n",
      "Iteration:  1287 loss:  0.006160561926662922\n",
      "Iteration:  1288 loss:  0.005028028506785631\n",
      "Iteration:  1289 loss:  0.005652203224599361\n",
      "Iteration:  1290 loss:  0.0055284942500293255\n",
      "Iteration:  1291 loss:  0.005896499380469322\n",
      "Iteration:  1292 loss:  0.005961849354207516\n",
      "Iteration:  1293 loss:  0.005945451091974974\n",
      "Iteration:  1294 loss:  0.0051552243530750275\n",
      "Iteration:  1295 loss:  0.005913975648581982\n",
      "Iteration:  1296 loss:  0.005341651383787394\n",
      "Iteration:  1297 loss:  0.00548629742115736\n",
      "Iteration:  1298 loss:  0.0054094078950583935\n",
      "Iteration:  1299 loss:  0.006066963542252779\n",
      "Iteration:  1300 loss:  0.00569175137206912\n",
      "Iteration:  1301 loss:  0.005329309031367302\n",
      "Iteration:  1302 loss:  0.005501277279108763\n",
      "Iteration:  1303 loss:  0.005084956530481577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1304 loss:  0.0044522956013679504\n",
      "Iteration:  1305 loss:  0.005247337743639946\n",
      "Iteration:  1306 loss:  0.00431102467700839\n",
      "Iteration:  1307 loss:  0.00435209134593606\n",
      "Iteration:  1308 loss:  0.004572991281747818\n",
      "Iteration:  1309 loss:  0.004465843550860882\n",
      "Iteration:  1310 loss:  0.004885513335466385\n",
      "Iteration:  1311 loss:  0.004563549067825079\n",
      "Iteration:  1312 loss:  0.00523752486333251\n",
      "Iteration:  1313 loss:  0.006544629577547312\n",
      "Iteration:  1314 loss:  0.0055253575555980206\n",
      "Iteration:  1315 loss:  0.006303579080849886\n",
      "Iteration:  1316 loss:  0.005489414557814598\n",
      "Iteration:  1317 loss:  0.005898351315408945\n",
      "Iteration:  1318 loss:  0.005702303256839514\n",
      "Iteration:  1319 loss:  0.005842522252351046\n",
      "Iteration:  1320 loss:  0.00596311641857028\n",
      "Iteration:  1321 loss:  0.00499375443905592\n",
      "Iteration:  1322 loss:  0.004344250541180372\n",
      "Iteration:  1323 loss:  0.004132736474275589\n",
      "Iteration:  1324 loss:  0.0041932775638997555\n",
      "Iteration:  1325 loss:  0.004637745674699545\n",
      "Iteration:  1326 loss:  0.004161220509558916\n",
      "Iteration:  1327 loss:  0.004738973453640938\n",
      "Iteration:  1328 loss:  0.004374254960566759\n",
      "Iteration:  1329 loss:  0.004589824005961418\n",
      "Iteration:  1330 loss:  0.005519421771168709\n",
      "Iteration:  1331 loss:  0.005358921363949776\n",
      "Iteration:  1332 loss:  0.004921555519104004\n",
      "Iteration:  1333 loss:  0.0049355472438037395\n",
      "Iteration:  1334 loss:  0.005935823544859886\n",
      "Iteration:  1335 loss:  0.005071681458503008\n",
      "Iteration:  1336 loss:  0.004900956526398659\n",
      "Iteration:  1337 loss:  0.0048797656781971455\n",
      "Iteration:  1338 loss:  0.005462757311761379\n",
      "Iteration:  1339 loss:  0.0038654387462884188\n",
      "Iteration:  1340 loss:  0.0049057696014642715\n",
      "Iteration:  1341 loss:  0.004953162278980017\n",
      "Iteration:  1342 loss:  0.004046267829835415\n",
      "Iteration:  1343 loss:  0.004801808390766382\n",
      "Iteration:  1344 loss:  0.0038590061012655497\n",
      "Iteration:  1345 loss:  0.00517850648611784\n",
      "Iteration:  1346 loss:  0.00457639480009675\n",
      "Iteration:  1347 loss:  0.004797595087438822\n",
      "Iteration:  1348 loss:  0.005483267828822136\n",
      "Iteration:  1349 loss:  0.00505912397056818\n",
      "Iteration:  1350 loss:  0.004993423353880644\n",
      "Iteration:  1351 loss:  0.005226144101470709\n",
      "Iteration:  1352 loss:  0.005270257592201233\n",
      "Iteration:  1353 loss:  0.0053033120930194855\n",
      "Iteration:  1354 loss:  0.005309673026204109\n",
      "Iteration:  1355 loss:  0.005046067759394646\n",
      "Iteration:  1356 loss:  0.004783129319548607\n",
      "Iteration:  1357 loss:  0.004966523963958025\n",
      "Iteration:  1358 loss:  0.0038587215822190046\n",
      "Iteration:  1359 loss:  0.004684507381170988\n",
      "Iteration:  1360 loss:  0.00402360875159502\n",
      "Iteration:  1361 loss:  0.004077543038874865\n",
      "Iteration:  1362 loss:  0.004238578490912914\n",
      "Iteration:  1363 loss:  0.004042194224894047\n",
      "Iteration:  1364 loss:  0.004505590535700321\n",
      "Iteration:  1365 loss:  0.005704197566956282\n",
      "Iteration:  1366 loss:  0.005572841502726078\n",
      "Iteration:  1367 loss:  0.00467437831684947\n",
      "Iteration:  1368 loss:  0.005850371439009905\n",
      "Iteration:  1369 loss:  0.005110081285238266\n",
      "Iteration:  1370 loss:  0.005112696439027786\n",
      "Iteration:  1371 loss:  0.004466342739760876\n",
      "Iteration:  1372 loss:  0.005362049676477909\n",
      "Iteration:  1373 loss:  0.005666758399456739\n",
      "Iteration:  1374 loss:  0.005965879186987877\n",
      "Iteration:  1375 loss:  0.005665089935064316\n",
      "Iteration:  1376 loss:  0.006378598045557737\n",
      "Iteration:  1377 loss:  0.00576375238597393\n",
      "Iteration:  1378 loss:  0.005640303250402212\n",
      "Iteration:  1379 loss:  0.005545515567064285\n",
      "Iteration:  1380 loss:  0.005323959048837423\n",
      "Iteration:  1381 loss:  0.00556938536465168\n",
      "Iteration:  1382 loss:  0.004741072654724121\n",
      "Iteration:  1383 loss:  0.00444061029702425\n",
      "Iteration:  1384 loss:  0.004502980969846249\n",
      "Iteration:  1385 loss:  0.0045714532025158405\n",
      "Iteration:  1386 loss:  0.00419682078063488\n",
      "Iteration:  1387 loss:  0.004455631598830223\n",
      "Iteration:  1388 loss:  0.004178162664175034\n",
      "Iteration:  1389 loss:  0.004552801139652729\n",
      "Iteration:  1390 loss:  0.004140965640544891\n",
      "Iteration:  1391 loss:  0.00471925875172019\n",
      "Iteration:  1392 loss:  0.005704111885279417\n",
      "Iteration:  1393 loss:  0.005350390449166298\n",
      "Iteration:  1394 loss:  0.005659638438373804\n",
      "Iteration:  1395 loss:  0.005875872913748026\n",
      "Iteration:  1396 loss:  0.004845018498599529\n",
      "Iteration:  1397 loss:  0.005130239296704531\n",
      "Iteration:  1398 loss:  0.0054704356007277966\n",
      "Iteration:  1399 loss:  0.0047415075823664665\n",
      "Iteration:  1400 loss:  0.0069587030448019505\n",
      "Iteration:  1401 loss:  0.00730796018615365\n",
      "Iteration:  1402 loss:  0.006433379836380482\n",
      "Iteration:  1403 loss:  0.008401626721024513\n",
      "Iteration:  1404 loss:  0.00695807533338666\n",
      "Iteration:  1405 loss:  0.006239708047360182\n",
      "Iteration:  1406 loss:  0.005406389012932777\n",
      "Iteration:  1407 loss:  0.00690761674195528\n",
      "Iteration:  1408 loss:  0.006779442075639963\n",
      "Iteration:  1409 loss:  0.005673468112945557\n",
      "Iteration:  1410 loss:  0.005208798684179783\n",
      "Iteration:  1411 loss:  0.005263943690806627\n",
      "Iteration:  1412 loss:  0.0060820309445261955\n",
      "Iteration:  1413 loss:  0.00575496070086956\n",
      "Iteration:  1414 loss:  0.005552713759243488\n",
      "Iteration:  1415 loss:  0.004521307535469532\n",
      "Iteration:  1416 loss:  0.004968954250216484\n",
      "Iteration:  1417 loss:  0.004977939650416374\n",
      "Iteration:  1418 loss:  0.00458861468359828\n",
      "Iteration:  1419 loss:  0.005221988074481487\n",
      "Iteration:  1420 loss:  0.005233239848166704\n",
      "Iteration:  1421 loss:  0.005442781839519739\n",
      "Iteration:  1422 loss:  0.00501596974208951\n",
      "Iteration:  1423 loss:  0.00481795147061348\n",
      "Iteration:  1424 loss:  0.005476849619299173\n",
      "Iteration:  1425 loss:  0.005214375909417868\n",
      "Iteration:  1426 loss:  0.004907545167952776\n",
      "Iteration:  1427 loss:  0.003800555830821395\n",
      "Iteration:  1428 loss:  0.004721769597381353\n",
      "Iteration:  1429 loss:  0.004501334857195616\n",
      "Iteration:  1430 loss:  0.0052759102545678616\n",
      "Iteration:  1431 loss:  0.005182855296880007\n",
      "Iteration:  1432 loss:  0.004114112351089716\n",
      "Iteration:  1433 loss:  0.005621486343443394\n",
      "Iteration:  1434 loss:  0.004401402547955513\n",
      "Iteration:  1435 loss:  0.004606467206031084\n",
      "Iteration:  1436 loss:  0.005620855372399092\n",
      "Iteration:  1437 loss:  0.005448467098176479\n",
      "Iteration:  1438 loss:  0.005140590947121382\n",
      "Iteration:  1439 loss:  0.006053719203919172\n",
      "Iteration:  1440 loss:  0.004846810828894377\n",
      "Iteration:  1441 loss:  0.005040367133915424\n",
      "Iteration:  1442 loss:  0.004401530604809523\n",
      "Iteration:  1443 loss:  0.00547747639939189\n",
      "Iteration:  1444 loss:  0.00529404217377305\n",
      "Iteration:  1445 loss:  0.005176504608243704\n",
      "Iteration:  1446 loss:  0.005076197441667318\n",
      "Iteration:  1447 loss:  0.005624128505587578\n",
      "Iteration:  1448 loss:  0.004693368449807167\n",
      "Iteration:  1449 loss:  0.00554538844153285\n",
      "Iteration:  1450 loss:  0.005188964772969484\n",
      "Iteration:  1451 loss:  0.004618080798536539\n",
      "Iteration:  1452 loss:  0.0044196853414177895\n",
      "Iteration:  1453 loss:  0.0057156020775437355\n",
      "Iteration:  1454 loss:  0.004920500796288252\n",
      "Iteration:  1455 loss:  0.005191103555262089\n",
      "Iteration:  1456 loss:  0.00465663056820631\n",
      "Iteration:  1457 loss:  0.004927704576402903\n",
      "Iteration:  1458 loss:  0.004827779717743397\n",
      "Iteration:  1459 loss:  0.005119201727211475\n",
      "Iteration:  1460 loss:  0.005255999509245157\n",
      "Iteration:  1461 loss:  0.005099428351968527\n",
      "Iteration:  1462 loss:  0.004465372301638126\n",
      "Iteration:  1463 loss:  0.004690684378147125\n",
      "Iteration:  1464 loss:  0.004980653524398804\n",
      "Iteration:  1465 loss:  0.005033410619944334\n",
      "Iteration:  1466 loss:  0.004333116579800844\n",
      "Iteration:  1467 loss:  0.004741362761706114\n",
      "Iteration:  1468 loss:  0.00517877796664834\n",
      "Iteration:  1469 loss:  0.004767849575728178\n",
      "Iteration:  1470 loss:  0.006866433657705784\n",
      "Iteration:  1471 loss:  0.005518293473869562\n",
      "Iteration:  1472 loss:  0.005523212254047394\n",
      "Iteration:  1473 loss:  0.004788524005562067\n",
      "Iteration:  1474 loss:  0.006776844151318073\n",
      "Iteration:  1475 loss:  0.0058195870369672775\n",
      "Iteration:  1476 loss:  0.005321106873452663\n",
      "Iteration:  1477 loss:  0.005516550503671169\n",
      "Iteration:  1478 loss:  0.006028137169778347\n",
      "Iteration:  1479 loss:  0.004437731578946114\n",
      "Iteration:  1480 loss:  0.00429154746234417\n",
      "Iteration:  1481 loss:  0.0049357423558831215\n",
      "Iteration:  1482 loss:  0.004347644746303558\n",
      "Iteration:  1483 loss:  0.004513480234891176\n",
      "Iteration:  1484 loss:  0.00434793159365654\n",
      "Iteration:  1485 loss:  0.004417732823640108\n",
      "Iteration:  1486 loss:  0.004515205509960651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1487 loss:  0.004674811847507954\n",
      "Iteration:  1488 loss:  0.0040555233135819435\n",
      "Iteration:  1489 loss:  0.004445288795977831\n",
      "Iteration:  1490 loss:  0.004490839317440987\n",
      "Iteration:  1491 loss:  0.004573856946080923\n",
      "Iteration:  1492 loss:  0.003965913318097591\n",
      "Iteration:  1493 loss:  0.004745527170598507\n",
      "Iteration:  1494 loss:  0.004266447853296995\n",
      "Iteration:  1495 loss:  0.004215918015688658\n",
      "Iteration:  1496 loss:  0.004487138241529465\n",
      "Iteration:  1497 loss:  0.004021420143544674\n",
      "Iteration:  1498 loss:  0.00474253436550498\n",
      "Iteration:  1499 loss:  0.00471467012539506\n",
      "Iteration:  1500 loss:  0.003997893538326025\n",
      "Iteration:  1501 loss:  0.00424911780282855\n",
      "Iteration:  1502 loss:  0.004646847024559975\n",
      "Iteration:  1503 loss:  0.004721388686448336\n",
      "Iteration:  1504 loss:  0.004556465428322554\n",
      "Iteration:  1505 loss:  0.005826473701745272\n",
      "Iteration:  1506 loss:  0.004798418842256069\n",
      "Iteration:  1507 loss:  0.0057614692486822605\n",
      "Iteration:  1508 loss:  0.006104787811636925\n",
      "Iteration:  1509 loss:  0.004687865264713764\n",
      "Iteration:  1510 loss:  0.005130448378622532\n",
      "Iteration:  1511 loss:  0.005215649493038654\n",
      "Iteration:  1512 loss:  0.005139744840562344\n",
      "Iteration:  1513 loss:  0.005414123646914959\n",
      "Iteration:  1514 loss:  0.005592139437794685\n",
      "Iteration:  1515 loss:  0.0050972537137568\n",
      "Iteration:  1516 loss:  0.005037473514676094\n",
      "Iteration:  1517 loss:  0.005259138997644186\n",
      "Iteration:  1518 loss:  0.005011678673326969\n",
      "Iteration:  1519 loss:  0.004879198502749205\n",
      "Iteration:  1520 loss:  0.0052294861525297165\n",
      "Iteration:  1521 loss:  0.005161972250789404\n",
      "Iteration:  1522 loss:  0.00508256908506155\n",
      "Iteration:  1523 loss:  0.005296328105032444\n",
      "Iteration:  1524 loss:  0.004955065902322531\n",
      "Iteration:  1525 loss:  0.005280951038002968\n",
      "Iteration:  1526 loss:  0.004979638382792473\n",
      "Iteration:  1527 loss:  0.004853493999689817\n",
      "Iteration:  1528 loss:  0.004720866680145264\n",
      "Iteration:  1529 loss:  0.005279533565044403\n",
      "Iteration:  1530 loss:  0.004781159572303295\n",
      "Iteration:  1531 loss:  0.0042699663899838924\n",
      "Iteration:  1532 loss:  0.004478693474084139\n",
      "Iteration:  1533 loss:  0.00513916090130806\n",
      "Iteration:  1534 loss:  0.0051529486663639545\n",
      "Iteration:  1535 loss:  0.005369315855205059\n",
      "Iteration:  1536 loss:  0.004879796411842108\n",
      "Iteration:  1537 loss:  0.005352117121219635\n",
      "Iteration:  1538 loss:  0.005344766192138195\n",
      "Iteration:  1539 loss:  0.0047535886988043785\n",
      "Iteration:  1540 loss:  0.007006481755524874\n",
      "Iteration:  1541 loss:  0.006444383412599564\n",
      "Iteration:  1542 loss:  0.006622820161283016\n",
      "Iteration:  1543 loss:  0.006418372038751841\n",
      "Iteration:  1544 loss:  0.006480940151959658\n",
      "Iteration:  1545 loss:  0.0067669907584786415\n",
      "Iteration:  1546 loss:  0.006249679252505302\n",
      "Iteration:  1547 loss:  0.005734219215810299\n",
      "Iteration:  1548 loss:  0.005968183279037476\n",
      "Iteration:  1549 loss:  0.005315416492521763\n",
      "Iteration:  1550 loss:  0.005597114097326994\n",
      "Iteration:  1551 loss:  0.0048003969714045525\n",
      "Iteration:  1552 loss:  0.004414981696754694\n",
      "Iteration:  1553 loss:  0.004292683210223913\n",
      "Iteration:  1554 loss:  0.005067036487162113\n",
      "Iteration:  1555 loss:  0.004694046452641487\n",
      "Iteration:  1556 loss:  0.005393043160438538\n",
      "Iteration:  1557 loss:  0.0054734814912080765\n",
      "Iteration:  1558 loss:  0.006082175299525261\n",
      "Iteration:  1559 loss:  0.004465803969651461\n",
      "Iteration:  1560 loss:  0.0054475245997309685\n",
      "Iteration:  1561 loss:  0.004492004867643118\n",
      "Iteration:  1562 loss:  0.005111347883939743\n",
      "Iteration:  1563 loss:  0.0050757997669279575\n",
      "Iteration:  1564 loss:  0.0051000602543354034\n",
      "Iteration:  1565 loss:  0.005517500918358564\n",
      "Iteration:  1566 loss:  0.007855621166527271\n",
      "Iteration:  1567 loss:  0.006251185666769743\n",
      "Iteration:  1568 loss:  0.009063691832125187\n",
      "Iteration:  1569 loss:  0.008295086212456226\n",
      "Iteration:  1570 loss:  0.007147554308176041\n",
      "Iteration:  1571 loss:  0.007658251095563173\n",
      "Iteration:  1572 loss:  0.007341854274272919\n",
      "Iteration:  1573 loss:  0.00801640935242176\n",
      "Iteration:  1574 loss:  0.0075740511529147625\n",
      "Iteration:  1575 loss:  0.004939217120409012\n",
      "Iteration:  1576 loss:  0.005076203029602766\n",
      "Iteration:  1577 loss:  0.004518457688391209\n",
      "Iteration:  1578 loss:  0.00414555286988616\n",
      "Iteration:  1579 loss:  0.004715512506663799\n",
      "Iteration:  1580 loss:  0.00475810281932354\n",
      "Iteration:  1581 loss:  0.004298386629670858\n",
      "Iteration:  1582 loss:  0.005690670572221279\n",
      "Iteration:  1583 loss:  0.004220370668917894\n",
      "Iteration:  1584 loss:  0.005537307821214199\n",
      "Iteration:  1585 loss:  0.006330382078886032\n",
      "Iteration:  1586 loss:  0.0067837173119187355\n",
      "Iteration:  1587 loss:  0.005379403475672007\n",
      "Iteration:  1588 loss:  0.00581710459664464\n",
      "Iteration:  1589 loss:  0.005510673858225346\n",
      "Iteration:  1590 loss:  0.00598008930683136\n",
      "Iteration:  1591 loss:  0.005343833472579718\n",
      "Iteration:  1592 loss:  0.005384224932640791\n",
      "Iteration:  1593 loss:  0.005409224424511194\n",
      "Iteration:  1594 loss:  0.004832988604903221\n",
      "Iteration:  1595 loss:  0.004902546294033527\n",
      "Iteration:  1596 loss:  0.0058356537483632565\n",
      "Iteration:  1597 loss:  0.005170076619833708\n",
      "Iteration:  1598 loss:  0.00501917814835906\n",
      "Iteration:  1599 loss:  0.0051709190011024475\n",
      "Iteration:  1600 loss:  0.004900874570012093\n",
      "Iteration:  1601 loss:  0.004921623971313238\n",
      "Iteration:  1602 loss:  0.00465305894613266\n",
      "Iteration:  1603 loss:  0.004282268229871988\n",
      "Iteration:  1604 loss:  0.0047475844621658325\n",
      "Iteration:  1605 loss:  0.00476407166570425\n",
      "Iteration:  1606 loss:  0.004880047403275967\n",
      "Iteration:  1607 loss:  0.004289898555725813\n",
      "Iteration:  1608 loss:  0.004393136128783226\n",
      "Iteration:  1609 loss:  0.0041420953348279\n",
      "Iteration:  1610 loss:  0.009102346375584602\n",
      "Iteration:  1611 loss:  0.01083541288971901\n",
      "Iteration:  1612 loss:  0.011610986664891243\n",
      "Iteration:  1613 loss:  0.010440994054079056\n",
      "Iteration:  1614 loss:  0.009909756481647491\n",
      "Iteration:  1615 loss:  0.010233328677713871\n",
      "Iteration:  1616 loss:  0.010154198855161667\n",
      "Iteration:  1617 loss:  0.009771928191184998\n",
      "Iteration:  1618 loss:  0.0065551334992051125\n",
      "Iteration:  1619 loss:  0.00609208457171917\n",
      "Iteration:  1620 loss:  0.0067461454309523106\n",
      "Iteration:  1621 loss:  0.007496280129998922\n",
      "Iteration:  1622 loss:  0.00673650810495019\n",
      "Iteration:  1623 loss:  0.007091522682458162\n",
      "Iteration:  1624 loss:  0.0065199690870940685\n",
      "Iteration:  1625 loss:  0.0074698044918477535\n",
      "Iteration:  1626 loss:  0.007383488584309816\n",
      "Iteration:  1627 loss:  0.00572529062628746\n",
      "Iteration:  1628 loss:  0.005437122657895088\n",
      "Iteration:  1629 loss:  0.005567014217376709\n",
      "Iteration:  1630 loss:  0.00602303259074688\n",
      "Iteration:  1631 loss:  0.005173241253942251\n",
      "Iteration:  1632 loss:  0.005316540598869324\n",
      "Iteration:  1633 loss:  0.004539883695542812\n",
      "Iteration:  1634 loss:  0.0047849016264081\n",
      "Iteration:  1635 loss:  0.004788568243384361\n",
      "Iteration:  1636 loss:  0.006058725528419018\n",
      "Iteration:  1637 loss:  0.00564806442707777\n",
      "Iteration:  1638 loss:  0.004836517386138439\n",
      "Iteration:  1639 loss:  0.004128777887672186\n",
      "Iteration:  1640 loss:  0.004273625090718269\n",
      "Iteration:  1641 loss:  0.004142034333199263\n",
      "Iteration:  1642 loss:  0.004989170003682375\n",
      "Iteration:  1643 loss:  0.0045639462769031525\n",
      "Iteration:  1644 loss:  0.0043229577131569386\n",
      "Iteration:  1645 loss:  0.0048713069409132\n",
      "Iteration:  1646 loss:  0.004472007974982262\n",
      "Iteration:  1647 loss:  0.004508731886744499\n",
      "Iteration:  1648 loss:  0.004731794353574514\n",
      "Iteration:  1649 loss:  0.004399270284920931\n",
      "Iteration:  1650 loss:  0.004699561279267073\n",
      "Iteration:  1651 loss:  0.005217612721025944\n",
      "Iteration:  1652 loss:  0.005061636213213205\n",
      "Iteration:  1653 loss:  0.005277893505990505\n",
      "Iteration:  1654 loss:  0.005131709389388561\n",
      "Iteration:  1655 loss:  0.0038926939014345407\n",
      "Iteration:  1656 loss:  0.004266052972525358\n",
      "Iteration:  1657 loss:  0.005282043945044279\n",
      "Iteration:  1658 loss:  0.004443790763616562\n",
      "Iteration:  1659 loss:  0.004101432394236326\n",
      "Iteration:  1660 loss:  0.005219737533479929\n",
      "Iteration:  1661 loss:  0.004167471081018448\n",
      "Iteration:  1662 loss:  0.004241381771862507\n",
      "Iteration:  1663 loss:  0.003522102953866124\n",
      "Iteration:  1664 loss:  0.0038680359721183777\n",
      "Iteration:  1665 loss:  0.00451049255207181\n",
      "Iteration:  1666 loss:  0.003901924006640911\n",
      "Iteration:  1667 loss:  0.0050544883124530315\n",
      "Iteration:  1668 loss:  0.003867608495056629\n",
      "Iteration:  1669 loss:  0.004340043291449547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1670 loss:  0.004594448488205671\n",
      "Iteration:  1671 loss:  0.004525596741586924\n",
      "Iteration:  1672 loss:  0.004507835954427719\n",
      "Iteration:  1673 loss:  0.004619993735104799\n",
      "Iteration:  1674 loss:  0.004231602419167757\n",
      "Iteration:  1675 loss:  0.00424522440880537\n",
      "Iteration:  1676 loss:  0.004172916058450937\n",
      "Iteration:  1677 loss:  0.004775933921337128\n",
      "Iteration:  1678 loss:  0.004481358919292688\n",
      "Iteration:  1679 loss:  0.004274330567568541\n",
      "Iteration:  1680 loss:  0.005015327129513025\n",
      "Iteration:  1681 loss:  0.004603826440870762\n",
      "Iteration:  1682 loss:  0.004561655689030886\n",
      "Iteration:  1683 loss:  0.004482479766011238\n",
      "Iteration:  1684 loss:  0.004302984103560448\n",
      "Iteration:  1685 loss:  0.004319758620113134\n",
      "Iteration:  1686 loss:  0.003973721992224455\n",
      "Iteration:  1687 loss:  0.004479824565351009\n",
      "Iteration:  1688 loss:  0.004257538355886936\n",
      "Iteration:  1689 loss:  0.004755884874612093\n",
      "Iteration:  1690 loss:  0.004104334395378828\n",
      "Iteration:  1691 loss:  0.004815070889890194\n",
      "Iteration:  1692 loss:  0.004799896385520697\n",
      "Iteration:  1693 loss:  0.004424768965691328\n",
      "Iteration:  1694 loss:  0.003817042801529169\n",
      "Iteration:  1695 loss:  0.0043633985333144665\n",
      "Iteration:  1696 loss:  0.004777329508215189\n",
      "Iteration:  1697 loss:  0.004660118371248245\n",
      "Iteration:  1698 loss:  0.006528475321829319\n",
      "Iteration:  1699 loss:  0.005395132582634687\n",
      "Iteration:  1700 loss:  0.006336580961942673\n",
      "Iteration:  1701 loss:  0.004951168317347765\n",
      "Iteration:  1702 loss:  0.005350690800696611\n",
      "Iteration:  1703 loss:  0.0054433392360806465\n",
      "Iteration:  1704 loss:  0.006319758947938681\n",
      "Iteration:  1705 loss:  0.00473850267007947\n",
      "Iteration:  1706 loss:  0.005625060759484768\n",
      "Iteration:  1707 loss:  0.004904413595795631\n",
      "Iteration:  1708 loss:  0.005166814662516117\n",
      "Iteration:  1709 loss:  0.005395430605858564\n",
      "Iteration:  1710 loss:  0.004823660012334585\n",
      "Iteration:  1711 loss:  0.005450587719678879\n",
      "Iteration:  1712 loss:  0.005635097157210112\n",
      "Iteration:  1713 loss:  0.005385545548051596\n",
      "Iteration:  1714 loss:  0.004952381830662489\n",
      "Iteration:  1715 loss:  0.006737790070474148\n",
      "Iteration:  1716 loss:  0.004510821774601936\n",
      "Iteration:  1717 loss:  0.005414862185716629\n",
      "Iteration:  1718 loss:  0.005131138022989035\n",
      "Iteration:  1719 loss:  0.0053574517369270325\n",
      "Iteration:  1720 loss:  0.005240697413682938\n",
      "Iteration:  1721 loss:  0.005094144027680159\n",
      "Iteration:  1722 loss:  0.005090009421110153\n",
      "Iteration:  1723 loss:  0.005315268877893686\n",
      "Iteration:  1724 loss:  0.005274756345897913\n",
      "Iteration:  1725 loss:  0.004966801032423973\n",
      "Iteration:  1726 loss:  0.0054900688119232655\n",
      "Iteration:  1727 loss:  0.004785238765180111\n",
      "Iteration:  1728 loss:  0.004522460047155619\n",
      "Iteration:  1729 loss:  0.005463782232254744\n",
      "Iteration:  1730 loss:  0.004439644515514374\n",
      "Iteration:  1731 loss:  0.004892711993306875\n",
      "Iteration:  1732 loss:  0.005712402984499931\n",
      "Iteration:  1733 loss:  0.004606927279382944\n",
      "Iteration:  1734 loss:  0.004891643300652504\n",
      "Iteration:  1735 loss:  0.005779691971838474\n",
      "Iteration:  1736 loss:  0.004842440132051706\n",
      "Iteration:  1737 loss:  0.004727930296212435\n",
      "Iteration:  1738 loss:  0.004787215497344732\n",
      "Iteration:  1739 loss:  0.005199945066124201\n",
      "Iteration:  1740 loss:  0.004793816711753607\n",
      "Iteration:  1741 loss:  0.0126161128282547\n",
      "Iteration:  1742 loss:  0.015309871174395084\n",
      "Iteration:  1743 loss:  0.013603076338768005\n",
      "Iteration:  1744 loss:  0.012665067799389362\n",
      "Iteration:  1745 loss:  0.012240725569427013\n",
      "Iteration:  1746 loss:  0.012160963378846645\n",
      "Iteration:  1747 loss:  0.012858065776526928\n",
      "Iteration:  1748 loss:  0.011930473148822784\n",
      "Iteration:  1749 loss:  0.010271407663822174\n",
      "Iteration:  1750 loss:  0.005578549113124609\n",
      "Iteration:  1751 loss:  0.005608574952930212\n",
      "Iteration:  1752 loss:  0.005047912709414959\n",
      "Iteration:  1753 loss:  0.005335736554116011\n",
      "Iteration:  1754 loss:  0.005537916906177998\n",
      "Iteration:  1755 loss:  0.005364969372749329\n",
      "Iteration:  1756 loss:  0.005769259296357632\n",
      "Iteration:  1757 loss:  0.00537729961797595\n",
      "Iteration:  1758 loss:  0.006685047410428524\n",
      "Iteration:  1759 loss:  0.005132583901286125\n",
      "Iteration:  1760 loss:  0.004465936217457056\n",
      "Iteration:  1761 loss:  0.004591119010001421\n",
      "Iteration:  1762 loss:  0.00512797711417079\n",
      "Iteration:  1763 loss:  0.0038839366752654314\n",
      "Iteration:  1764 loss:  0.0047118160873651505\n",
      "Iteration:  1765 loss:  0.0044629513286054134\n",
      "Iteration:  1766 loss:  0.005091592203825712\n",
      "Iteration:  1767 loss:  0.004899979569017887\n",
      "Iteration:  1768 loss:  0.005602315068244934\n",
      "Iteration:  1769 loss:  0.005247068125754595\n",
      "Iteration:  1770 loss:  0.004685503430664539\n",
      "Iteration:  1771 loss:  0.005808002781122923\n",
      "Iteration:  1772 loss:  0.0051759472116827965\n",
      "Iteration:  1773 loss:  0.005041417665779591\n",
      "Iteration:  1774 loss:  0.005177605431526899\n",
      "Iteration:  1775 loss:  0.0052753412164747715\n",
      "Iteration:  1776 loss:  0.00565553130581975\n",
      "Iteration:  1777 loss:  0.005317270755767822\n",
      "Iteration:  1778 loss:  0.005832612980157137\n",
      "Iteration:  1779 loss:  0.005993844009935856\n",
      "Iteration:  1780 loss:  0.006218853872269392\n",
      "Iteration:  1781 loss:  0.005268256179988384\n",
      "Iteration:  1782 loss:  0.006809581536799669\n",
      "Iteration:  1783 loss:  0.005339073482900858\n",
      "Iteration:  1784 loss:  0.005347558297216892\n",
      "Iteration:  1785 loss:  0.004641283769160509\n",
      "Iteration:  1786 loss:  0.0046737599186599255\n",
      "Iteration:  1787 loss:  0.0045813159085810184\n",
      "Iteration:  1788 loss:  0.005061489995568991\n",
      "Iteration:  1789 loss:  0.0041831983253359795\n",
      "Iteration:  1790 loss:  0.004876159131526947\n",
      "Iteration:  1791 loss:  0.0043544284999370575\n",
      "Iteration:  1792 loss:  0.004981301724910736\n",
      "Iteration:  1793 loss:  0.0039443629793822765\n",
      "Iteration:  1794 loss:  0.0052167149260640144\n",
      "Iteration:  1795 loss:  0.004169586580246687\n",
      "Iteration:  1796 loss:  0.004306326620280743\n",
      "Iteration:  1797 loss:  0.004480251111090183\n",
      "Iteration:  1798 loss:  0.004263651091605425\n",
      "Iteration:  1799 loss:  0.004743583500385284\n",
      "Iteration:  1800 loss:  0.004524856340140104\n",
      "Iteration:  1801 loss:  0.004753370303660631\n",
      "Iteration:  1802 loss:  0.005066899582743645\n",
      "Iteration:  1803 loss:  0.005076230503618717\n",
      "Iteration:  1804 loss:  0.004782655742019415\n",
      "Iteration:  1805 loss:  0.005593108478933573\n",
      "Iteration:  1806 loss:  0.004768855404108763\n",
      "Iteration:  1807 loss:  0.00558097567409277\n",
      "Iteration:  1808 loss:  0.005233277101069689\n",
      "Iteration:  1809 loss:  0.005034972447901964\n",
      "Iteration:  1810 loss:  0.0057464176788926125\n",
      "Iteration:  1811 loss:  0.005602069664746523\n",
      "Iteration:  1812 loss:  0.004512029234319925\n",
      "Iteration:  1813 loss:  0.005473320372402668\n",
      "Iteration:  1814 loss:  0.0054716081358492374\n",
      "Iteration:  1815 loss:  0.00456270994618535\n",
      "Iteration:  1816 loss:  0.005899365060031414\n",
      "Iteration:  1817 loss:  0.005498126614838839\n",
      "Iteration:  1818 loss:  0.005283372476696968\n",
      "Iteration:  1819 loss:  0.005082337651401758\n",
      "Iteration:  1820 loss:  0.030507756397128105\n",
      "Iteration:  1821 loss:  0.03244181349873543\n",
      "Iteration:  1822 loss:  0.03167084977030754\n",
      "Iteration:  1823 loss:  0.029048126190900803\n",
      "Iteration:  1824 loss:  0.027822844684123993\n",
      "Iteration:  1825 loss:  0.03013262152671814\n",
      "Iteration:  1826 loss:  0.027722395956516266\n",
      "Iteration:  1827 loss:  0.02591521479189396\n",
      "Iteration:  1828 loss:  0.021816467866301537\n",
      "Iteration:  1829 loss:  0.006672436837106943\n",
      "Iteration:  1830 loss:  0.005734461359679699\n",
      "Iteration:  1831 loss:  0.006589566357433796\n",
      "Iteration:  1832 loss:  0.0075316964648664\n",
      "Iteration:  1833 loss:  0.0067595262080430984\n",
      "Iteration:  1834 loss:  0.007043020334094763\n",
      "Iteration:  1835 loss:  0.0079121058806777\n",
      "Iteration:  1836 loss:  0.007601954974234104\n",
      "Iteration:  1837 loss:  0.005745233036577702\n",
      "Iteration:  1838 loss:  0.005584951490163803\n",
      "Iteration:  1839 loss:  0.005225337576121092\n",
      "Iteration:  1840 loss:  0.005147730465978384\n",
      "Iteration:  1841 loss:  0.004387427121400833\n",
      "Iteration:  1842 loss:  0.0050913747400045395\n",
      "Iteration:  1843 loss:  0.004765039309859276\n",
      "Iteration:  1844 loss:  0.004455959424376488\n",
      "Iteration:  1845 loss:  0.00492378044873476\n",
      "Iteration:  1846 loss:  0.005465862341225147\n",
      "Iteration:  1847 loss:  0.0057718753814697266\n",
      "Iteration:  1848 loss:  0.006004747934639454\n",
      "Iteration:  1849 loss:  0.005184629466384649\n",
      "Iteration:  1850 loss:  0.0057009123265743256\n",
      "Iteration:  1851 loss:  0.005453046876937151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1852 loss:  0.006195687688887119\n",
      "Iteration:  1853 loss:  0.005778281018137932\n",
      "Iteration:  1854 loss:  0.005351008847355843\n",
      "Iteration:  1855 loss:  0.00973740965127945\n",
      "Iteration:  1856 loss:  0.009546313434839249\n",
      "Iteration:  1857 loss:  0.007389835547655821\n",
      "Iteration:  1858 loss:  0.008407327346503735\n",
      "Iteration:  1859 loss:  0.008043334819376469\n",
      "Iteration:  1860 loss:  0.008286749012768269\n",
      "Iteration:  1861 loss:  0.006599060259759426\n",
      "Iteration:  1862 loss:  0.007948564365506172\n",
      "Iteration:  1863 loss:  0.007328251842409372\n",
      "Iteration:  1864 loss:  0.007924741134047508\n",
      "Iteration:  1865 loss:  0.0073178173042833805\n",
      "Iteration:  1866 loss:  0.00812158826738596\n",
      "Iteration:  1867 loss:  0.007690520957112312\n",
      "Iteration:  1868 loss:  0.008182027377188206\n",
      "Iteration:  1869 loss:  0.008208480663597584\n",
      "Iteration:  1870 loss:  0.008247340098023415\n",
      "Iteration:  1871 loss:  0.008224732242524624\n",
      "Iteration:  1872 loss:  0.007662387564778328\n",
      "Iteration:  1873 loss:  0.006408435758203268\n",
      "Iteration:  1874 loss:  0.007391402497887611\n",
      "Iteration:  1875 loss:  0.006311408709734678\n",
      "Iteration:  1876 loss:  0.0063954065553843975\n",
      "Iteration:  1877 loss:  0.007378339767456055\n",
      "Iteration:  1878 loss:  0.006890881806612015\n",
      "Iteration:  1879 loss:  0.007253510411828756\n",
      "Iteration:  1880 loss:  0.007969089783728123\n",
      "Iteration:  1881 loss:  0.005212145857512951\n",
      "Iteration:  1882 loss:  0.0051337191835045815\n",
      "Iteration:  1883 loss:  0.0055717648938298225\n",
      "Iteration:  1884 loss:  0.004311410244554281\n",
      "Iteration:  1885 loss:  0.004285001195967197\n",
      "Iteration:  1886 loss:  0.004974291194230318\n",
      "Iteration:  1887 loss:  0.00532795675098896\n",
      "Iteration:  1888 loss:  0.005488874390721321\n",
      "Iteration:  1889 loss:  0.004917143378406763\n",
      "Iteration:  1890 loss:  0.004000723361968994\n",
      "Iteration:  1891 loss:  0.0051881433464586735\n",
      "Iteration:  1892 loss:  0.004581690300256014\n",
      "Iteration:  1893 loss:  0.004788924008607864\n",
      "Iteration:  1894 loss:  0.0045224400237202644\n",
      "Iteration:  1895 loss:  0.004813029896467924\n",
      "Iteration:  1896 loss:  0.005092358682304621\n",
      "Iteration:  1897 loss:  0.004235619213432074\n",
      "Iteration:  1898 loss:  0.005325626116245985\n",
      "Iteration:  1899 loss:  0.005792961455881596\n",
      "Iteration:  1900 loss:  0.006146907340735197\n",
      "Iteration:  1901 loss:  0.005737134255468845\n",
      "Iteration:  1902 loss:  0.006738510448485613\n",
      "Iteration:  1903 loss:  0.00553668849170208\n",
      "Iteration:  1904 loss:  0.005293342750519514\n",
      "Iteration:  1905 loss:  0.005692972801625729\n",
      "Iteration:  1906 loss:  0.005774203222244978\n",
      "Iteration:  1907 loss:  0.005755667574703693\n",
      "Iteration:  1908 loss:  0.004688618239015341\n",
      "Iteration:  1909 loss:  0.005389816593378782\n",
      "Iteration:  1910 loss:  0.00524534797295928\n",
      "Iteration:  1911 loss:  0.005509520415216684\n",
      "Iteration:  1912 loss:  0.005418357439339161\n",
      "Iteration:  1913 loss:  0.005732632242143154\n",
      "Iteration:  1914 loss:  0.005837502423673868\n",
      "Iteration:  1915 loss:  0.005033025983721018\n",
      "Iteration:  1916 loss:  0.00466438801959157\n",
      "Iteration:  1917 loss:  0.006021862383931875\n",
      "Iteration:  1918 loss:  0.004950514063239098\n",
      "Iteration:  1919 loss:  0.006074492819607258\n",
      "Iteration:  1920 loss:  0.005388558842241764\n",
      "Iteration:  1921 loss:  0.005138698499649763\n",
      "Iteration:  1922 loss:  0.0056207068264484406\n",
      "Iteration:  1923 loss:  0.005387819372117519\n",
      "Iteration:  1924 loss:  0.005974150262773037\n",
      "Iteration:  1925 loss:  0.005419939756393433\n",
      "Iteration:  1926 loss:  0.005320942960679531\n",
      "Iteration:  1927 loss:  0.004821481183171272\n",
      "Iteration:  1928 loss:  0.005436547566205263\n",
      "Iteration:  1929 loss:  0.006194741930812597\n",
      "Iteration:  1930 loss:  0.00571884959936142\n",
      "Iteration:  1931 loss:  0.004832922481000423\n",
      "Iteration:  1932 loss:  0.006105705630034208\n",
      "Iteration:  1933 loss:  0.004587504081428051\n",
      "Iteration:  1934 loss:  0.005137865897268057\n",
      "Iteration:  1935 loss:  0.005158627405762672\n",
      "Iteration:  1936 loss:  0.005020993761718273\n",
      "Iteration:  1937 loss:  0.004356138873845339\n",
      "Iteration:  1938 loss:  0.004391834139823914\n",
      "Iteration:  1939 loss:  0.004604574758559465\n",
      "Iteration:  1940 loss:  0.00455015804618597\n",
      "Iteration:  1941 loss:  0.004268461838364601\n",
      "Iteration:  1942 loss:  0.005865675397217274\n",
      "Iteration:  1943 loss:  0.008354878053069115\n",
      "Iteration:  1944 loss:  0.007872844114899635\n",
      "Iteration:  1945 loss:  0.007588214240968227\n",
      "Iteration:  1946 loss:  0.007297390606254339\n",
      "Iteration:  1947 loss:  0.00833117961883545\n",
      "Iteration:  1948 loss:  0.007023201789706945\n",
      "Iteration:  1949 loss:  0.006442922633141279\n",
      "Iteration:  1950 loss:  0.006044658366590738\n",
      "Iteration:  1951 loss:  0.006063740234822035\n",
      "Iteration:  1952 loss:  0.0061494954861700535\n",
      "Iteration:  1953 loss:  0.005056233610957861\n",
      "Iteration:  1954 loss:  0.00560419075191021\n",
      "Iteration:  1955 loss:  0.0060867988504469395\n",
      "Iteration:  1956 loss:  0.005678791087120771\n",
      "Iteration:  1957 loss:  0.005935321561992168\n",
      "Iteration:  1958 loss:  0.0046609872952103615\n",
      "Iteration:  1959 loss:  0.005622441880404949\n",
      "Iteration:  1960 loss:  0.005319560412317514\n",
      "Iteration:  1961 loss:  0.005294582806527615\n",
      "Iteration:  1962 loss:  0.00555631797760725\n",
      "Iteration:  1963 loss:  0.005862243473529816\n",
      "Iteration:  1964 loss:  0.005706061143428087\n",
      "Iteration:  1965 loss:  0.0061934953555464745\n",
      "Iteration:  1966 loss:  0.006708502769470215\n",
      "Iteration:  1967 loss:  0.006228657905012369\n",
      "Iteration:  1968 loss:  0.005391302518546581\n",
      "Iteration:  1969 loss:  0.004776656161993742\n",
      "Iteration:  1970 loss:  0.005660689901560545\n",
      "Iteration:  1971 loss:  0.005461022257804871\n",
      "Iteration:  1972 loss:  0.005703811068087816\n",
      "Iteration:  1973 loss:  0.006061165127903223\n",
      "Iteration:  1974 loss:  0.005606633611023426\n",
      "Iteration:  1975 loss:  0.0046685608103871346\n",
      "Iteration:  1976 loss:  0.0061194985173642635\n",
      "Iteration:  1977 loss:  0.005476417485624552\n",
      "Iteration:  1978 loss:  0.005913258995860815\n",
      "Iteration:  1979 loss:  0.005861370358616114\n",
      "Iteration:  1980 loss:  0.00543991569429636\n",
      "Iteration:  1981 loss:  0.004527108743786812\n",
      "Iteration:  1982 loss:  0.005253276787698269\n",
      "Iteration:  1983 loss:  0.00478904927149415\n",
      "Iteration:  1984 loss:  0.0055213430896401405\n",
      "Iteration:  1985 loss:  0.005978606175631285\n",
      "Iteration:  1986 loss:  0.005855250637978315\n",
      "Iteration:  1987 loss:  0.005233049392700195\n",
      "Iteration:  1988 loss:  0.005872273817658424\n",
      "Iteration:  1989 loss:  0.005996802356094122\n",
      "Iteration:  1990 loss:  0.0055432445369660854\n",
      "Iteration:  1991 loss:  0.005498931277543306\n",
      "Iteration:  1992 loss:  0.005748969502747059\n",
      "Iteration:  1993 loss:  0.005130060948431492\n",
      "Iteration:  1994 loss:  0.005623533856123686\n",
      "Iteration:  1995 loss:  0.005136239808052778\n",
      "Iteration:  1996 loss:  0.005767643451690674\n",
      "Iteration:  1997 loss:  0.005081994459033012\n",
      "Iteration:  1998 loss:  0.005073012784123421\n",
      "Iteration:  1999 loss:  0.004919338971376419\n",
      "Iteration:  2000 loss:  0.0053505063988268375\n",
      "Iteration:  2001 loss:  0.00552900368347764\n",
      "Iteration:  2002 loss:  0.005457964260131121\n",
      "Iteration:  2003 loss:  0.005649605765938759\n",
      "Iteration:  2004 loss:  0.004266584757715464\n",
      "Iteration:  2005 loss:  0.004895768128335476\n",
      "Iteration:  2006 loss:  0.004346791654825211\n",
      "Iteration:  2007 loss:  0.004452994558960199\n",
      "Iteration:  2008 loss:  0.004845335613936186\n",
      "Iteration:  2009 loss:  0.004437682684510946\n",
      "Iteration:  2010 loss:  0.004689230117946863\n",
      "Iteration:  2011 loss:  0.004233807325363159\n",
      "Iteration:  2012 loss:  0.003378226887434721\n",
      "Iteration:  2013 loss:  0.00440070079639554\n",
      "Iteration:  2014 loss:  0.004837827291339636\n",
      "Iteration:  2015 loss:  0.00480498606339097\n",
      "Iteration:  2016 loss:  0.0047492217272520065\n",
      "Iteration:  2017 loss:  0.004686555825173855\n",
      "Iteration:  2018 loss:  0.004488836508244276\n",
      "Iteration:  2019 loss:  0.0041967653669416904\n",
      "Iteration:  2020 loss:  0.004753697197884321\n",
      "Iteration:  2021 loss:  0.004592466168105602\n",
      "Iteration:  2022 loss:  0.005377905908972025\n",
      "Iteration:  2023 loss:  0.00519020389765501\n",
      "Iteration:  2024 loss:  0.005438337568193674\n",
      "Iteration:  2025 loss:  0.006034537218511105\n",
      "Iteration:  2026 loss:  0.005860934965312481\n",
      "Iteration:  2027 loss:  0.005124802701175213\n",
      "Iteration:  2028 loss:  0.005108317825943232\n",
      "Iteration:  2029 loss:  0.005323350895196199\n",
      "Iteration:  2030 loss:  0.005880107171833515\n",
      "Iteration:  2031 loss:  0.005332806147634983\n",
      "Iteration:  2032 loss:  0.005440169014036655\n",
      "Iteration:  2033 loss:  0.005618473514914513\n",
      "Iteration:  2034 loss:  0.0053861103951931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2035 loss:  0.004500668030232191\n",
      "Iteration:  2036 loss:  0.00609730975702405\n",
      "Iteration:  2037 loss:  0.005033024121075869\n",
      "Iteration:  2038 loss:  0.005659496411681175\n",
      "Iteration:  2039 loss:  0.008082486689090729\n",
      "Iteration:  2040 loss:  0.0056964801624417305\n",
      "Iteration:  2041 loss:  0.00641887541860342\n",
      "Iteration:  2042 loss:  0.005581422243267298\n",
      "Iteration:  2043 loss:  0.0064204041846096516\n",
      "Iteration:  2044 loss:  0.006218262482434511\n",
      "Iteration:  2045 loss:  0.006165062077343464\n",
      "Iteration:  2046 loss:  0.005441451445221901\n",
      "Iteration:  2047 loss:  0.006103590130805969\n",
      "Iteration:  2048 loss:  0.0060693309642374516\n",
      "Iteration:  2049 loss:  0.006105126813054085\n",
      "Iteration:  2050 loss:  0.0060608661733567715\n",
      "Iteration:  2051 loss:  0.006053428631275892\n",
      "Iteration:  2052 loss:  0.005445612594485283\n",
      "Iteration:  2053 loss:  0.0054483236744999886\n",
      "Iteration:  2054 loss:  0.00561502343043685\n",
      "Iteration:  2055 loss:  0.0052453759126365185\n",
      "Iteration:  2056 loss:  0.004529393743723631\n",
      "Iteration:  2057 loss:  0.004380908794701099\n",
      "Iteration:  2058 loss:  0.004291304387152195\n",
      "Iteration:  2059 loss:  0.0041759624145925045\n",
      "Iteration:  2060 loss:  0.004578621592372656\n",
      "Iteration:  2061 loss:  0.004963994491845369\n",
      "Iteration:  2062 loss:  0.004836976062506437\n",
      "Iteration:  2063 loss:  0.004873716738075018\n",
      "Iteration:  2064 loss:  0.003904667217284441\n",
      "Iteration:  2065 loss:  0.00801653042435646\n",
      "Iteration:  2066 loss:  0.008268504403531551\n",
      "Iteration:  2067 loss:  0.00834722351282835\n",
      "Iteration:  2068 loss:  0.007302202749997377\n",
      "Iteration:  2069 loss:  0.008171250112354755\n",
      "Iteration:  2070 loss:  0.008288419805467129\n",
      "Iteration:  2071 loss:  0.008341382257640362\n",
      "Iteration:  2072 loss:  0.007856532000005245\n",
      "Iteration:  2073 loss:  0.0067296698689460754\n",
      "Iteration:  2074 loss:  0.00572666060179472\n",
      "Iteration:  2075 loss:  0.00530263502150774\n",
      "Iteration:  2076 loss:  0.00616876594722271\n",
      "Iteration:  2077 loss:  0.005454809404909611\n",
      "Iteration:  2078 loss:  0.005562257952988148\n",
      "Iteration:  2079 loss:  0.005735670682042837\n",
      "Iteration:  2080 loss:  0.005736958235502243\n",
      "Iteration:  2081 loss:  0.0059037902392446995\n",
      "Iteration:  2082 loss:  0.005077259615063667\n",
      "Iteration:  2083 loss:  0.004289152100682259\n",
      "Iteration:  2084 loss:  0.004599285777658224\n",
      "Iteration:  2085 loss:  0.005095112603157759\n",
      "Iteration:  2086 loss:  0.004608334973454475\n",
      "Iteration:  2087 loss:  0.004707247484475374\n",
      "Iteration:  2088 loss:  0.004202335141599178\n",
      "Iteration:  2089 loss:  0.0046336837112903595\n",
      "Iteration:  2090 loss:  0.0050218128599226475\n",
      "Iteration:  2091 loss:  0.005113602615892887\n",
      "Iteration:  2092 loss:  0.006106825079768896\n",
      "Iteration:  2093 loss:  0.005603342317044735\n",
      "Iteration:  2094 loss:  0.004812463186681271\n",
      "Iteration:  2095 loss:  0.005329066887497902\n",
      "Iteration:  2096 loss:  0.006218946538865566\n",
      "Iteration:  2097 loss:  0.004484090488404036\n",
      "Iteration:  2098 loss:  0.005538104567676783\n",
      "Iteration:  2099 loss:  0.005094464868307114\n",
      "Iteration:  2100 loss:  0.028430523350834846\n",
      "Iteration:  2101 loss:  0.029401862993836403\n",
      "Iteration:  2102 loss:  0.0292988121509552\n",
      "Iteration:  2103 loss:  0.029834458604454994\n",
      "Iteration:  2104 loss:  0.028475303202867508\n",
      "Iteration:  2105 loss:  0.02822692319750786\n",
      "Iteration:  2106 loss:  0.026406271383166313\n",
      "Iteration:  2107 loss:  0.026702871546149254\n",
      "Iteration:  2108 loss:  0.01835687831044197\n",
      "Iteration:  2109 loss:  0.004910610616207123\n",
      "Iteration:  2110 loss:  0.00427872221916914\n",
      "Iteration:  2111 loss:  0.005807146430015564\n",
      "Iteration:  2112 loss:  0.0059511191211640835\n",
      "Iteration:  2113 loss:  0.006130471359938383\n",
      "Iteration:  2114 loss:  0.006928917486220598\n",
      "Iteration:  2115 loss:  0.005870284512639046\n",
      "Iteration:  2116 loss:  0.005591887049376965\n",
      "Iteration:  2117 loss:  0.007990709505975246\n",
      "Iteration:  2118 loss:  0.009568434208631516\n",
      "Iteration:  2119 loss:  0.007488757837563753\n",
      "Iteration:  2120 loss:  0.010204688645899296\n",
      "Iteration:  2121 loss:  0.009001965634524822\n",
      "Iteration:  2122 loss:  0.007624420337378979\n",
      "Iteration:  2123 loss:  0.008921690285205841\n",
      "Iteration:  2124 loss:  0.007076288107782602\n",
      "Iteration:  2125 loss:  0.007920113392174244\n",
      "Iteration:  2126 loss:  0.007795160636305809\n",
      "Iteration:  2127 loss:  0.007210592273622751\n",
      "Iteration:  2128 loss:  0.0072520580142736435\n",
      "Iteration:  2129 loss:  0.006036254111677408\n",
      "Iteration:  2130 loss:  0.008414143696427345\n",
      "Iteration:  2131 loss:  0.006879412569105625\n",
      "Iteration:  2132 loss:  0.006488919258117676\n",
      "Iteration:  2133 loss:  0.006557077169418335\n",
      "Iteration:  2134 loss:  0.006487660109996796\n",
      "Iteration:  2135 loss:  0.004856750834733248\n",
      "Iteration:  2136 loss:  0.004572546109557152\n",
      "Iteration:  2137 loss:  0.005031083710491657\n",
      "Iteration:  2138 loss:  0.005007088650017977\n",
      "Iteration:  2139 loss:  0.004653120879083872\n",
      "Iteration:  2140 loss:  0.004515264183282852\n",
      "Iteration:  2141 loss:  0.004567364696413279\n",
      "Iteration:  2142 loss:  0.004446006845682859\n",
      "Iteration:  2143 loss:  0.0040434724651277065\n",
      "Iteration:  2144 loss:  0.00477500818669796\n",
      "Iteration:  2145 loss:  0.004712947178632021\n",
      "Iteration:  2146 loss:  0.003959726542234421\n",
      "Iteration:  2147 loss:  0.0037528318352997303\n",
      "Iteration:  2148 loss:  0.004423197824507952\n",
      "Iteration:  2149 loss:  0.004294815007597208\n",
      "Iteration:  2150 loss:  0.004632121417671442\n",
      "Iteration:  2151 loss:  0.004767275881022215\n",
      "Iteration:  2152 loss:  0.004491343162953854\n",
      "Iteration:  2153 loss:  0.005012869834899902\n",
      "Iteration:  2154 loss:  0.004993772599846125\n",
      "Iteration:  2155 loss:  0.005283976439386606\n",
      "Iteration:  2156 loss:  0.005751234479248524\n",
      "Iteration:  2157 loss:  0.006264460273087025\n",
      "Iteration:  2158 loss:  0.004978517536073923\n",
      "Iteration:  2159 loss:  0.005029965192079544\n",
      "Iteration:  2160 loss:  0.005669948644936085\n",
      "Iteration:  2161 loss:  0.005774773191660643\n",
      "Iteration:  2162 loss:  0.005842912010848522\n",
      "Iteration:  2163 loss:  0.004993205890059471\n",
      "Iteration:  2164 loss:  0.005172434728592634\n",
      "Iteration:  2165 loss:  0.005638927221298218\n",
      "Iteration:  2166 loss:  0.005061371251940727\n",
      "Iteration:  2167 loss:  0.005822018254548311\n",
      "Iteration:  2168 loss:  0.005405453499406576\n",
      "Iteration:  2169 loss:  0.0040229265578091145\n",
      "Iteration:  2170 loss:  0.05463526397943497\n",
      "Iteration:  2171 loss:  0.05601431801915169\n",
      "Iteration:  2172 loss:  0.052457764744758606\n",
      "Iteration:  2173 loss:  0.05590527877211571\n",
      "Iteration:  2174 loss:  0.05603397637605667\n",
      "Iteration:  2175 loss:  0.05081237107515335\n",
      "Iteration:  2176 loss:  0.04818619042634964\n",
      "Iteration:  2177 loss:  0.04582839086651802\n",
      "Iteration:  2178 loss:  0.03648295998573303\n",
      "Iteration:  2179 loss:  0.004643910098820925\n",
      "Iteration:  2180 loss:  0.005144963506609201\n",
      "Iteration:  2181 loss:  0.005525374319404364\n",
      "Iteration:  2182 loss:  0.006338920444250107\n",
      "Iteration:  2183 loss:  0.0053476872853934765\n",
      "Iteration:  2184 loss:  0.005908165592700243\n",
      "Iteration:  2185 loss:  0.0050855292938649654\n",
      "Iteration:  2186 loss:  0.005885397084057331\n",
      "Iteration:  2187 loss:  0.005639870651066303\n",
      "Iteration:  2188 loss:  0.007776969578117132\n",
      "Iteration:  2189 loss:  0.006483549252152443\n",
      "Iteration:  2190 loss:  0.006981613114476204\n",
      "Iteration:  2191 loss:  0.0070291608572006226\n",
      "Iteration:  2192 loss:  0.006006449460983276\n",
      "Iteration:  2193 loss:  0.0065416330471634865\n",
      "Iteration:  2194 loss:  0.005764279514551163\n",
      "Iteration:  2195 loss:  0.006495635956525803\n",
      "Iteration:  2196 loss:  0.014491346664726734\n",
      "Iteration:  2197 loss:  0.01889236457645893\n",
      "Iteration:  2198 loss:  0.01776917278766632\n",
      "Iteration:  2199 loss:  0.01697191596031189\n",
      "Iteration:  2200 loss:  0.017308009788393974\n",
      "Iteration:  2201 loss:  0.014536164700984955\n",
      "Iteration:  2202 loss:  0.016323350369930267\n",
      "Iteration:  2203 loss:  0.015310326591134071\n",
      "Iteration:  2204 loss:  0.013535390608012676\n",
      "Iteration:  2205 loss:  0.015153588727116585\n",
      "Iteration:  2206 loss:  0.012673630379140377\n",
      "Iteration:  2207 loss:  0.013878804631531239\n",
      "Iteration:  2208 loss:  0.011344843544065952\n",
      "Iteration:  2209 loss:  0.013268467970192432\n",
      "Iteration:  2210 loss:  0.011668767780065536\n",
      "Iteration:  2211 loss:  0.010225890204310417\n",
      "Iteration:  2212 loss:  0.0095880888402462\n",
      "Iteration:  2213 loss:  0.009279654361307621\n",
      "Iteration:  2214 loss:  0.005689327605068684\n",
      "Iteration:  2215 loss:  0.007030677516013384\n",
      "Iteration:  2216 loss:  0.006490528583526611\n",
      "Iteration:  2217 loss:  0.006110373884439468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2218 loss:  0.005933650303632021\n",
      "Iteration:  2219 loss:  0.008006949909031391\n",
      "Iteration:  2220 loss:  0.0067675430327653885\n",
      "Iteration:  2221 loss:  0.007409166544675827\n",
      "Iteration:  2222 loss:  0.008210355415940285\n",
      "Iteration:  2223 loss:  0.010921773500740528\n",
      "Iteration:  2224 loss:  0.012950295582413673\n",
      "Iteration:  2225 loss:  0.012191451154649258\n",
      "Iteration:  2226 loss:  0.009801165200769901\n",
      "Iteration:  2227 loss:  0.011621149256825447\n",
      "Iteration:  2228 loss:  0.0105091268196702\n",
      "Iteration:  2229 loss:  0.011509154923260212\n",
      "Iteration:  2230 loss:  0.010802004486322403\n",
      "Iteration:  2231 loss:  0.011781217530369759\n",
      "Iteration:  2232 loss:  0.01260427013039589\n",
      "Iteration:  2233 loss:  0.01146614644676447\n",
      "Iteration:  2234 loss:  0.01154518686234951\n",
      "Iteration:  2235 loss:  0.0118172662332654\n",
      "Iteration:  2236 loss:  0.009266737848520279\n",
      "Iteration:  2237 loss:  0.008037085644900799\n",
      "Iteration:  2238 loss:  0.009270363487303257\n",
      "Iteration:  2239 loss:  0.008104325272142887\n",
      "Iteration:  2240 loss:  0.006346629932522774\n",
      "Iteration:  2241 loss:  0.00614313455298543\n",
      "Iteration:  2242 loss:  0.006292673293501139\n",
      "Iteration:  2243 loss:  0.006454465910792351\n",
      "Iteration:  2244 loss:  0.005330962594598532\n",
      "Iteration:  2245 loss:  0.006475038826465607\n",
      "Iteration:  2246 loss:  0.00557327177375555\n",
      "Iteration:  2247 loss:  0.00544366892427206\n",
      "Iteration:  2248 loss:  0.0057725002989172935\n",
      "Iteration:  2249 loss:  0.005605506710708141\n",
      "Iteration:  2250 loss:  0.005596719682216644\n",
      "Iteration:  2251 loss:  0.00496835820376873\n",
      "Iteration:  2252 loss:  0.005125389900058508\n",
      "Iteration:  2253 loss:  0.004713716451078653\n",
      "Iteration:  2254 loss:  0.005384657997637987\n",
      "Iteration:  2255 loss:  0.005198986269533634\n",
      "Iteration:  2256 loss:  0.004596848040819168\n",
      "Iteration:  2257 loss:  0.005554541014134884\n",
      "Iteration:  2258 loss:  0.00505882827565074\n",
      "Iteration:  2259 loss:  0.004981212317943573\n",
      "Iteration:  2260 loss:  0.005211660172790289\n",
      "Iteration:  2261 loss:  0.004807643126696348\n",
      "Iteration:  2262 loss:  0.005992680322378874\n",
      "Iteration:  2263 loss:  0.0056115007027983665\n",
      "Iteration:  2264 loss:  0.004925732500851154\n",
      "Iteration:  2265 loss:  0.004661316052079201\n",
      "Iteration:  2266 loss:  0.005786724388599396\n",
      "Iteration:  2267 loss:  0.005263121798634529\n",
      "Iteration:  2268 loss:  0.005364567972719669\n",
      "Iteration:  2269 loss:  0.005489372182637453\n",
      "Iteration:  2270 loss:  0.005474273581057787\n",
      "Iteration:  2271 loss:  0.004880620166659355\n",
      "Iteration:  2272 loss:  0.00496975053101778\n",
      "Iteration:  2273 loss:  0.0044837878085672855\n",
      "Iteration:  2274 loss:  0.005389841739088297\n",
      "Iteration:  2275 loss:  0.004146682098507881\n",
      "Iteration:  2276 loss:  0.005483867134898901\n",
      "Iteration:  2277 loss:  0.006000370252877474\n",
      "Iteration:  2278 loss:  0.0058394791558384895\n",
      "Iteration:  2279 loss:  0.00564963836222887\n",
      "Iteration:  2280 loss:  0.004319127649068832\n",
      "Iteration:  2281 loss:  0.004588708281517029\n",
      "Iteration:  2282 loss:  0.005111596547067165\n",
      "Iteration:  2283 loss:  0.004680129233747721\n",
      "Iteration:  2284 loss:  0.007205752655863762\n",
      "Iteration:  2285 loss:  0.006914475001394749\n",
      "Iteration:  2286 loss:  0.006078490987420082\n",
      "Iteration:  2287 loss:  0.00669966172426939\n",
      "Iteration:  2288 loss:  0.00715505750849843\n",
      "Iteration:  2289 loss:  0.005921867676079273\n",
      "Iteration:  2290 loss:  0.007304227910935879\n",
      "Iteration:  2291 loss:  0.0049552032724022865\n",
      "Iteration:  2292 loss:  0.006692897994071245\n",
      "Iteration:  2293 loss:  0.006063740700483322\n",
      "Iteration:  2294 loss:  0.006114219781011343\n",
      "Iteration:  2295 loss:  0.006210674066096544\n",
      "Iteration:  2296 loss:  0.006079183425754309\n",
      "Iteration:  2297 loss:  0.005865069571882486\n",
      "Iteration:  2298 loss:  0.006946244277060032\n",
      "Iteration:  2299 loss:  0.005322533659636974\n",
      "Iteration:  2300 loss:  0.005633881781250238\n",
      "Iteration:  2301 loss:  0.00950238760560751\n",
      "Iteration:  2302 loss:  0.010970561765134335\n",
      "Iteration:  2303 loss:  0.011420081369578838\n",
      "Iteration:  2304 loss:  0.010574441403150558\n",
      "Iteration:  2305 loss:  0.010381237603724003\n",
      "Iteration:  2306 loss:  0.011900046840310097\n",
      "Iteration:  2307 loss:  0.010070639662444592\n",
      "Iteration:  2308 loss:  0.00947526190429926\n",
      "Iteration:  2309 loss:  0.009693488478660583\n",
      "Iteration:  2310 loss:  0.004533813335001469\n",
      "Iteration:  2311 loss:  0.004374226555228233\n",
      "Iteration:  2312 loss:  0.004759022034704685\n",
      "Iteration:  2313 loss:  0.00474998913705349\n",
      "Iteration:  2314 loss:  0.004560917150229216\n",
      "Iteration:  2315 loss:  0.003918120171874762\n",
      "Iteration:  2316 loss:  0.004328214097768068\n",
      "Iteration:  2317 loss:  0.004754344001412392\n",
      "Iteration:  2318 loss:  0.0052789608016610146\n",
      "Iteration:  2319 loss:  0.006400304846465588\n",
      "Iteration:  2320 loss:  0.006309063173830509\n",
      "Iteration:  2321 loss:  0.006014286074787378\n",
      "Iteration:  2322 loss:  0.006135419476777315\n",
      "Iteration:  2323 loss:  0.006239487789571285\n",
      "Iteration:  2324 loss:  0.006739412434399128\n",
      "Iteration:  2325 loss:  0.006231648847460747\n",
      "Iteration:  2326 loss:  0.0065639135427773\n",
      "Iteration:  2327 loss:  0.0059730070643126965\n",
      "Iteration:  2328 loss:  0.006661016959697008\n",
      "Iteration:  2329 loss:  0.005455034784972668\n",
      "Iteration:  2330 loss:  0.005583186633884907\n",
      "Iteration:  2331 loss:  0.006054722238332033\n",
      "Iteration:  2332 loss:  0.006296934559941292\n",
      "Iteration:  2333 loss:  0.005985187366604805\n",
      "Iteration:  2334 loss:  0.006034265272319317\n",
      "Iteration:  2335 loss:  0.00601758761331439\n",
      "Iteration:  2336 loss:  0.006386350374668837\n",
      "Iteration:  2337 loss:  0.006066036876291037\n",
      "Iteration:  2338 loss:  0.006879606284201145\n",
      "Iteration:  2339 loss:  0.006463169120252132\n",
      "Iteration:  2340 loss:  0.006562079768627882\n",
      "Iteration:  2341 loss:  0.0073989033699035645\n",
      "Iteration:  2342 loss:  0.007687482517212629\n",
      "Iteration:  2343 loss:  0.006107983645051718\n",
      "Iteration:  2344 loss:  0.006950029172003269\n",
      "Iteration:  2345 loss:  0.005329477600753307\n",
      "Iteration:  2346 loss:  0.0053059980273246765\n",
      "Iteration:  2347 loss:  0.005564808379858732\n",
      "Iteration:  2348 loss:  0.006732103414833546\n",
      "Iteration:  2349 loss:  0.004783889278769493\n",
      "Iteration:  2350 loss:  0.005433393642306328\n",
      "Iteration:  2351 loss:  0.005501469597220421\n",
      "Iteration:  2352 loss:  0.006258648820221424\n",
      "Iteration:  2353 loss:  0.005774291697889566\n",
      "Iteration:  2354 loss:  0.003681673901155591\n",
      "Iteration:  2355 loss:  0.004640546627342701\n",
      "Iteration:  2356 loss:  0.004723765887320042\n",
      "Iteration:  2357 loss:  0.004371212795376778\n",
      "Iteration:  2358 loss:  0.004697068594396114\n",
      "Iteration:  2359 loss:  0.004462211858481169\n",
      "Iteration:  2360 loss:  0.004367381799966097\n",
      "Iteration:  2361 loss:  0.004365706350654364\n",
      "Iteration:  2362 loss:  0.00484253978356719\n",
      "Iteration:  2363 loss:  0.005465993192046881\n",
      "Iteration:  2364 loss:  0.005509356968104839\n",
      "Iteration:  2365 loss:  0.005435079336166382\n",
      "Iteration:  2366 loss:  0.005320757161825895\n",
      "Iteration:  2367 loss:  0.005448588170111179\n",
      "Iteration:  2368 loss:  0.005694469902664423\n",
      "Iteration:  2369 loss:  0.004843618255108595\n",
      "Iteration:  2370 loss:  0.0054965876042842865\n",
      "Iteration:  2371 loss:  0.006338200531899929\n",
      "Iteration:  2372 loss:  0.007746318355202675\n",
      "Iteration:  2373 loss:  0.007070574909448624\n",
      "Iteration:  2374 loss:  0.00704782223328948\n",
      "Iteration:  2375 loss:  0.007618010509759188\n",
      "Iteration:  2376 loss:  0.007101742085069418\n",
      "Iteration:  2377 loss:  0.007125591393560171\n",
      "Iteration:  2378 loss:  0.006988066248595715\n",
      "Iteration:  2379 loss:  0.006764400750398636\n",
      "Iteration:  2380 loss:  0.004758002236485481\n",
      "Iteration:  2381 loss:  0.004601477179676294\n",
      "Iteration:  2382 loss:  0.006068862043321133\n",
      "Iteration:  2383 loss:  0.0055857873521745205\n",
      "Iteration:  2384 loss:  0.006304633803665638\n",
      "Iteration:  2385 loss:  0.006448013242334127\n",
      "Iteration:  2386 loss:  0.005647163838148117\n",
      "Iteration:  2387 loss:  0.005469026044011116\n",
      "Iteration:  2388 loss:  0.005049470346421003\n",
      "Iteration:  2389 loss:  0.004076659679412842\n",
      "Iteration:  2390 loss:  0.004831789992749691\n",
      "Iteration:  2391 loss:  0.0046382080763578415\n",
      "Iteration:  2392 loss:  0.004056645091623068\n",
      "Iteration:  2393 loss:  0.004840006586164236\n",
      "Iteration:  2394 loss:  0.004194723907858133\n",
      "Iteration:  2395 loss:  0.004611570853739977\n",
      "Iteration:  2396 loss:  0.004264329094439745\n",
      "Iteration:  2397 loss:  0.004420074634253979\n",
      "Iteration:  2398 loss:  0.0059239063411951065\n",
      "Iteration:  2399 loss:  0.00526992604136467\n",
      "Iteration:  2400 loss:  0.005765247158706188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2401 loss:  0.005487172398716211\n",
      "Iteration:  2402 loss:  0.006013559643179178\n",
      "Iteration:  2403 loss:  0.005290483124554157\n",
      "Iteration:  2404 loss:  0.005707785487174988\n",
      "Iteration:  2405 loss:  0.005874865688383579\n",
      "Iteration:  2406 loss:  0.0048195114359259605\n",
      "Iteration:  2407 loss:  0.005357416346669197\n",
      "Iteration:  2408 loss:  0.005002626217901707\n",
      "Iteration:  2409 loss:  0.005469665862619877\n",
      "Iteration:  2410 loss:  0.005890707019716501\n",
      "Iteration:  2411 loss:  0.00588420033454895\n",
      "Iteration:  2412 loss:  0.005244754254817963\n",
      "Iteration:  2413 loss:  0.005542815197259188\n",
      "Iteration:  2414 loss:  0.005045243538916111\n",
      "Iteration:  2415 loss:  0.005551957990974188\n",
      "Iteration:  2416 loss:  0.005610773805528879\n",
      "Iteration:  2417 loss:  0.005016610026359558\n",
      "Iteration:  2418 loss:  0.005505319219082594\n",
      "Iteration:  2419 loss:  0.004396785981953144\n",
      "Iteration:  2420 loss:  0.0057005626149475574\n",
      "Iteration:  2421 loss:  0.005010797176510096\n",
      "Iteration:  2422 loss:  0.005256098695099354\n",
      "Iteration:  2423 loss:  0.005207194481045008\n",
      "Iteration:  2424 loss:  0.006494070868939161\n",
      "Iteration:  2425 loss:  0.006099059712141752\n",
      "Iteration:  2426 loss:  0.005864594131708145\n",
      "Iteration:  2427 loss:  0.006571660749614239\n",
      "Iteration:  2428 loss:  0.006222492549568415\n",
      "Iteration:  2429 loss:  0.00605766149237752\n",
      "Iteration:  2430 loss:  0.0060755726881325245\n",
      "Iteration:  2431 loss:  0.006744274869561195\n",
      "Iteration:  2432 loss:  0.005972837097942829\n",
      "Iteration:  2433 loss:  0.005563894286751747\n",
      "Iteration:  2434 loss:  0.004536835011094809\n",
      "Iteration:  2435 loss:  0.004994935821741819\n",
      "Iteration:  2436 loss:  0.005229640286415815\n",
      "Iteration:  2437 loss:  0.004818162880837917\n",
      "Iteration:  2438 loss:  0.004606022499501705\n",
      "Iteration:  2439 loss:  0.005489486735314131\n",
      "Iteration:  2440 loss:  0.004657072480767965\n",
      "Iteration:  2441 loss:  0.005952350329607725\n",
      "Iteration:  2442 loss:  0.005908239632844925\n",
      "Iteration:  2443 loss:  0.0060384878888726234\n",
      "Iteration:  2444 loss:  0.005564181134104729\n",
      "Iteration:  2445 loss:  0.005921535659581423\n",
      "Iteration:  2446 loss:  0.006133542396128178\n",
      "Iteration:  2447 loss:  0.005485829897224903\n",
      "Iteration:  2448 loss:  0.006376801058650017\n",
      "Iteration:  2449 loss:  0.005863767117261887\n",
      "Iteration:  2450 loss:  0.005194610450416803\n",
      "Iteration:  2451 loss:  0.005570427048951387\n",
      "Iteration:  2452 loss:  0.0051092589274048805\n",
      "Iteration:  2453 loss:  0.005351116880774498\n",
      "Iteration:  2454 loss:  0.005165389738976955\n",
      "Iteration:  2455 loss:  0.006166183389723301\n",
      "Iteration:  2456 loss:  0.004746008664369583\n",
      "Iteration:  2457 loss:  0.005551252048462629\n",
      "Iteration:  2458 loss:  0.004992129281163216\n",
      "Iteration:  2459 loss:  0.005173958837985992\n",
      "Iteration:  2460 loss:  0.005583379417657852\n",
      "Iteration:  2461 loss:  0.00542792771011591\n",
      "Iteration:  2462 loss:  0.005768215283751488\n",
      "Iteration:  2463 loss:  0.004348741844296455\n",
      "Iteration:  2464 loss:  0.004692821763455868\n",
      "Iteration:  2465 loss:  0.005724276416003704\n",
      "Iteration:  2466 loss:  0.004894789308309555\n",
      "Iteration:  2467 loss:  0.00590484170243144\n",
      "Iteration:  2468 loss:  0.007494148798286915\n",
      "Iteration:  2469 loss:  0.007842321880161762\n",
      "Iteration:  2470 loss:  0.006371937692165375\n",
      "Iteration:  2471 loss:  0.006706489250063896\n",
      "Iteration:  2472 loss:  0.0073234871961176395\n",
      "Iteration:  2473 loss:  0.006020538043230772\n",
      "Iteration:  2474 loss:  0.007408237550407648\n",
      "Iteration:  2475 loss:  0.007278822362422943\n",
      "Iteration:  2476 loss:  0.006830001249909401\n",
      "Iteration:  2477 loss:  0.007879049517214298\n",
      "Iteration:  2478 loss:  0.007788280490785837\n",
      "Iteration:  2479 loss:  0.006311584264039993\n",
      "Iteration:  2480 loss:  0.005202335771173239\n",
      "Iteration:  2481 loss:  0.006642537657171488\n",
      "Iteration:  2482 loss:  0.0061117736622691154\n",
      "Iteration:  2483 loss:  0.005471502430737019\n",
      "Iteration:  2484 loss:  0.006452742498368025\n",
      "Iteration:  2485 loss:  0.00584073644131422\n",
      "Iteration:  2486 loss:  0.006395395379513502\n",
      "Iteration:  2487 loss:  0.006307828240096569\n",
      "Iteration:  2488 loss:  0.005634572356939316\n",
      "Iteration:  2489 loss:  0.005194875877350569\n",
      "Iteration:  2490 loss:  0.006115492898970842\n",
      "Iteration:  2491 loss:  0.006815153639763594\n",
      "Iteration:  2492 loss:  0.005824427120387554\n",
      "Iteration:  2493 loss:  0.006461089476943016\n",
      "Iteration:  2494 loss:  0.006055619567632675\n",
      "Iteration:  2495 loss:  0.006768906023353338\n",
      "Iteration:  2496 loss:  0.006655707955360413\n",
      "Iteration:  2497 loss:  0.005575818475335836\n",
      "Iteration:  2498 loss:  0.0057517471723258495\n",
      "Iteration:  2499 loss:  0.006182693410664797\n",
      "Iteration:  2500 loss:  0.005169249139726162\n",
      "Iteration:  2501 loss:  0.005965393967926502\n",
      "Iteration:  2502 loss:  0.0043408870697021484\n",
      "Iteration:  2503 loss:  0.004401454236358404\n",
      "Iteration:  2504 loss:  0.004801722709089518\n",
      "Iteration:  2505 loss:  0.003935901913791895\n",
      "Iteration:  2506 loss:  0.004511041101068258\n",
      "Iteration:  2507 loss:  0.004157665651291609\n",
      "Iteration:  2508 loss:  0.004484973382204771\n",
      "Iteration:  2509 loss:  0.004168179351836443\n",
      "Iteration:  2510 loss:  0.0041627828031778336\n",
      "Iteration:  2511 loss:  0.005168304778635502\n",
      "Iteration:  2512 loss:  0.005974828731268644\n",
      "Iteration:  2513 loss:  0.0054710120894014835\n",
      "Iteration:  2514 loss:  0.005234190728515387\n",
      "Iteration:  2515 loss:  0.004819332621991634\n",
      "Iteration:  2516 loss:  0.005609029904007912\n",
      "Iteration:  2517 loss:  0.005104012321680784\n",
      "Iteration:  2518 loss:  0.004921507090330124\n",
      "Iteration:  2519 loss:  0.0057906205765903\n",
      "Iteration:  2520 loss:  0.003928065299987793\n",
      "Iteration:  2521 loss:  0.004529436118900776\n",
      "Iteration:  2522 loss:  0.0049888975918293\n",
      "Iteration:  2523 loss:  0.00443032942712307\n",
      "Iteration:  2524 loss:  0.004500128328800201\n",
      "Iteration:  2525 loss:  0.004684509709477425\n",
      "Iteration:  2526 loss:  0.004383404739201069\n",
      "Iteration:  2527 loss:  0.004036969970911741\n",
      "Iteration:  2528 loss:  0.005125544033944607\n",
      "Iteration:  2529 loss:  0.006956153549253941\n",
      "Iteration:  2530 loss:  0.006437631323933601\n",
      "Iteration:  2531 loss:  0.0067411018535494804\n",
      "Iteration:  2532 loss:  0.008427280932664871\n",
      "Iteration:  2533 loss:  0.006546022370457649\n",
      "Iteration:  2534 loss:  0.0075442311353981495\n",
      "Iteration:  2535 loss:  0.006885102484375238\n",
      "Iteration:  2536 loss:  0.005845909006893635\n",
      "Iteration:  2537 loss:  0.005023350939154625\n",
      "Iteration:  2538 loss:  0.004475521855056286\n",
      "Iteration:  2539 loss:  0.004354919772595167\n",
      "Iteration:  2540 loss:  0.004395104944705963\n",
      "Iteration:  2541 loss:  0.004400294739753008\n",
      "Iteration:  2542 loss:  0.004449255298823118\n",
      "Iteration:  2543 loss:  0.0036754454486072063\n",
      "Iteration:  2544 loss:  0.004642023239284754\n",
      "Iteration:  2545 loss:  0.004257652908563614\n",
      "Iteration:  2546 loss:  0.006526295095682144\n",
      "Iteration:  2547 loss:  0.006441975478082895\n",
      "Iteration:  2548 loss:  0.006170173641294241\n",
      "Iteration:  2549 loss:  0.006805045064538717\n",
      "Iteration:  2550 loss:  0.006034608464688063\n",
      "Iteration:  2551 loss:  0.006093563977628946\n",
      "Iteration:  2552 loss:  0.004558689426630735\n",
      "Iteration:  2553 loss:  0.005403857212513685\n",
      "Iteration:  2554 loss:  0.00493276072666049\n",
      "Iteration:  2555 loss:  0.00636418117210269\n",
      "Iteration:  2556 loss:  0.005849604960530996\n",
      "Iteration:  2557 loss:  0.006178118754178286\n",
      "Iteration:  2558 loss:  0.005645984783768654\n",
      "Iteration:  2559 loss:  0.006227261386811733\n",
      "Iteration:  2560 loss:  0.006916338112205267\n",
      "Iteration:  2561 loss:  0.005477658938616514\n",
      "Iteration:  2562 loss:  0.005693723447620869\n",
      "Iteration:  2563 loss:  0.006346845533698797\n",
      "Iteration:  2564 loss:  0.006452446337789297\n",
      "Iteration:  2565 loss:  0.00518864905461669\n",
      "Iteration:  2566 loss:  0.0054204766638576984\n",
      "Iteration:  2567 loss:  0.005648172460496426\n",
      "Iteration:  2568 loss:  0.005527886562049389\n",
      "Iteration:  2569 loss:  0.006079346407204866\n",
      "Iteration:  2570 loss:  0.006282750982791185\n",
      "Iteration:  2571 loss:  0.0052392808720469475\n",
      "Iteration:  2572 loss:  0.006136160343885422\n",
      "Iteration:  2573 loss:  0.005631857551634312\n",
      "Iteration:  2574 loss:  0.005775121971964836\n",
      "Iteration:  2575 loss:  0.005683607421815395\n",
      "Iteration:  2576 loss:  0.0056899781338870525\n",
      "Iteration:  2577 loss:  0.005509073380380869\n",
      "Iteration:  2578 loss:  0.005307211074978113\n",
      "Iteration:  2579 loss:  0.004897274076938629\n",
      "Iteration:  2580 loss:  0.006204208359122276\n",
      "Iteration:  2581 loss:  0.005560272838920355\n",
      "Iteration:  2582 loss:  0.006893531884998083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2583 loss:  0.006929539609700441\n",
      "Iteration:  2584 loss:  0.007039763033390045\n",
      "Iteration:  2585 loss:  0.006957514677196741\n",
      "Iteration:  2586 loss:  0.006368512753397226\n",
      "Iteration:  2587 loss:  0.0056831385008990765\n",
      "Iteration:  2588 loss:  0.006182056851685047\n",
      "Iteration:  2589 loss:  0.00649845227599144\n",
      "Iteration:  2590 loss:  0.005674253683537245\n",
      "Iteration:  2591 loss:  0.0062935007736086845\n",
      "Iteration:  2592 loss:  0.005887576378881931\n",
      "Iteration:  2593 loss:  0.00513535225763917\n",
      "Iteration:  2594 loss:  0.005934248212724924\n",
      "Iteration:  2595 loss:  0.006452116649597883\n",
      "Iteration:  2596 loss:  0.006037874612957239\n",
      "Iteration:  2597 loss:  0.005414342507719994\n",
      "Iteration:  2598 loss:  0.005918423645198345\n",
      "Iteration:  2599 loss:  0.005652043968439102\n",
      "Iteration:  2600 loss:  0.0062407529912889\n",
      "Iteration:  2601 loss:  0.0065226987935602665\n",
      "Iteration:  2602 loss:  0.006481416057795286\n",
      "Iteration:  2603 loss:  0.006429780274629593\n",
      "Iteration:  2604 loss:  0.005884409882128239\n",
      "Iteration:  2605 loss:  0.006254164967685938\n",
      "Iteration:  2606 loss:  0.0066228630021214485\n",
      "Iteration:  2607 loss:  0.04253387451171875\n",
      "Iteration:  2608 loss:  0.07165514677762985\n",
      "Iteration:  2609 loss:  0.07114052772521973\n",
      "Iteration:  2610 loss:  0.07048352807760239\n",
      "Iteration:  2611 loss:  0.07154564559459686\n",
      "Iteration:  2612 loss:  0.0682113841176033\n",
      "Iteration:  2613 loss:  0.05912713333964348\n",
      "Iteration:  2614 loss:  0.061224620789289474\n",
      "Iteration:  2615 loss:  0.05470038950443268\n",
      "Iteration:  2616 loss:  0.01824142225086689\n",
      "Iteration:  2617 loss:  0.007636146619915962\n",
      "Iteration:  2618 loss:  0.009376905858516693\n",
      "Iteration:  2619 loss:  0.009531653486192226\n",
      "Iteration:  2620 loss:  0.009127679280936718\n",
      "Iteration:  2621 loss:  0.010574227198958397\n",
      "Iteration:  2622 loss:  0.010169334709644318\n",
      "Iteration:  2623 loss:  0.011895102448761463\n",
      "Iteration:  2624 loss:  0.009918599389493465\n",
      "Iteration:  2625 loss:  0.009616976603865623\n",
      "Iteration:  2626 loss:  0.009001514874398708\n",
      "Iteration:  2627 loss:  0.009557563811540604\n",
      "Iteration:  2628 loss:  0.010298549197614193\n",
      "Iteration:  2629 loss:  0.00866671558469534\n",
      "Iteration:  2630 loss:  0.008600246161222458\n",
      "Iteration:  2631 loss:  0.009143189527094364\n",
      "Iteration:  2632 loss:  0.008693818934261799\n",
      "Iteration:  2633 loss:  0.009365170262753963\n",
      "Iteration:  2634 loss:  0.00981722678989172\n",
      "Iteration:  2635 loss:  0.010169187560677528\n",
      "Iteration:  2636 loss:  0.010162499733269215\n",
      "Iteration:  2637 loss:  0.008954312652349472\n",
      "Iteration:  2638 loss:  0.010664215311408043\n",
      "Iteration:  2639 loss:  0.007702074013650417\n",
      "Iteration:  2640 loss:  0.008232161402702332\n",
      "Iteration:  2641 loss:  0.008276417851448059\n",
      "Iteration:  2642 loss:  0.009099279530346394\n",
      "Iteration:  2643 loss:  0.0075114089995622635\n",
      "Iteration:  2644 loss:  0.008924226276576519\n",
      "Iteration:  2645 loss:  0.006288058590143919\n",
      "Iteration:  2646 loss:  0.007024732884019613\n",
      "Iteration:  2647 loss:  0.006327382288873196\n",
      "Iteration:  2648 loss:  0.006282345857471228\n",
      "Iteration:  2649 loss:  0.006251105107367039\n",
      "Iteration:  2650 loss:  0.00483782310038805\n",
      "Iteration:  2651 loss:  0.008354129269719124\n",
      "Iteration:  2652 loss:  0.00875798985362053\n",
      "Iteration:  2653 loss:  0.007784297224134207\n",
      "Iteration:  2654 loss:  0.007734554819762707\n",
      "Iteration:  2655 loss:  0.007577266078442335\n",
      "Iteration:  2656 loss:  0.007280129007995129\n",
      "Iteration:  2657 loss:  0.007202816195785999\n",
      "Iteration:  2658 loss:  0.008610607124865055\n",
      "Iteration:  2659 loss:  0.007444013375788927\n",
      "Iteration:  2660 loss:  0.007055745460093021\n",
      "Iteration:  2661 loss:  0.006355758290737867\n",
      "Iteration:  2662 loss:  0.006858566775918007\n",
      "Iteration:  2663 loss:  0.004943904001265764\n",
      "Iteration:  2664 loss:  0.0053877499885857105\n",
      "Iteration:  2665 loss:  0.00551763316616416\n",
      "Iteration:  2666 loss:  0.006088535767048597\n",
      "Iteration:  2667 loss:  0.006012476049363613\n",
      "Iteration:  2668 loss:  0.0059479009360075\n",
      "Iteration:  2669 loss:  0.005975120700895786\n",
      "Iteration:  2670 loss:  0.006925185211002827\n",
      "Iteration:  2671 loss:  0.005902599077671766\n",
      "Iteration:  2672 loss:  0.0074396999552845955\n",
      "Iteration:  2673 loss:  0.006439297925680876\n",
      "Iteration:  2674 loss:  0.007472564931958914\n",
      "Iteration:  2675 loss:  0.006417198572307825\n",
      "Iteration:  2676 loss:  0.00559638999402523\n",
      "Iteration:  2677 loss:  0.005908511579036713\n",
      "Iteration:  2678 loss:  0.006174619309604168\n",
      "Iteration:  2679 loss:  0.006266735959798098\n",
      "Iteration:  2680 loss:  0.005478728096932173\n",
      "Iteration:  2681 loss:  0.006107261870056391\n",
      "Iteration:  2682 loss:  0.006060323212295771\n",
      "Iteration:  2683 loss:  0.005045634228736162\n",
      "Iteration:  2684 loss:  0.005435418803244829\n",
      "Iteration:  2685 loss:  0.005852824077010155\n",
      "Iteration:  2686 loss:  0.005733119789510965\n",
      "Iteration:  2687 loss:  0.005259045399725437\n",
      "Iteration:  2688 loss:  0.0057947770692408085\n",
      "Iteration:  2689 loss:  0.005211189389228821\n",
      "Iteration:  2690 loss:  0.005723923910409212\n",
      "Iteration:  2691 loss:  0.00477305892854929\n",
      "Iteration:  2692 loss:  0.004646332934498787\n",
      "Iteration:  2693 loss:  0.005431079771369696\n",
      "Iteration:  2694 loss:  0.0053236051462590694\n",
      "Iteration:  2695 loss:  0.005761356558650732\n",
      "Iteration:  2696 loss:  0.005594904068857431\n",
      "Iteration:  2697 loss:  0.0057753343135118484\n",
      "Iteration:  2698 loss:  0.005488865543156862\n",
      "Iteration:  2699 loss:  0.004816061817109585\n",
      "Iteration:  2700 loss:  0.0057339146733284\n",
      "Iteration:  2701 loss:  0.005255543626844883\n",
      "Iteration:  2702 loss:  0.005653346888720989\n",
      "Iteration:  2703 loss:  0.004944455809891224\n",
      "Iteration:  2704 loss:  0.0050193192437291145\n",
      "Iteration:  2705 loss:  0.005902992561459541\n",
      "Iteration:  2706 loss:  0.006010361015796661\n",
      "Iteration:  2707 loss:  0.005954606458544731\n",
      "Iteration:  2708 loss:  0.00624909857288003\n",
      "Iteration:  2709 loss:  0.00509013794362545\n",
      "Iteration:  2710 loss:  0.004690322559326887\n",
      "Iteration:  2711 loss:  0.005162621848285198\n",
      "Iteration:  2712 loss:  0.0063502052798867226\n",
      "Iteration:  2713 loss:  0.004914778284728527\n",
      "Iteration:  2714 loss:  0.005196876358240843\n",
      "Iteration:  2715 loss:  0.005397900473326445\n",
      "Iteration:  2716 loss:  0.005048554390668869\n",
      "Iteration:  2717 loss:  0.005952781066298485\n",
      "Iteration:  2718 loss:  0.004733017645776272\n",
      "Iteration:  2719 loss:  0.004272180609405041\n",
      "Iteration:  2720 loss:  0.004697364754974842\n",
      "Iteration:  2721 loss:  0.0057686809450387955\n",
      "Iteration:  2722 loss:  0.006062756292521954\n",
      "Iteration:  2723 loss:  0.005973351188004017\n",
      "Iteration:  2724 loss:  0.006236574146896601\n",
      "Iteration:  2725 loss:  0.005457886960357428\n",
      "Iteration:  2726 loss:  0.005945752374827862\n",
      "Iteration:  2727 loss:  0.006617238279432058\n",
      "Iteration:  2728 loss:  0.0052770283073186874\n",
      "Iteration:  2729 loss:  0.005503760185092688\n",
      "Iteration:  2730 loss:  0.004977739416062832\n",
      "Iteration:  2731 loss:  0.004732474684715271\n",
      "Iteration:  2732 loss:  0.004569848533719778\n",
      "Iteration:  2733 loss:  0.004931434523314238\n",
      "Iteration:  2734 loss:  0.0045210388489067554\n",
      "Iteration:  2735 loss:  0.004484337754547596\n",
      "Iteration:  2736 loss:  0.004084864631295204\n",
      "Iteration:  2737 loss:  0.004230754915624857\n",
      "Iteration:  2738 loss:  0.005886733066290617\n",
      "Iteration:  2739 loss:  0.005530433263629675\n",
      "Iteration:  2740 loss:  0.005295846611261368\n",
      "Iteration:  2741 loss:  0.005981032736599445\n",
      "Iteration:  2742 loss:  0.0053894976153969765\n",
      "Iteration:  2743 loss:  0.0063499463722109795\n",
      "Iteration:  2744 loss:  0.005490470677614212\n",
      "Iteration:  2745 loss:  0.005643495824187994\n",
      "Iteration:  2746 loss:  0.005028443410992622\n",
      "Iteration:  2747 loss:  0.006115582771599293\n",
      "Iteration:  2748 loss:  0.0052666435949504375\n",
      "Iteration:  2749 loss:  0.005402502603828907\n",
      "Iteration:  2750 loss:  0.0057109808549284935\n",
      "Iteration:  2751 loss:  0.005690650083124638\n",
      "Iteration:  2752 loss:  0.005592817440629005\n",
      "Iteration:  2753 loss:  0.004823004826903343\n",
      "Iteration:  2754 loss:  0.005145740695297718\n",
      "Iteration:  2755 loss:  0.005142892710864544\n",
      "Iteration:  2756 loss:  0.006292480044066906\n",
      "Iteration:  2757 loss:  0.005304390098899603\n",
      "Iteration:  2758 loss:  0.005627058446407318\n",
      "Iteration:  2759 loss:  0.0059463586658239365\n",
      "Iteration:  2760 loss:  0.005910436622798443\n",
      "Iteration:  2761 loss:  0.005303673911839724\n",
      "Iteration:  2762 loss:  0.0052728173322975636\n",
      "Iteration:  2763 loss:  0.006413353607058525\n",
      "Iteration:  2764 loss:  0.004141499288380146\n",
      "Iteration:  2765 loss:  0.005088295321911573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2766 loss:  0.005016162991523743\n",
      "Iteration:  2767 loss:  0.005737454164773226\n",
      "Iteration:  2768 loss:  0.004828244913369417\n",
      "Iteration:  2769 loss:  0.0043519046157598495\n",
      "Iteration:  2770 loss:  0.0057414802722632885\n",
      "Iteration:  2771 loss:  0.004381908569484949\n",
      "Iteration:  2772 loss:  0.004742924124002457\n",
      "Iteration:  2773 loss:  0.005206308793276548\n",
      "Iteration:  2774 loss:  0.004210088402032852\n",
      "Iteration:  2775 loss:  0.004404012113809586\n",
      "Iteration:  2776 loss:  0.004645207896828651\n",
      "Iteration:  2777 loss:  0.00478223105892539\n",
      "Iteration:  2778 loss:  0.0046063256449997425\n",
      "Iteration:  2779 loss:  0.0049032485112547874\n",
      "Iteration:  2780 loss:  0.0052712406031787395\n",
      "Iteration:  2781 loss:  0.004525350872427225\n",
      "Iteration:  2782 loss:  0.004966577049344778\n",
      "Iteration:  2783 loss:  0.0048133740201592445\n",
      "Iteration:  2784 loss:  0.0052405656315386295\n",
      "Iteration:  2785 loss:  0.0051438775844872\n",
      "Iteration:  2786 loss:  0.005322274286299944\n",
      "Iteration:  2787 loss:  0.005130417179316282\n",
      "Iteration:  2788 loss:  0.005431600380688906\n",
      "Iteration:  2789 loss:  0.004929329734295607\n",
      "Iteration:  2790 loss:  0.004845115356147289\n",
      "Iteration:  2791 loss:  0.004507021047174931\n",
      "Iteration:  2792 loss:  0.0060453214682638645\n",
      "Iteration:  2793 loss:  0.005170340184122324\n",
      "Iteration:  2794 loss:  0.005334407091140747\n",
      "Iteration:  2795 loss:  0.0051725213415920734\n",
      "Iteration:  2796 loss:  0.005160515196621418\n",
      "Iteration:  2797 loss:  0.00469606788828969\n",
      "Iteration:  2798 loss:  0.004979129880666733\n",
      "Iteration:  2799 loss:  0.005055699031800032\n",
      "Iteration:  2800 loss:  0.00508906552568078\n",
      "Iteration:  2801 loss:  0.0046248603612184525\n",
      "Iteration:  2802 loss:  0.005057146772742271\n",
      "Iteration:  2803 loss:  0.005470918491482735\n",
      "Iteration:  2804 loss:  0.00440575834363699\n",
      "Iteration:  2805 loss:  0.00561619782820344\n",
      "Iteration:  2806 loss:  0.004996933043003082\n",
      "Iteration:  2807 loss:  0.004759073257446289\n",
      "Iteration:  2808 loss:  0.005157386884093285\n",
      "Iteration:  2809 loss:  0.005020712036639452\n",
      "Iteration:  2810 loss:  0.0050919875502586365\n",
      "Iteration:  2811 loss:  0.004434891510754824\n",
      "Iteration:  2812 loss:  0.0040976135060191154\n",
      "Iteration:  2813 loss:  0.004292656201869249\n",
      "Iteration:  2814 loss:  0.005163581110537052\n",
      "Iteration:  2815 loss:  0.004367636516690254\n",
      "Iteration:  2816 loss:  0.00509689562022686\n",
      "Iteration:  2817 loss:  0.006333041936159134\n",
      "Iteration:  2818 loss:  0.009053373709321022\n",
      "Iteration:  2819 loss:  0.010104452259838581\n",
      "Iteration:  2820 loss:  0.008718181401491165\n",
      "Iteration:  2821 loss:  0.008098404854536057\n",
      "Iteration:  2822 loss:  0.008615385740995407\n",
      "Iteration:  2823 loss:  0.00826362892985344\n",
      "Iteration:  2824 loss:  0.006976692471653223\n",
      "Iteration:  2825 loss:  0.0077409460209310055\n",
      "Iteration:  2826 loss:  0.005173172801733017\n",
      "Iteration:  2827 loss:  0.004699345678091049\n",
      "Iteration:  2828 loss:  0.005072779953479767\n",
      "Iteration:  2829 loss:  0.004541905131191015\n",
      "Iteration:  2830 loss:  0.004848693963140249\n",
      "Iteration:  2831 loss:  0.004870877601206303\n",
      "Iteration:  2832 loss:  0.005296604707837105\n",
      "Iteration:  2833 loss:  0.0042453124187886715\n",
      "Iteration:  2834 loss:  0.00465792790055275\n",
      "Iteration:  2835 loss:  0.005248903296887875\n",
      "Iteration:  2836 loss:  0.005845696199685335\n",
      "Iteration:  2837 loss:  0.0054061273112893105\n",
      "Iteration:  2838 loss:  0.0053801690228283405\n",
      "Iteration:  2839 loss:  0.004689060151576996\n",
      "Iteration:  2840 loss:  0.006062707398086786\n",
      "Iteration:  2841 loss:  0.004945689346641302\n",
      "Iteration:  2842 loss:  0.005560480989515781\n",
      "Iteration:  2843 loss:  0.0047125788405537605\n",
      "Iteration:  2844 loss:  0.004503834992647171\n",
      "Iteration:  2845 loss:  0.004542142152786255\n",
      "Iteration:  2846 loss:  0.004461187869310379\n",
      "Iteration:  2847 loss:  0.004910769872367382\n",
      "Iteration:  2848 loss:  0.004938461352139711\n",
      "Iteration:  2849 loss:  0.004659884609282017\n",
      "Iteration:  2850 loss:  0.004256296902894974\n",
      "Iteration:  2851 loss:  0.00466009508818388\n",
      "Iteration:  2852 loss:  0.005293796770274639\n",
      "Iteration:  2853 loss:  0.005050960928201675\n",
      "Iteration:  2854 loss:  0.005055602174252272\n",
      "Iteration:  2855 loss:  0.0048448508605360985\n",
      "Iteration:  2856 loss:  0.00535357603803277\n",
      "Iteration:  2857 loss:  0.005675105843693018\n",
      "Iteration:  2858 loss:  0.005656498018652201\n",
      "Iteration:  2859 loss:  0.005590695887804031\n",
      "Iteration:  2860 loss:  0.00523386662825942\n",
      "Iteration:  2861 loss:  0.005205533467233181\n",
      "Iteration:  2862 loss:  0.004771762993186712\n",
      "Iteration:  2863 loss:  0.0048611522652208805\n",
      "Iteration:  2864 loss:  0.00472237216308713\n",
      "Iteration:  2865 loss:  0.0037160953506827354\n",
      "Iteration:  2866 loss:  0.004713086411356926\n",
      "Iteration:  2867 loss:  0.00467368820682168\n",
      "Iteration:  2868 loss:  0.004246768541634083\n",
      "Iteration:  2869 loss:  0.00478564016520977\n",
      "Iteration:  2870 loss:  0.004386860877275467\n",
      "Iteration:  2871 loss:  0.004774182569235563\n",
      "Iteration:  2872 loss:  0.004216998349875212\n",
      "Iteration:  2873 loss:  0.004920532461255789\n",
      "Iteration:  2874 loss:  0.004285183269530535\n",
      "Iteration:  2875 loss:  0.004693918861448765\n",
      "Iteration:  2876 loss:  0.004448980558663607\n",
      "Iteration:  2877 loss:  0.004599795211106539\n",
      "Iteration:  2878 loss:  0.003971889615058899\n",
      "Iteration:  2879 loss:  0.005447238218039274\n",
      "Iteration:  2880 loss:  0.004676425829529762\n",
      "Iteration:  2881 loss:  0.005064568016678095\n",
      "Iteration:  2882 loss:  0.006378752179443836\n",
      "Iteration:  2883 loss:  0.005089681129902601\n",
      "Iteration:  2884 loss:  0.005131712649017572\n",
      "Iteration:  2885 loss:  0.005479553248733282\n",
      "Iteration:  2886 loss:  0.0054887430742383\n",
      "Iteration:  2887 loss:  0.005479536484926939\n",
      "Iteration:  2888 loss:  0.00547053012996912\n",
      "Iteration:  2889 loss:  0.005337429232895374\n",
      "Iteration:  2890 loss:  0.0056843687780201435\n",
      "Iteration:  2891 loss:  0.005302727688103914\n",
      "Iteration:  2892 loss:  0.005192707758396864\n",
      "Iteration:  2893 loss:  0.005745922680944204\n",
      "Iteration:  2894 loss:  0.005119495093822479\n",
      "Iteration:  2895 loss:  0.005575311370193958\n",
      "Iteration:  2896 loss:  0.005820489954203367\n",
      "Iteration:  2897 loss:  0.00657571479678154\n",
      "Iteration:  2898 loss:  0.006118046119809151\n",
      "Iteration:  2899 loss:  0.00712775532156229\n",
      "Iteration:  2900 loss:  0.006160670425742865\n",
      "Iteration:  2901 loss:  0.006844065617769957\n",
      "Iteration:  2902 loss:  0.006230786442756653\n",
      "Iteration:  2903 loss:  0.006065752357244492\n",
      "Iteration:  2904 loss:  0.0063324784860014915\n",
      "Iteration:  2905 loss:  0.006121331825852394\n",
      "Iteration:  2906 loss:  0.006310106720775366\n",
      "Iteration:  2907 loss:  0.005848584696650505\n",
      "Iteration:  2908 loss:  0.005962829105556011\n",
      "Iteration:  2909 loss:  0.005821950733661652\n",
      "Iteration:  2910 loss:  0.005391938146203756\n",
      "Iteration:  2911 loss:  0.0054516904056072235\n",
      "Iteration:  2912 loss:  0.005844777915626764\n",
      "Iteration:  2913 loss:  0.005112968385219574\n",
      "Iteration:  2914 loss:  0.004578940570354462\n",
      "Iteration:  2915 loss:  0.004396413918584585\n",
      "Iteration:  2916 loss:  0.004684301558881998\n",
      "Iteration:  2917 loss:  0.004888094030320644\n",
      "Iteration:  2918 loss:  0.00420174328610301\n",
      "Iteration:  2919 loss:  0.004021463915705681\n",
      "Iteration:  2920 loss:  0.004173368215560913\n",
      "Iteration:  2921 loss:  0.004933416843414307\n",
      "Iteration:  2922 loss:  0.005391257349401712\n",
      "Iteration:  2923 loss:  0.005317269824445248\n",
      "Iteration:  2924 loss:  0.006585360039025545\n",
      "Iteration:  2925 loss:  0.004934257362037897\n",
      "Iteration:  2926 loss:  0.005181679502129555\n",
      "Iteration:  2927 loss:  0.006389881484210491\n",
      "Iteration:  2928 loss:  0.006100069731473923\n",
      "Iteration:  2929 loss:  0.0054468754678964615\n",
      "Iteration:  2930 loss:  0.005646238103508949\n",
      "Iteration:  2931 loss:  0.00447558518499136\n",
      "Iteration:  2932 loss:  0.006649852730333805\n",
      "Iteration:  2933 loss:  0.005509776063263416\n",
      "Iteration:  2934 loss:  0.005115147680044174\n",
      "Iteration:  2935 loss:  0.004939728882163763\n",
      "Iteration:  2936 loss:  0.005543870851397514\n",
      "Iteration:  2937 loss:  0.005442127585411072\n",
      "Iteration:  2938 loss:  0.0052926428616046906\n",
      "Iteration:  2939 loss:  0.005365924909710884\n",
      "Iteration:  2940 loss:  0.005318826530128717\n",
      "Iteration:  2941 loss:  0.005205478053539991\n",
      "Iteration:  2942 loss:  0.004407971166074276\n",
      "Iteration:  2943 loss:  0.005225440487265587\n",
      "Iteration:  2944 loss:  0.005365729797631502\n",
      "Iteration:  2945 loss:  0.0050992099568247795\n",
      "Iteration:  2946 loss:  0.005194413475692272\n",
      "Iteration:  2947 loss:  0.004996975883841515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2948 loss:  0.003979243338108063\n",
      "Iteration:  2949 loss:  0.005277713760733604\n",
      "Iteration:  2950 loss:  0.0049225930124521255\n",
      "Iteration:  2951 loss:  0.004843954462558031\n",
      "Iteration:  2952 loss:  0.004421614576131105\n",
      "Iteration:  2953 loss:  0.005011986941099167\n",
      "Iteration:  2954 loss:  0.005908671300858259\n",
      "Iteration:  2955 loss:  0.0049613178707659245\n",
      "Iteration:  2956 loss:  0.005496490281075239\n",
      "Iteration:  2957 loss:  0.005255269352346659\n",
      "Iteration:  2958 loss:  0.004642031155526638\n",
      "Iteration:  2959 loss:  0.004538161214441061\n",
      "Iteration:  2960 loss:  0.005624914541840553\n",
      "Iteration:  2961 loss:  0.005061494652181864\n",
      "Iteration:  2962 loss:  0.0050501637160778046\n",
      "Iteration:  2963 loss:  0.004732037428766489\n",
      "Iteration:  2964 loss:  0.0050430865958333015\n",
      "Iteration:  2965 loss:  0.004698523320257664\n",
      "Iteration:  2966 loss:  0.0043899668380618095\n",
      "Iteration:  2967 loss:  0.004840509966015816\n",
      "Iteration:  2968 loss:  0.004368065390735865\n",
      "Iteration:  2969 loss:  0.005539352539926767\n",
      "Iteration:  2970 loss:  0.004670629743486643\n",
      "Iteration:  2971 loss:  0.004579974338412285\n",
      "Iteration:  2972 loss:  0.004588332027196884\n",
      "Iteration:  2973 loss:  0.004184659570455551\n",
      "Iteration:  2974 loss:  0.00470468495041132\n",
      "Iteration:  2975 loss:  0.004853517282754183\n",
      "Iteration:  2976 loss:  0.004644438624382019\n",
      "Iteration:  2977 loss:  0.00457721296697855\n",
      "Iteration:  2978 loss:  0.004730314947664738\n",
      "Iteration:  2979 loss:  0.005135650280863047\n",
      "Iteration:  2980 loss:  0.004179648123681545\n",
      "Iteration:  2981 loss:  0.005262386053800583\n",
      "Iteration:  2982 loss:  0.0050246804021298885\n",
      "Iteration:  2983 loss:  0.004665113054215908\n",
      "Iteration:  2984 loss:  0.005065545905381441\n",
      "Iteration:  2985 loss:  0.004333593416959047\n",
      "Iteration:  2986 loss:  0.004516106564551592\n",
      "Iteration:  2987 loss:  0.005114017520099878\n",
      "Iteration:  2988 loss:  0.005168414209038019\n",
      "Iteration:  2989 loss:  0.004945958033204079\n",
      "Iteration:  2990 loss:  0.005110589787364006\n",
      "Iteration:  2991 loss:  0.005045455414801836\n",
      "Iteration:  2992 loss:  0.004294629208743572\n",
      "Iteration:  2993 loss:  0.004802413750439882\n",
      "Iteration:  2994 loss:  0.004197709262371063\n",
      "Iteration:  2995 loss:  0.004624352790415287\n",
      "Iteration:  2996 loss:  0.004973865579813719\n",
      "Iteration:  2997 loss:  0.006032933015376329\n",
      "Iteration:  2998 loss:  0.004363367334008217\n",
      "Iteration:  2999 loss:  0.005346108693629503\n",
      "Iteration:  3000 loss:  0.005027909763157368\n",
      "Iteration:  3001 loss:  0.00795281957834959\n",
      "Iteration:  3002 loss:  0.007553564850240946\n",
      "Iteration:  3003 loss:  0.008829005993902683\n",
      "Iteration:  3004 loss:  0.00799185037612915\n",
      "Iteration:  3005 loss:  0.008801694959402084\n",
      "Iteration:  3006 loss:  0.00747892539948225\n",
      "Iteration:  3007 loss:  0.008577045053243637\n",
      "Iteration:  3008 loss:  0.007628236897289753\n",
      "Iteration:  3009 loss:  0.008133942261338234\n",
      "Iteration:  3010 loss:  0.00580052612349391\n",
      "Iteration:  3011 loss:  0.005628365091979504\n",
      "Iteration:  3012 loss:  0.005690571386367083\n",
      "Iteration:  3013 loss:  0.005716345272958279\n",
      "Iteration:  3014 loss:  0.005432722624391317\n",
      "Iteration:  3015 loss:  0.005460391286760569\n",
      "Iteration:  3016 loss:  0.005542101804167032\n",
      "Iteration:  3017 loss:  0.005551236215978861\n",
      "Iteration:  3018 loss:  0.006805101875215769\n",
      "Iteration:  3019 loss:  0.009999140165746212\n",
      "Iteration:  3020 loss:  0.010015178471803665\n",
      "Iteration:  3021 loss:  0.00925291795283556\n",
      "Iteration:  3022 loss:  0.008907146751880646\n",
      "Iteration:  3023 loss:  0.008678542450070381\n",
      "Iteration:  3024 loss:  0.008210328407585621\n",
      "Iteration:  3025 loss:  0.008889546617865562\n",
      "Iteration:  3026 loss:  0.00803509820252657\n",
      "Iteration:  3027 loss:  0.005809135735034943\n",
      "Iteration:  3028 loss:  0.004630538169294596\n",
      "Iteration:  3029 loss:  0.004702998790889978\n",
      "Iteration:  3030 loss:  0.005179145373404026\n",
      "Iteration:  3031 loss:  0.0052818479016423225\n",
      "Iteration:  3032 loss:  0.0054290140978991985\n",
      "Iteration:  3033 loss:  0.005141386762261391\n",
      "Iteration:  3034 loss:  0.0046152276918292046\n",
      "Iteration:  3035 loss:  0.005549916531890631\n",
      "Iteration:  3036 loss:  0.006473835092037916\n",
      "Iteration:  3037 loss:  0.008364836685359478\n",
      "Iteration:  3038 loss:  0.008436801843345165\n",
      "Iteration:  3039 loss:  0.006913581397384405\n",
      "Iteration:  3040 loss:  0.0073210992850363255\n",
      "Iteration:  3041 loss:  0.006777173839509487\n",
      "Iteration:  3042 loss:  0.006810336373746395\n",
      "Iteration:  3043 loss:  0.0070947641506791115\n",
      "Iteration:  3044 loss:  0.007250606548041105\n",
      "Iteration:  3045 loss:  0.005251035094261169\n",
      "Iteration:  3046 loss:  0.006401076912879944\n",
      "Iteration:  3047 loss:  0.005277921445667744\n",
      "Iteration:  3048 loss:  0.003915173001587391\n",
      "Iteration:  3049 loss:  0.0046664015389978886\n",
      "Iteration:  3050 loss:  0.004747895058244467\n",
      "Iteration:  3051 loss:  0.0047872187569737434\n",
      "Iteration:  3052 loss:  0.004548903554677963\n",
      "Iteration:  3053 loss:  0.004230717197060585\n",
      "Iteration:  3054 loss:  0.004650720860809088\n",
      "Iteration:  3055 loss:  0.004297530744224787\n",
      "Iteration:  3056 loss:  0.004373267292976379\n",
      "Iteration:  3057 loss:  0.005919568706303835\n",
      "Iteration:  3058 loss:  0.004611631855368614\n",
      "Iteration:  3059 loss:  0.004431251436471939\n",
      "Iteration:  3060 loss:  0.00470447214320302\n",
      "Iteration:  3061 loss:  0.003718198277056217\n",
      "Iteration:  3062 loss:  0.007445705123245716\n",
      "Iteration:  3063 loss:  0.009167144075036049\n",
      "Iteration:  3064 loss:  0.009207518771290779\n",
      "Iteration:  3065 loss:  0.009470685385167599\n",
      "Iteration:  3066 loss:  0.009780376218259335\n",
      "Iteration:  3067 loss:  0.00994373019784689\n",
      "Iteration:  3068 loss:  0.009049678221344948\n",
      "Iteration:  3069 loss:  0.010229515843093395\n",
      "Iteration:  3070 loss:  0.009367992170155048\n",
      "Iteration:  3071 loss:  0.00581738306209445\n",
      "Iteration:  3072 loss:  0.005728751886636019\n",
      "Iteration:  3073 loss:  0.006823214702308178\n",
      "Iteration:  3074 loss:  0.0067840199917554855\n",
      "Iteration:  3075 loss:  0.0064881546422839165\n",
      "Iteration:  3076 loss:  0.005867342930287123\n",
      "Iteration:  3077 loss:  0.006674623116850853\n",
      "Iteration:  3078 loss:  0.007772373501211405\n",
      "Iteration:  3079 loss:  0.006230740807950497\n",
      "Iteration:  3080 loss:  0.004653335548937321\n",
      "Iteration:  3081 loss:  0.004971649032086134\n",
      "Iteration:  3082 loss:  0.004341272637248039\n",
      "Iteration:  3083 loss:  0.004333844408392906\n",
      "Iteration:  3084 loss:  0.0041471305303275585\n",
      "Iteration:  3085 loss:  0.004967662505805492\n",
      "Iteration:  3086 loss:  0.0045744916424155235\n",
      "Iteration:  3087 loss:  0.00570298545062542\n",
      "Iteration:  3088 loss:  0.004726100713014603\n",
      "Iteration:  3089 loss:  0.005163326393812895\n",
      "Iteration:  3090 loss:  0.004787835292518139\n",
      "Iteration:  3091 loss:  0.004158881027251482\n",
      "Iteration:  3092 loss:  0.0040272497572004795\n",
      "Iteration:  3093 loss:  0.004338704515248537\n",
      "Iteration:  3094 loss:  0.0042104097083210945\n",
      "Iteration:  3095 loss:  0.004949768073856831\n",
      "Iteration:  3096 loss:  0.004438519012182951\n",
      "Iteration:  3097 loss:  0.004946375265717506\n",
      "Iteration:  3098 loss:  0.005745687987655401\n",
      "Iteration:  3099 loss:  0.005707297474145889\n",
      "Iteration:  3100 loss:  0.0056780194863677025\n",
      "Iteration:  3101 loss:  0.005848218686878681\n",
      "Iteration:  3102 loss:  0.005498112645000219\n",
      "Iteration:  3103 loss:  0.004965661093592644\n",
      "Iteration:  3104 loss:  0.0059976717457175255\n",
      "Iteration:  3105 loss:  0.005343453027307987\n",
      "Iteration:  3106 loss:  0.022760896012187004\n",
      "Iteration:  3107 loss:  0.03071536123752594\n",
      "Iteration:  3108 loss:  0.029305696487426758\n",
      "Iteration:  3109 loss:  0.028817957267165184\n",
      "Iteration:  3110 loss:  0.026194928213953972\n",
      "Iteration:  3111 loss:  0.02492627687752247\n",
      "Iteration:  3112 loss:  0.026034057140350342\n",
      "Iteration:  3113 loss:  0.0251291636377573\n",
      "Iteration:  3114 loss:  0.025033725425601006\n",
      "Iteration:  3115 loss:  0.00804787129163742\n",
      "Iteration:  3116 loss:  0.007421471644192934\n",
      "Iteration:  3117 loss:  0.006499882321804762\n",
      "Iteration:  3118 loss:  0.0073201316408813\n",
      "Iteration:  3119 loss:  0.006248924881219864\n",
      "Iteration:  3120 loss:  0.007397021166980267\n",
      "Iteration:  3121 loss:  0.00684703653678298\n",
      "Iteration:  3122 loss:  0.0066202557645738125\n",
      "Iteration:  3123 loss:  0.006681744009256363\n",
      "Iteration:  3124 loss:  0.009931273758411407\n",
      "Iteration:  3125 loss:  0.009614983573555946\n",
      "Iteration:  3126 loss:  0.010337568819522858\n",
      "Iteration:  3127 loss:  0.00922582857310772\n",
      "Iteration:  3128 loss:  0.01040705107152462\n",
      "Iteration:  3129 loss:  0.010467628948390484\n",
      "Iteration:  3130 loss:  0.010433848015964031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3131 loss:  0.01078992709517479\n",
      "Iteration:  3132 loss:  0.009786106646060944\n",
      "Iteration:  3133 loss:  0.010047921910881996\n",
      "Iteration:  3134 loss:  0.00983346626162529\n",
      "Iteration:  3135 loss:  0.009595602750778198\n",
      "Iteration:  3136 loss:  0.009106929413974285\n",
      "Iteration:  3137 loss:  0.008896240033209324\n",
      "Iteration:  3138 loss:  0.008126487024128437\n",
      "Iteration:  3139 loss:  0.008635470643639565\n",
      "Iteration:  3140 loss:  0.007889771834015846\n",
      "Iteration:  3141 loss:  0.006138305179774761\n",
      "Iteration:  3142 loss:  0.005168926436454058\n",
      "Iteration:  3143 loss:  0.004841781686991453\n",
      "Iteration:  3144 loss:  0.005260309670120478\n",
      "Iteration:  3145 loss:  0.005638511385768652\n",
      "Iteration:  3146 loss:  0.005061160773038864\n",
      "Iteration:  3147 loss:  0.00539400614798069\n",
      "Iteration:  3148 loss:  0.005138446111232042\n",
      "Iteration:  3149 loss:  0.005045326892286539\n",
      "Iteration:  3150 loss:  0.006636523175984621\n",
      "Iteration:  3151 loss:  0.007342031691223383\n",
      "Iteration:  3152 loss:  0.006823357660323381\n",
      "Iteration:  3153 loss:  0.006042048335075378\n",
      "Iteration:  3154 loss:  0.005869471468031406\n",
      "Iteration:  3155 loss:  0.006573263555765152\n",
      "Iteration:  3156 loss:  0.006333851255476475\n",
      "Iteration:  3157 loss:  0.0054486640729010105\n",
      "Iteration:  3158 loss:  0.005695876199752092\n",
      "Iteration:  3159 loss:  0.005361104384064674\n",
      "Iteration:  3160 loss:  0.005644647404551506\n",
      "Iteration:  3161 loss:  0.005225776694715023\n",
      "Iteration:  3162 loss:  0.004834908992052078\n",
      "Iteration:  3163 loss:  0.00541662285104394\n",
      "Iteration:  3164 loss:  0.005195172969251871\n",
      "Iteration:  3165 loss:  0.005445638205856085\n",
      "Iteration:  3166 loss:  0.005218478851020336\n",
      "Iteration:  3167 loss:  0.0048147845081985\n",
      "Iteration:  3168 loss:  0.005053760018199682\n",
      "Iteration:  3169 loss:  0.0057090348564088345\n",
      "Iteration:  3170 loss:  0.005105908028781414\n",
      "Iteration:  3171 loss:  0.005197315011173487\n",
      "Iteration:  3172 loss:  0.005881687626242638\n",
      "Iteration:  3173 loss:  0.0059415786527097225\n",
      "Iteration:  3174 loss:  0.005252332892268896\n",
      "Iteration:  3175 loss:  0.005044261459261179\n",
      "Iteration:  3176 loss:  0.006908695213496685\n",
      "Iteration:  3177 loss:  0.00581843638792634\n",
      "Iteration:  3178 loss:  0.005904626101255417\n",
      "Iteration:  3179 loss:  0.006640713196247816\n",
      "Iteration:  3180 loss:  0.006173391360789537\n",
      "Iteration:  3181 loss:  0.006329856812953949\n",
      "Iteration:  3182 loss:  0.0060972473584115505\n",
      "Iteration:  3183 loss:  0.006006637588143349\n",
      "Iteration:  3184 loss:  0.006365459877997637\n",
      "Iteration:  3185 loss:  0.006524138152599335\n",
      "Iteration:  3186 loss:  0.005540136247873306\n",
      "Iteration:  3187 loss:  0.005446860566735268\n",
      "Iteration:  3188 loss:  0.0057533481158316135\n",
      "Iteration:  3189 loss:  0.00639237929135561\n",
      "Iteration:  3190 loss:  0.005325739737600088\n",
      "Iteration:  3191 loss:  0.0060578882694244385\n",
      "Iteration:  3192 loss:  0.005800839513540268\n",
      "Iteration:  3193 loss:  0.004550379700958729\n",
      "Iteration:  3194 loss:  0.004982421174645424\n",
      "Iteration:  3195 loss:  0.0057379198260605335\n",
      "Iteration:  3196 loss:  0.005287438165396452\n",
      "Iteration:  3197 loss:  0.005853853654116392\n",
      "Iteration:  3198 loss:  0.004282644484192133\n",
      "Iteration:  3199 loss:  0.005377817898988724\n",
      "Iteration:  3200 loss:  0.005433611571788788\n",
      "Iteration:  3201 loss:  0.005520631093531847\n",
      "Iteration:  3202 loss:  0.005270650610327721\n",
      "Iteration:  3203 loss:  0.005146400537341833\n",
      "Iteration:  3204 loss:  0.00531857879832387\n",
      "Iteration:  3205 loss:  0.005100794602185488\n",
      "Iteration:  3206 loss:  0.004789268132299185\n",
      "Iteration:  3207 loss:  0.005055372603237629\n",
      "Iteration:  3208 loss:  0.005456449929624796\n",
      "Iteration:  3209 loss:  0.004633389879018068\n",
      "Iteration:  3210 loss:  0.005291569512337446\n",
      "Iteration:  3211 loss:  0.00653613917529583\n",
      "Iteration:  3212 loss:  0.007525797933340073\n",
      "Iteration:  3213 loss:  0.007631178013980389\n",
      "Iteration:  3214 loss:  0.006431111134588718\n",
      "Iteration:  3215 loss:  0.007167709991335869\n",
      "Iteration:  3216 loss:  0.00885229092091322\n",
      "Iteration:  3217 loss:  0.00745878741145134\n",
      "Iteration:  3218 loss:  0.007027790416032076\n",
      "Iteration:  3219 loss:  0.006177651695907116\n",
      "Iteration:  3220 loss:  0.005796270910650492\n",
      "Iteration:  3221 loss:  0.005000520031899214\n",
      "Iteration:  3222 loss:  0.005557818803936243\n",
      "Iteration:  3223 loss:  0.005581258796155453\n",
      "Iteration:  3224 loss:  0.006121396087110043\n",
      "Iteration:  3225 loss:  0.005665491335093975\n",
      "Iteration:  3226 loss:  0.0052387588657438755\n",
      "Iteration:  3227 loss:  0.005690779536962509\n",
      "Iteration:  3228 loss:  0.006132203619927168\n",
      "Iteration:  3229 loss:  0.005973229184746742\n",
      "Iteration:  3230 loss:  0.005394388921558857\n",
      "Iteration:  3231 loss:  0.005691354628652334\n",
      "Iteration:  3232 loss:  0.005741656757891178\n",
      "Iteration:  3233 loss:  0.005137154366821051\n",
      "Iteration:  3234 loss:  0.005409647710621357\n",
      "Iteration:  3235 loss:  0.005039219278842211\n",
      "Iteration:  3236 loss:  0.004810905084013939\n",
      "Iteration:  3237 loss:  0.0050895558670163155\n",
      "Iteration:  3238 loss:  0.005578723270446062\n",
      "Iteration:  3239 loss:  0.005965135525912046\n",
      "Iteration:  3240 loss:  0.006002421025186777\n",
      "Iteration:  3241 loss:  0.005097184795886278\n",
      "Iteration:  3242 loss:  0.004747655242681503\n",
      "Iteration:  3243 loss:  0.005958743393421173\n",
      "Iteration:  3244 loss:  0.004600471816956997\n",
      "Iteration:  3245 loss:  0.004839536268264055\n",
      "Iteration:  3246 loss:  0.014842989854514599\n",
      "Iteration:  3247 loss:  0.020403379574418068\n",
      "Iteration:  3248 loss:  0.01928885467350483\n",
      "Iteration:  3249 loss:  0.01843971200287342\n",
      "Iteration:  3250 loss:  0.01824025809764862\n",
      "Iteration:  3251 loss:  0.01892981305718422\n",
      "Iteration:  3252 loss:  0.01617365889251232\n",
      "Iteration:  3253 loss:  0.01706753298640251\n",
      "Iteration:  3254 loss:  0.01605929620563984\n",
      "Iteration:  3255 loss:  0.005115805193781853\n",
      "Iteration:  3256 loss:  0.004419587552547455\n",
      "Iteration:  3257 loss:  0.0037681080866605043\n",
      "Iteration:  3258 loss:  0.004229962360113859\n",
      "Iteration:  3259 loss:  0.004464726895093918\n",
      "Iteration:  3260 loss:  0.005214971024543047\n",
      "Iteration:  3261 loss:  0.0048133390955626965\n",
      "Iteration:  3262 loss:  0.004364592954516411\n",
      "Iteration:  3263 loss:  0.004452343564480543\n",
      "Iteration:  3264 loss:  0.004754913039505482\n",
      "Iteration:  3265 loss:  0.004961679223924875\n",
      "Iteration:  3266 loss:  0.004475144669413567\n",
      "Iteration:  3267 loss:  0.0053364732302725315\n",
      "Iteration:  3268 loss:  0.005276581738144159\n",
      "Iteration:  3269 loss:  0.00456531997770071\n",
      "Iteration:  3270 loss:  0.00487471790984273\n",
      "Iteration:  3271 loss:  0.005098934751003981\n",
      "Iteration:  3272 loss:  0.005942761432379484\n",
      "Iteration:  3273 loss:  0.006197677925229073\n",
      "Iteration:  3274 loss:  0.006438965443521738\n",
      "Iteration:  3275 loss:  0.007178174331784248\n",
      "Iteration:  3276 loss:  0.007028515450656414\n",
      "Iteration:  3277 loss:  0.00554411206394434\n",
      "Iteration:  3278 loss:  0.006272835657000542\n",
      "Iteration:  3279 loss:  0.006067187059670687\n",
      "Iteration:  3280 loss:  0.006468385457992554\n",
      "Iteration:  3281 loss:  0.006792191881686449\n",
      "Iteration:  3282 loss:  0.006469362415373325\n",
      "Iteration:  3283 loss:  0.006672263145446777\n",
      "Iteration:  3284 loss:  0.006003589369356632\n",
      "Iteration:  3285 loss:  0.005245671141892672\n",
      "Iteration:  3286 loss:  0.0060507929883897305\n",
      "Iteration:  3287 loss:  0.0053712064400315285\n",
      "Iteration:  3288 loss:  0.005297282710671425\n",
      "Iteration:  3289 loss:  0.005657001864165068\n",
      "Iteration:  3290 loss:  0.005835091229528189\n",
      "Iteration:  3291 loss:  0.0058141350746154785\n",
      "Iteration:  3292 loss:  0.005290160421282053\n",
      "Iteration:  3293 loss:  0.00609098793938756\n",
      "Iteration:  3294 loss:  0.004397714510560036\n",
      "Iteration:  3295 loss:  0.005853959359228611\n",
      "Iteration:  3296 loss:  0.004983874969184399\n",
      "Iteration:  3297 loss:  0.005109373014420271\n",
      "Iteration:  3298 loss:  0.00567392585799098\n",
      "Iteration:  3299 loss:  0.00491051422432065\n",
      "Iteration:  3300 loss:  0.005304541904479265\n",
      "Iteration:  3301 loss:  0.005802670959383249\n",
      "Iteration:  3302 loss:  0.005488157272338867\n",
      "Iteration:  3303 loss:  0.005272481590509415\n",
      "Iteration:  3304 loss:  0.005641268100589514\n",
      "Iteration:  3305 loss:  0.005759826395660639\n",
      "Iteration:  3306 loss:  0.00436354847624898\n",
      "Iteration:  3307 loss:  0.005324586294591427\n",
      "Iteration:  3308 loss:  0.0052621630020439625\n",
      "Iteration:  3309 loss:  0.0051672630943357944\n",
      "Iteration:  3310 loss:  0.005310460459440947\n",
      "Iteration:  3311 loss:  0.0061797890812158585\n",
      "Iteration:  3312 loss:  0.0051290662959218025\n",
      "Iteration:  3313 loss:  0.004971842281520367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3314 loss:  0.005968790035694838\n",
      "Iteration:  3315 loss:  0.00542471231892705\n",
      "Iteration:  3316 loss:  0.005535988602787256\n",
      "Iteration:  3317 loss:  0.005772234871983528\n",
      "Iteration:  3318 loss:  0.004810864571481943\n",
      "Iteration:  3319 loss:  0.004778897389769554\n",
      "Iteration:  3320 loss:  0.00478809978812933\n",
      "Iteration:  3321 loss:  0.00487376656383276\n",
      "Iteration:  3322 loss:  0.004827091470360756\n",
      "Iteration:  3323 loss:  0.0051965066231787205\n",
      "Iteration:  3324 loss:  0.0047486769035458565\n",
      "Iteration:  3325 loss:  0.005438296124339104\n",
      "Iteration:  3326 loss:  0.0046842824667692184\n",
      "Iteration:  3327 loss:  0.0044838679023087025\n",
      "Iteration:  3328 loss:  0.00549121480435133\n",
      "Iteration:  3329 loss:  0.004393585026264191\n",
      "Iteration:  3330 loss:  0.004564512055367231\n",
      "Iteration:  3331 loss:  0.004672629293054342\n",
      "Iteration:  3332 loss:  0.004707751329988241\n",
      "Iteration:  3333 loss:  0.004501370247453451\n",
      "Iteration:  3334 loss:  0.00517228851094842\n",
      "Iteration:  3335 loss:  0.004890635143965483\n",
      "Iteration:  3336 loss:  0.005017376504838467\n",
      "Iteration:  3337 loss:  0.005382549483329058\n",
      "Iteration:  3338 loss:  0.005268994718790054\n",
      "Iteration:  3339 loss:  0.005056651309132576\n",
      "Iteration:  3340 loss:  0.004425218794494867\n",
      "Iteration:  3341 loss:  0.004696718417108059\n",
      "Iteration:  3342 loss:  0.004953557625412941\n",
      "Iteration:  3343 loss:  0.004778448026627302\n",
      "Iteration:  3344 loss:  0.005602770950645208\n",
      "Iteration:  3345 loss:  0.00436294823884964\n",
      "Iteration:  3346 loss:  0.004660308361053467\n",
      "Iteration:  3347 loss:  0.004629278089851141\n",
      "Iteration:  3348 loss:  0.004854075610637665\n",
      "Iteration:  3349 loss:  0.005437054671347141\n",
      "Iteration:  3350 loss:  0.005722451955080032\n",
      "Iteration:  3351 loss:  0.004848298616707325\n",
      "Iteration:  3352 loss:  0.005201558582484722\n",
      "Iteration:  3353 loss:  0.005095561500638723\n",
      "Iteration:  3354 loss:  0.004858958534896374\n",
      "Iteration:  3355 loss:  0.00507322559133172\n",
      "Iteration:  3356 loss:  0.0046339016407728195\n",
      "Iteration:  3357 loss:  0.0051660081371665\n",
      "Iteration:  3358 loss:  0.0042707952670753\n",
      "Iteration:  3359 loss:  0.005263181868940592\n",
      "Iteration:  3360 loss:  0.004963637795299292\n",
      "Iteration:  3361 loss:  0.005188667215406895\n",
      "Iteration:  3362 loss:  0.005260097328573465\n",
      "Iteration:  3363 loss:  0.005047715734690428\n",
      "Iteration:  3364 loss:  0.004634673707187176\n",
      "Iteration:  3365 loss:  0.004692711401730776\n",
      "Iteration:  3366 loss:  0.004099639132618904\n",
      "Iteration:  3367 loss:  0.004789952654391527\n",
      "Iteration:  3368 loss:  0.00464137177914381\n",
      "Iteration:  3369 loss:  0.0045572444796562195\n",
      "Iteration:  3370 loss:  0.0053861550986766815\n",
      "Iteration:  3371 loss:  0.0046628364361822605\n",
      "Iteration:  3372 loss:  0.005593989044427872\n",
      "Iteration:  3373 loss:  0.0045613693073391914\n",
      "Iteration:  3374 loss:  0.004758653696626425\n",
      "Iteration:  3375 loss:  0.004838333465158939\n",
      "Iteration:  3376 loss:  0.004566626623272896\n",
      "Iteration:  3377 loss:  0.005003734957426786\n",
      "Iteration:  3378 loss:  0.005353167187422514\n",
      "Iteration:  3379 loss:  0.004950220230966806\n",
      "Iteration:  3380 loss:  0.00545764435082674\n",
      "Iteration:  3381 loss:  0.004570514429360628\n",
      "Iteration:  3382 loss:  0.004772757645696402\n",
      "Iteration:  3383 loss:  0.004346439614892006\n",
      "Iteration:  3384 loss:  0.005405711010098457\n",
      "Iteration:  3385 loss:  0.005142326932400465\n",
      "Iteration:  3386 loss:  0.005254246294498444\n",
      "Iteration:  3387 loss:  0.005951565690338612\n",
      "Iteration:  3388 loss:  0.005051546264439821\n",
      "Iteration:  3389 loss:  0.005038198549300432\n",
      "Iteration:  3390 loss:  0.004621569998562336\n",
      "Iteration:  3391 loss:  0.005652190651744604\n",
      "Iteration:  3392 loss:  0.004943788051605225\n",
      "Iteration:  3393 loss:  0.004910092335194349\n",
      "Iteration:  3394 loss:  0.0046369596384465694\n",
      "Iteration:  3395 loss:  0.004945138934999704\n",
      "Iteration:  3396 loss:  0.004637758247554302\n",
      "Iteration:  3397 loss:  0.004598970990628004\n",
      "Iteration:  3398 loss:  0.005280359648168087\n",
      "Iteration:  3399 loss:  0.005039653740823269\n",
      "Iteration:  3400 loss:  0.004690506961196661\n",
      "Iteration:  3401 loss:  0.005530702881515026\n",
      "Iteration:  3402 loss:  0.004725176841020584\n",
      "Iteration:  3403 loss:  0.004583656322211027\n",
      "Iteration:  3404 loss:  0.009053589776158333\n",
      "Iteration:  3405 loss:  0.008202441036701202\n",
      "Iteration:  3406 loss:  0.009221954271197319\n",
      "Iteration:  3407 loss:  0.008978747762739658\n",
      "Iteration:  3408 loss:  0.008235079236328602\n",
      "Iteration:  3409 loss:  0.007540473248809576\n",
      "Iteration:  3410 loss:  0.008927964605391026\n",
      "Iteration:  3411 loss:  0.006738040130585432\n",
      "Iteration:  3412 loss:  0.006672543473541737\n",
      "Iteration:  3413 loss:  0.007095527835190296\n",
      "Iteration:  3414 loss:  0.006986104883253574\n",
      "Iteration:  3415 loss:  0.006289415527135134\n",
      "Iteration:  3416 loss:  0.0060145971365273\n",
      "Iteration:  3417 loss:  0.0068186004646122456\n",
      "Iteration:  3418 loss:  0.005928742699325085\n",
      "Iteration:  3419 loss:  0.005516494624316692\n",
      "Iteration:  3420 loss:  0.005890725180506706\n",
      "Iteration:  3421 loss:  0.0052381884306669235\n",
      "Iteration:  3422 loss:  0.005430856253951788\n",
      "Iteration:  3423 loss:  0.005870040040463209\n",
      "Iteration:  3424 loss:  0.005329571198672056\n",
      "Iteration:  3425 loss:  0.005058435723185539\n",
      "Iteration:  3426 loss:  0.004149488639086485\n",
      "Iteration:  3427 loss:  0.005560700781643391\n",
      "Iteration:  3428 loss:  0.005316152237355709\n",
      "Iteration:  3429 loss:  0.005454552359879017\n",
      "Iteration:  3430 loss:  0.005551162641495466\n",
      "Iteration:  3431 loss:  0.005081094801425934\n",
      "Iteration:  3432 loss:  0.004983172751963139\n",
      "Iteration:  3433 loss:  0.005065519362688065\n",
      "Iteration:  3434 loss:  0.004924780689179897\n",
      "Iteration:  3435 loss:  0.00530537823215127\n",
      "Iteration:  3436 loss:  0.00482188118621707\n",
      "Iteration:  3437 loss:  0.005034641362726688\n",
      "Iteration:  3438 loss:  0.0057608745992183685\n",
      "Iteration:  3439 loss:  0.007030270528048277\n",
      "Iteration:  3440 loss:  0.006555280648171902\n",
      "Iteration:  3441 loss:  0.007487377151846886\n",
      "Iteration:  3442 loss:  0.006277294829487801\n",
      "Iteration:  3443 loss:  0.006681847386062145\n",
      "Iteration:  3444 loss:  0.007426094729453325\n",
      "Iteration:  3445 loss:  0.007392144296318293\n",
      "Iteration:  3446 loss:  0.006975503172725439\n",
      "Iteration:  3447 loss:  0.006297146435827017\n",
      "Iteration:  3448 loss:  0.007438115309923887\n",
      "Iteration:  3449 loss:  0.006530582904815674\n",
      "Iteration:  3450 loss:  0.005465824622660875\n",
      "Iteration:  3451 loss:  0.006123440805822611\n",
      "Iteration:  3452 loss:  0.006379495840519667\n",
      "Iteration:  3453 loss:  0.006036493461579084\n",
      "Iteration:  3454 loss:  0.006280018948018551\n",
      "Iteration:  3455 loss:  0.005483725108206272\n",
      "Iteration:  3456 loss:  0.004436926916241646\n",
      "Iteration:  3457 loss:  0.004836723208427429\n",
      "Iteration:  3458 loss:  0.004949147347360849\n",
      "Iteration:  3459 loss:  0.004493367858231068\n",
      "Iteration:  3460 loss:  0.004358169622719288\n",
      "Iteration:  3461 loss:  0.004438613075762987\n",
      "Iteration:  3462 loss:  0.004330560099333525\n",
      "Iteration:  3463 loss:  0.0048195854760706425\n",
      "Iteration:  3464 loss:  0.004418198484927416\n",
      "Iteration:  3465 loss:  0.0055006118491292\n",
      "Iteration:  3466 loss:  0.004736284725368023\n",
      "Iteration:  3467 loss:  0.005656110588461161\n",
      "Iteration:  3468 loss:  0.005553719121962786\n",
      "Iteration:  3469 loss:  0.005678923800587654\n",
      "Iteration:  3470 loss:  0.005808460991829634\n",
      "Iteration:  3471 loss:  0.005404972471296787\n",
      "Iteration:  3472 loss:  0.0055507468059659\n",
      "Iteration:  3473 loss:  0.004923854488879442\n",
      "Iteration:  3474 loss:  0.004907113499939442\n",
      "Iteration:  3475 loss:  0.004576364532113075\n",
      "Iteration:  3476 loss:  0.004782413132488728\n",
      "Iteration:  3477 loss:  0.0039958166889846325\n",
      "Iteration:  3478 loss:  0.0045497966930270195\n",
      "Iteration:  3479 loss:  0.00415803724899888\n",
      "Iteration:  3480 loss:  0.00448283227160573\n",
      "Iteration:  3481 loss:  0.00441342405974865\n",
      "Iteration:  3482 loss:  0.005240414757281542\n",
      "Iteration:  3483 loss:  0.0059430599212646484\n",
      "Iteration:  3484 loss:  0.005853337701410055\n",
      "Iteration:  3485 loss:  0.005633585155010223\n",
      "Iteration:  3486 loss:  0.005800959654152393\n",
      "Iteration:  3487 loss:  0.005733046215027571\n",
      "Iteration:  3488 loss:  0.005532484035938978\n",
      "Iteration:  3489 loss:  0.005118540022522211\n",
      "Iteration:  3490 loss:  0.005784457549452782\n",
      "Iteration:  3491 loss:  0.005213760305196047\n",
      "Iteration:  3492 loss:  0.005671660415828228\n",
      "Iteration:  3493 loss:  0.004918548744171858\n",
      "Iteration:  3494 loss:  0.005719852168112993\n",
      "Iteration:  3495 loss:  0.005236927419900894\n",
      "Iteration:  3496 loss:  0.004994346294552088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3497 loss:  0.0057473559863865376\n",
      "Iteration:  3498 loss:  0.0055567240342497826\n",
      "Iteration:  3499 loss:  0.0056141093373298645\n",
      "Iteration:  3500 loss:  0.0063455430790781975\n",
      "Iteration:  3501 loss:  0.005740233696997166\n",
      "Iteration:  3502 loss:  0.0061784908175468445\n",
      "Iteration:  3503 loss:  0.0058900113217532635\n",
      "Iteration:  3504 loss:  0.005204890854656696\n",
      "Iteration:  3505 loss:  0.005430279299616814\n",
      "Iteration:  3506 loss:  0.006095652002841234\n",
      "Iteration:  3507 loss:  0.006245991215109825\n",
      "Iteration:  3508 loss:  0.005083043593913317\n",
      "Iteration:  3509 loss:  0.005934249609708786\n",
      "Iteration:  3510 loss:  0.005777365528047085\n",
      "Iteration:  3511 loss:  0.005872391629964113\n",
      "Iteration:  3512 loss:  0.005182972177863121\n",
      "Iteration:  3513 loss:  0.005758872255682945\n",
      "Iteration:  3514 loss:  0.005794839467853308\n",
      "Iteration:  3515 loss:  0.00569008057937026\n",
      "Iteration:  3516 loss:  0.005114352330565453\n",
      "Iteration:  3517 loss:  0.0054449173621833324\n",
      "Iteration:  3518 loss:  0.005260865204036236\n",
      "Iteration:  3519 loss:  0.00609660753980279\n",
      "Iteration:  3520 loss:  0.005432288162410259\n",
      "Iteration:  3521 loss:  0.006133088376373053\n",
      "Iteration:  3522 loss:  0.0051390547305345535\n",
      "Iteration:  3523 loss:  0.00460484204813838\n",
      "Iteration:  3524 loss:  0.005114895757287741\n",
      "Iteration:  3525 loss:  0.00530217494815588\n",
      "Iteration:  3526 loss:  0.005758901592344046\n",
      "Iteration:  3527 loss:  0.0057053216733038425\n",
      "Iteration:  3528 loss:  0.006004341412335634\n",
      "Iteration:  3529 loss:  0.006149468943476677\n",
      "Iteration:  3530 loss:  0.005690836347639561\n",
      "Iteration:  3531 loss:  0.006301436573266983\n",
      "Iteration:  3532 loss:  0.0062739732675254345\n",
      "Iteration:  3533 loss:  0.005487850867211819\n",
      "Iteration:  3534 loss:  0.006147576961666346\n",
      "Iteration:  3535 loss:  0.0056116608902812\n",
      "Iteration:  3536 loss:  0.005776616744697094\n",
      "Iteration:  3537 loss:  0.0056827133521437645\n",
      "Iteration:  3538 loss:  0.004953854717314243\n",
      "Iteration:  3539 loss:  0.004828190430998802\n",
      "Iteration:  3540 loss:  0.004260065034031868\n",
      "Iteration:  3541 loss:  0.005236349534243345\n",
      "Iteration:  3542 loss:  0.004912729375064373\n",
      "Iteration:  3543 loss:  0.004600345157086849\n",
      "Iteration:  3544 loss:  0.005514073185622692\n",
      "Iteration:  3545 loss:  0.005937634035944939\n",
      "Iteration:  3546 loss:  0.004661583807319403\n",
      "Iteration:  3547 loss:  0.004683635197579861\n",
      "Iteration:  3548 loss:  0.005099951755255461\n",
      "Iteration:  3549 loss:  0.0047243498265743256\n",
      "Iteration:  3550 loss:  0.004256695043295622\n",
      "Iteration:  3551 loss:  0.00469674076884985\n",
      "Iteration:  3552 loss:  0.005181015003472567\n",
      "Iteration:  3553 loss:  0.004581561777740717\n",
      "Iteration:  3554 loss:  0.005090977996587753\n",
      "Iteration:  3555 loss:  0.0049942960031330585\n",
      "Iteration:  3556 loss:  0.006250328850001097\n",
      "Iteration:  3557 loss:  0.004988684318959713\n",
      "Iteration:  3558 loss:  0.005497238598763943\n",
      "Iteration:  3559 loss:  0.004688887391239405\n",
      "Iteration:  3560 loss:  0.00572609156370163\n",
      "Iteration:  3561 loss:  0.0072446041740477085\n",
      "Iteration:  3562 loss:  0.008240778930485249\n",
      "Iteration:  3563 loss:  0.008007122203707695\n",
      "Iteration:  3564 loss:  0.008030853234231472\n",
      "Iteration:  3565 loss:  0.008389743976294994\n",
      "Iteration:  3566 loss:  0.00803475733846426\n",
      "Iteration:  3567 loss:  0.007868397980928421\n",
      "Iteration:  3568 loss:  0.007517373189330101\n",
      "Iteration:  3569 loss:  0.00725929532200098\n",
      "Iteration:  3570 loss:  0.0054918257519602776\n",
      "Iteration:  3571 loss:  0.005994431208819151\n",
      "Iteration:  3572 loss:  0.005329418461769819\n",
      "Iteration:  3573 loss:  0.005088093224912882\n",
      "Iteration:  3574 loss:  0.005275741685181856\n",
      "Iteration:  3575 loss:  0.005130695179104805\n",
      "Iteration:  3576 loss:  0.005543651524931192\n",
      "Iteration:  3577 loss:  0.005799621343612671\n",
      "Iteration:  3578 loss:  0.0055518788285553455\n",
      "Iteration:  3579 loss:  0.011266264133155346\n",
      "Iteration:  3580 loss:  0.010250717401504517\n",
      "Iteration:  3581 loss:  0.00975281186401844\n",
      "Iteration:  3582 loss:  0.010202442295849323\n",
      "Iteration:  3583 loss:  0.010762463323771954\n",
      "Iteration:  3584 loss:  0.008954539895057678\n",
      "Iteration:  3585 loss:  0.009084369987249374\n",
      "Iteration:  3586 loss:  0.008798368275165558\n",
      "Iteration:  3587 loss:  0.007291322108358145\n",
      "Iteration:  3588 loss:  0.005610626190900803\n",
      "Iteration:  3589 loss:  0.005874472204595804\n",
      "Iteration:  3590 loss:  0.00606916006654501\n",
      "Iteration:  3591 loss:  0.006056822836399078\n",
      "Iteration:  3592 loss:  0.005867938511073589\n",
      "Iteration:  3593 loss:  0.007373811677098274\n",
      "Iteration:  3594 loss:  0.006750065367668867\n",
      "Iteration:  3595 loss:  0.006690416485071182\n",
      "Iteration:  3596 loss:  0.004227620083838701\n",
      "Iteration:  3597 loss:  0.005511329043656588\n",
      "Iteration:  3598 loss:  0.004684268031269312\n",
      "Iteration:  3599 loss:  0.004959455691277981\n",
      "Iteration:  3600 loss:  0.005007616709917784\n",
      "Iteration:  3601 loss:  0.004819255322217941\n",
      "Iteration:  3602 loss:  0.0041284216567873955\n",
      "Iteration:  3603 loss:  0.004391914699226618\n",
      "Iteration:  3604 loss:  0.004373614210635424\n",
      "Iteration:  3605 loss:  0.00655037397518754\n",
      "Iteration:  3606 loss:  0.005072076339274645\n",
      "Iteration:  3607 loss:  0.006425534840673208\n",
      "Iteration:  3608 loss:  0.005843167658895254\n",
      "Iteration:  3609 loss:  0.006218541879206896\n",
      "Iteration:  3610 loss:  0.005384786985814571\n",
      "Iteration:  3611 loss:  0.005776744335889816\n",
      "Iteration:  3612 loss:  0.00603119982406497\n",
      "Iteration:  3613 loss:  0.005544696468859911\n",
      "Iteration:  3614 loss:  0.004014269448816776\n",
      "Iteration:  3615 loss:  0.004548279102891684\n",
      "Iteration:  3616 loss:  0.003954637795686722\n",
      "Iteration:  3617 loss:  0.004895584657788277\n",
      "Iteration:  3618 loss:  0.004319807048887014\n",
      "Iteration:  3619 loss:  0.003945112694054842\n",
      "Iteration:  3620 loss:  0.004152142442762852\n",
      "Iteration:  3621 loss:  0.004575967788696289\n",
      "Iteration:  3622 loss:  0.004566050134599209\n",
      "Iteration:  3623 loss:  0.005543339066207409\n",
      "Iteration:  3624 loss:  0.005724525079131126\n",
      "Iteration:  3625 loss:  0.005036639515310526\n",
      "Iteration:  3626 loss:  0.005416359286755323\n",
      "Iteration:  3627 loss:  0.005879418924450874\n",
      "Iteration:  3628 loss:  0.004668994806706905\n",
      "Iteration:  3629 loss:  0.005025157239288092\n",
      "Iteration:  3630 loss:  0.004493769723922014\n",
      "Iteration:  3631 loss:  0.009978946298360825\n",
      "Iteration:  3632 loss:  0.012098037637770176\n",
      "Iteration:  3633 loss:  0.011611293070018291\n",
      "Iteration:  3634 loss:  0.014133831486105919\n",
      "Iteration:  3635 loss:  0.012161636725068092\n",
      "Iteration:  3636 loss:  0.011902655474841595\n",
      "Iteration:  3637 loss:  0.010470418259501457\n",
      "Iteration:  3638 loss:  0.01106112077832222\n",
      "Iteration:  3639 loss:  0.012629888951778412\n",
      "Iteration:  3640 loss:  0.0041758036240935326\n",
      "Iteration:  3641 loss:  0.0048446389846503735\n",
      "Iteration:  3642 loss:  0.0047391317784786224\n",
      "Iteration:  3643 loss:  0.004822239745408297\n",
      "Iteration:  3644 loss:  0.005466247908771038\n",
      "Iteration:  3645 loss:  0.004138304851949215\n",
      "Iteration:  3646 loss:  0.004421580117195845\n",
      "Iteration:  3647 loss:  0.003894712310284376\n",
      "Iteration:  3648 loss:  0.004483453463762999\n",
      "Iteration:  3649 loss:  0.005659307818859816\n",
      "Iteration:  3650 loss:  0.005480034742504358\n",
      "Iteration:  3651 loss:  0.004501495510339737\n",
      "Iteration:  3652 loss:  0.004804857075214386\n",
      "Iteration:  3653 loss:  0.004748397506773472\n",
      "Iteration:  3654 loss:  0.005820702761411667\n",
      "Iteration:  3655 loss:  0.004589103627949953\n",
      "Iteration:  3656 loss:  0.005364431533962488\n",
      "Iteration:  3657 loss:  0.004671161528676748\n",
      "Iteration:  3658 loss:  0.004781682044267654\n",
      "Iteration:  3659 loss:  0.004767635837197304\n",
      "Iteration:  3660 loss:  0.004573735408484936\n",
      "Iteration:  3661 loss:  0.004431955050677061\n",
      "Iteration:  3662 loss:  0.004681138787418604\n",
      "Iteration:  3663 loss:  0.004241419956088066\n",
      "Iteration:  3664 loss:  0.004628995433449745\n",
      "Iteration:  3665 loss:  0.0046708909794688225\n",
      "Iteration:  3666 loss:  0.004811832215636969\n",
      "Iteration:  3667 loss:  0.004595601465553045\n",
      "Iteration:  3668 loss:  0.004638004116714001\n",
      "Iteration:  3669 loss:  0.0048267231322824955\n",
      "Iteration:  3670 loss:  0.004143365193158388\n",
      "Iteration:  3671 loss:  0.004951483570039272\n",
      "Iteration:  3672 loss:  0.00442376546561718\n",
      "Iteration:  3673 loss:  0.004792655818164349\n",
      "Iteration:  3674 loss:  0.004699353128671646\n",
      "Iteration:  3675 loss:  0.006974151358008385\n",
      "Iteration:  3676 loss:  0.006852278020232916\n",
      "Iteration:  3677 loss:  0.00529052596539259\n",
      "Iteration:  3678 loss:  0.0064031630754470825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3679 loss:  0.005715568084269762\n",
      "Iteration:  3680 loss:  0.005352885462343693\n",
      "Iteration:  3681 loss:  0.0059849414974451065\n",
      "Iteration:  3682 loss:  0.005895662121474743\n",
      "Iteration:  3683 loss:  0.006197710987180471\n",
      "Iteration:  3684 loss:  0.005324863828718662\n",
      "Iteration:  3685 loss:  0.005807656794786453\n",
      "Iteration:  3686 loss:  0.005117264576256275\n",
      "Iteration:  3687 loss:  0.005921764299273491\n",
      "Iteration:  3688 loss:  0.005632224027067423\n",
      "Iteration:  3689 loss:  0.005953144747763872\n",
      "Iteration:  3690 loss:  0.005451648961752653\n",
      "Iteration:  3691 loss:  0.005340619012713432\n",
      "Iteration:  3692 loss:  0.004742648918181658\n",
      "Iteration:  3693 loss:  0.004830523394048214\n",
      "Iteration:  3694 loss:  0.004706284496933222\n",
      "Iteration:  3695 loss:  0.003985672723501921\n",
      "Iteration:  3696 loss:  0.0049577257595956326\n",
      "Iteration:  3697 loss:  0.005012182518839836\n",
      "Iteration:  3698 loss:  0.004381835460662842\n",
      "Iteration:  3699 loss:  0.004439213778823614\n",
      "Iteration:  3700 loss:  0.005072248633950949\n",
      "Iteration:  3701 loss:  0.003851718967780471\n",
      "Iteration:  3702 loss:  0.004964265041053295\n",
      "Iteration:  3703 loss:  0.004676139447838068\n",
      "Iteration:  3704 loss:  0.004044012166559696\n",
      "Iteration:  3705 loss:  0.004573201760649681\n",
      "Iteration:  3706 loss:  0.004200488328933716\n",
      "Iteration:  3707 loss:  0.004540964029729366\n",
      "Iteration:  3708 loss:  0.004706720355898142\n",
      "Iteration:  3709 loss:  0.004136005416512489\n",
      "Iteration:  3710 loss:  0.004840563517063856\n",
      "Iteration:  3711 loss:  0.0059644379653036594\n",
      "Iteration:  3712 loss:  0.005745116621255875\n",
      "Iteration:  3713 loss:  0.005456227343529463\n",
      "Iteration:  3714 loss:  0.005602199584245682\n",
      "Iteration:  3715 loss:  0.005965434014797211\n",
      "Iteration:  3716 loss:  0.004792830906808376\n",
      "Iteration:  3717 loss:  0.005342819727957249\n",
      "Iteration:  3718 loss:  0.004671820439398289\n",
      "Iteration:  3719 loss:  0.0036686842795461416\n",
      "Iteration:  3720 loss:  0.00507727125659585\n",
      "Iteration:  3721 loss:  0.004786656703799963\n",
      "Iteration:  3722 loss:  0.003879477269947529\n",
      "Iteration:  3723 loss:  0.004694510251283646\n",
      "Iteration:  3724 loss:  0.004065768327564001\n",
      "Iteration:  3725 loss:  0.00388354086317122\n",
      "Iteration:  3726 loss:  0.004001072142273188\n",
      "Iteration:  3727 loss:  0.004513823892921209\n",
      "Iteration:  3728 loss:  0.004153062589466572\n",
      "Iteration:  3729 loss:  0.004377533681690693\n",
      "Iteration:  3730 loss:  0.004224812611937523\n",
      "Iteration:  3731 loss:  0.004119102843105793\n",
      "Iteration:  3732 loss:  0.0045094480738043785\n",
      "Iteration:  3733 loss:  0.004892014432698488\n",
      "Iteration:  3734 loss:  0.0042269486002624035\n",
      "Iteration:  3735 loss:  0.0042132120579481125\n",
      "Iteration:  3736 loss:  0.005956429522484541\n",
      "Iteration:  3737 loss:  0.006312480662018061\n",
      "Iteration:  3738 loss:  0.005575106479227543\n",
      "Iteration:  3739 loss:  0.0052341558039188385\n",
      "Iteration:  3740 loss:  0.005799885373562574\n",
      "Iteration:  3741 loss:  0.006295340601354837\n",
      "Iteration:  3742 loss:  0.005698762834072113\n",
      "Iteration:  3743 loss:  0.0050934539176523685\n",
      "Iteration:  3744 loss:  0.00493368785828352\n",
      "Iteration:  3745 loss:  0.004508592188358307\n",
      "Iteration:  3746 loss:  0.005192285869270563\n",
      "Iteration:  3747 loss:  0.004321138374507427\n",
      "Iteration:  3748 loss:  0.0043851640075445175\n",
      "Iteration:  3749 loss:  0.004349458031356335\n",
      "Iteration:  3750 loss:  0.00445829750970006\n",
      "Iteration:  3751 loss:  0.004657362587749958\n",
      "Iteration:  3752 loss:  0.0044286856427788734\n",
      "Iteration:  3753 loss:  0.004871448036283255\n",
      "Iteration:  3754 loss:  0.004062899854034185\n",
      "Iteration:  3755 loss:  0.004661009646952152\n",
      "Iteration:  3756 loss:  0.004740037955343723\n",
      "Iteration:  3757 loss:  0.0047215125523507595\n",
      "Iteration:  3758 loss:  0.004798373207449913\n",
      "Iteration:  3759 loss:  0.003998801112174988\n",
      "Iteration:  3760 loss:  0.004346419591456652\n",
      "Iteration:  3761 loss:  0.004468880593776703\n",
      "Iteration:  3762 loss:  0.0052565038204193115\n",
      "Iteration:  3763 loss:  0.00451694568619132\n",
      "Iteration:  3764 loss:  0.006074035540223122\n",
      "Iteration:  3765 loss:  0.004298142623156309\n",
      "Iteration:  3766 loss:  0.005310506094247103\n",
      "Iteration:  3767 loss:  0.005197771824896336\n",
      "Iteration:  3768 loss:  0.005590080749243498\n",
      "Iteration:  3769 loss:  0.0055461484007537365\n",
      "Iteration:  3770 loss:  0.005215080454945564\n",
      "Iteration:  3771 loss:  0.0056718518026173115\n",
      "Iteration:  3772 loss:  0.00515643460676074\n",
      "Iteration:  3773 loss:  0.005336091388016939\n",
      "Iteration:  3774 loss:  0.0054571242071688175\n",
      "Iteration:  3775 loss:  0.004610513802617788\n",
      "Iteration:  3776 loss:  0.0051774391904473305\n",
      "Iteration:  3777 loss:  0.005491467658430338\n",
      "Iteration:  3778 loss:  0.005171854980289936\n",
      "Iteration:  3779 loss:  0.005727181676775217\n",
      "Iteration:  3780 loss:  0.005011992994695902\n",
      "Iteration:  3781 loss:  0.004822541959583759\n",
      "Iteration:  3782 loss:  0.004831203259527683\n",
      "Iteration:  3783 loss:  0.005326727405190468\n",
      "Iteration:  3784 loss:  0.005787529516965151\n",
      "Iteration:  3785 loss:  0.004780844319611788\n",
      "Iteration:  3786 loss:  0.005550882313400507\n",
      "Iteration:  3787 loss:  0.004801733419299126\n",
      "Iteration:  3788 loss:  0.0047053382731974125\n",
      "Iteration:  3789 loss:  0.005168582778424025\n",
      "Iteration:  3790 loss:  0.004487466067075729\n",
      "Iteration:  3791 loss:  0.004925197921693325\n",
      "Iteration:  3792 loss:  0.005516971927136183\n",
      "Iteration:  3793 loss:  0.005284971557557583\n",
      "Iteration:  3794 loss:  0.005149702075868845\n",
      "Iteration:  3795 loss:  0.004924516659229994\n",
      "Iteration:  3796 loss:  0.004428688902407885\n",
      "Iteration:  3797 loss:  0.005530239082872868\n",
      "Iteration:  3798 loss:  0.004386671353131533\n",
      "Iteration:  3799 loss:  0.0051948376931250095\n",
      "Iteration:  3800 loss:  0.005599173717200756\n",
      "Iteration:  3801 loss:  0.005057907197624445\n",
      "Iteration:  3802 loss:  0.00428817281499505\n",
      "Iteration:  3803 loss:  0.005055529996752739\n",
      "Iteration:  3804 loss:  0.005309150088578463\n",
      "Iteration:  3805 loss:  0.004933816846460104\n",
      "Iteration:  3806 loss:  0.007851413451135159\n",
      "Iteration:  3807 loss:  0.009015903808176517\n",
      "Iteration:  3808 loss:  0.007447169627994299\n",
      "Iteration:  3809 loss:  0.009239408187568188\n",
      "Iteration:  3810 loss:  0.007789937779307365\n",
      "Iteration:  3811 loss:  0.007551278453320265\n",
      "Iteration:  3812 loss:  0.0076262494549155235\n",
      "Iteration:  3813 loss:  0.006990380119532347\n",
      "Iteration:  3814 loss:  0.007739555556327105\n",
      "Iteration:  3815 loss:  0.005938413552939892\n",
      "Iteration:  3816 loss:  0.005736816208809614\n",
      "Iteration:  3817 loss:  0.0056429775431752205\n",
      "Iteration:  3818 loss:  0.005587968975305557\n",
      "Iteration:  3819 loss:  0.005208151880651712\n",
      "Iteration:  3820 loss:  0.0057769836857914925\n",
      "Iteration:  3821 loss:  0.00546464417129755\n",
      "Iteration:  3822 loss:  0.005052623804658651\n",
      "Iteration:  3823 loss:  0.0051353382878005505\n",
      "Iteration:  3824 loss:  0.005814271979033947\n",
      "Iteration:  3825 loss:  0.005320719443261623\n",
      "Iteration:  3826 loss:  0.0050902641378343105\n",
      "Iteration:  3827 loss:  0.00537343742325902\n",
      "Iteration:  3828 loss:  0.005233862902969122\n",
      "Iteration:  3829 loss:  0.004405774176120758\n",
      "Iteration:  3830 loss:  0.004879929590970278\n",
      "Iteration:  3831 loss:  0.004895864054560661\n",
      "Iteration:  3832 loss:  0.006189421750605106\n",
      "Iteration:  3833 loss:  0.005925361067056656\n",
      "Iteration:  3834 loss:  0.006920528132468462\n",
      "Iteration:  3835 loss:  0.006399998441338539\n",
      "Iteration:  3836 loss:  0.005550153087824583\n",
      "Iteration:  3837 loss:  0.006342354230582714\n",
      "Iteration:  3838 loss:  0.006115354131907225\n",
      "Iteration:  3839 loss:  0.006285069510340691\n",
      "Iteration:  3840 loss:  0.006819566711783409\n",
      "Iteration:  3841 loss:  0.0052881366573274136\n",
      "Iteration:  3842 loss:  0.005164321977645159\n",
      "Iteration:  3843 loss:  0.005513159092515707\n",
      "Iteration:  3844 loss:  0.004689600318670273\n",
      "Iteration:  3845 loss:  0.00417129835113883\n",
      "Iteration:  3846 loss:  0.005330324172973633\n",
      "Iteration:  3847 loss:  0.004747000057250261\n",
      "Iteration:  3848 loss:  0.005122114904224873\n",
      "Iteration:  3849 loss:  0.005403852555900812\n",
      "Iteration:  3850 loss:  0.023250175639986992\n",
      "Iteration:  3851 loss:  0.024850955232977867\n",
      "Iteration:  3852 loss:  0.026936596259474754\n",
      "Iteration:  3853 loss:  0.022958829998970032\n",
      "Iteration:  3854 loss:  0.025671236217021942\n",
      "Iteration:  3855 loss:  0.023363137617707253\n",
      "Iteration:  3856 loss:  0.02171802520751953\n",
      "Iteration:  3857 loss:  0.02072846330702305\n",
      "Iteration:  3858 loss:  0.016360759735107422\n",
      "Iteration:  3859 loss:  0.004667511209845543\n",
      "Iteration:  3860 loss:  0.004690530244261026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3861 loss:  0.004772251006215811\n",
      "Iteration:  3862 loss:  0.004503186792135239\n",
      "Iteration:  3863 loss:  0.004475089721381664\n",
      "Iteration:  3864 loss:  0.00395230483263731\n",
      "Iteration:  3865 loss:  0.004677820019423962\n",
      "Iteration:  3866 loss:  0.004150402266532183\n",
      "Iteration:  3867 loss:  0.00530782388523221\n",
      "Iteration:  3868 loss:  0.006829412188380957\n",
      "Iteration:  3869 loss:  0.007763288449496031\n",
      "Iteration:  3870 loss:  0.00564995501190424\n",
      "Iteration:  3871 loss:  0.007447914220392704\n",
      "Iteration:  3872 loss:  0.007328409235924482\n",
      "Iteration:  3873 loss:  0.006544703152030706\n",
      "Iteration:  3874 loss:  0.007110266480594873\n",
      "Iteration:  3875 loss:  0.006827428471297026\n",
      "Iteration:  3876 loss:  0.007324382662773132\n",
      "Iteration:  3877 loss:  0.006767412181943655\n",
      "Iteration:  3878 loss:  0.004898101091384888\n",
      "Iteration:  3879 loss:  0.006385108456015587\n",
      "Iteration:  3880 loss:  0.0069465055130422115\n",
      "Iteration:  3881 loss:  0.0066199833527207375\n",
      "Iteration:  3882 loss:  0.006647192407399416\n",
      "Iteration:  3883 loss:  0.006630428601056337\n",
      "Iteration:  3884 loss:  0.006558304652571678\n",
      "Iteration:  3885 loss:  0.010509793646633625\n",
      "Iteration:  3886 loss:  0.009487034752964973\n",
      "Iteration:  3887 loss:  0.007579987868666649\n",
      "Iteration:  3888 loss:  0.009283456020057201\n",
      "Iteration:  3889 loss:  0.008387909270823002\n",
      "Iteration:  3890 loss:  0.009883152320981026\n",
      "Iteration:  3891 loss:  0.006929968483746052\n",
      "Iteration:  3892 loss:  0.0089449817314744\n",
      "Iteration:  3893 loss:  0.0074243624694645405\n",
      "Iteration:  3894 loss:  0.00731580751016736\n",
      "Iteration:  3895 loss:  0.007193651981651783\n",
      "Iteration:  3896 loss:  0.0075221979059278965\n",
      "Iteration:  3897 loss:  0.007248738780617714\n",
      "Iteration:  3898 loss:  0.0061604795046150684\n",
      "Iteration:  3899 loss:  0.006682150065898895\n",
      "Iteration:  3900 loss:  0.005171795375645161\n",
      "Iteration:  3901 loss:  0.005257105454802513\n",
      "Iteration:  3902 loss:  0.00512117613106966\n",
      "Iteration:  3903 loss:  0.005202169064432383\n",
      "Iteration:  3904 loss:  0.0047098309732973576\n",
      "Iteration:  3905 loss:  0.0053677307441830635\n",
      "Iteration:  3906 loss:  0.0055922940373420715\n",
      "Iteration:  3907 loss:  0.005201374646276236\n",
      "Iteration:  3908 loss:  0.005169029347598553\n",
      "Iteration:  3909 loss:  0.006059826351702213\n",
      "Iteration:  3910 loss:  0.006065927445888519\n",
      "Iteration:  3911 loss:  0.006182600744068623\n",
      "Iteration:  3912 loss:  0.006629933137446642\n",
      "Iteration:  3913 loss:  0.005842728540301323\n",
      "Iteration:  3914 loss:  0.005421624053269625\n",
      "Iteration:  3915 loss:  0.005673779174685478\n",
      "Iteration:  3916 loss:  0.006116167176514864\n",
      "Iteration:  3917 loss:  0.005237873177975416\n",
      "Iteration:  3918 loss:  0.005549141205847263\n",
      "Iteration:  3919 loss:  0.005816570948809385\n",
      "Iteration:  3920 loss:  0.005457295570522547\n",
      "Iteration:  3921 loss:  0.006212927401065826\n",
      "Iteration:  3922 loss:  0.005167664494365454\n",
      "Iteration:  3923 loss:  0.005317131523042917\n",
      "Iteration:  3924 loss:  0.005220662336796522\n",
      "Iteration:  3925 loss:  0.0046454137191176414\n",
      "Iteration:  3926 loss:  0.0048312717117369175\n",
      "Iteration:  3927 loss:  0.005418568849563599\n",
      "Iteration:  3928 loss:  0.004577405285090208\n",
      "Iteration:  3929 loss:  0.00432247668504715\n",
      "Iteration:  3930 loss:  0.005247169174253941\n",
      "Iteration:  3931 loss:  0.0041189370676875114\n",
      "Iteration:  3932 loss:  0.00438116816803813\n",
      "Iteration:  3933 loss:  0.005361920688301325\n",
      "Iteration:  3934 loss:  0.004705267958343029\n",
      "Iteration:  3935 loss:  0.005138929933309555\n",
      "Iteration:  3936 loss:  0.004541372414678335\n",
      "Iteration:  3937 loss:  0.005329569336026907\n",
      "Iteration:  3938 loss:  0.006085726898163557\n",
      "Iteration:  3939 loss:  0.0066808369010686874\n",
      "Iteration:  3940 loss:  0.00545558100566268\n",
      "Iteration:  3941 loss:  0.006165185011923313\n",
      "Iteration:  3942 loss:  0.0060401312075555325\n",
      "Iteration:  3943 loss:  0.006417450029402971\n",
      "Iteration:  3944 loss:  0.0062374453991651535\n",
      "Iteration:  3945 loss:  0.0058769648894667625\n",
      "Iteration:  3946 loss:  0.004804377909749746\n",
      "Iteration:  3947 loss:  0.004215095657855272\n",
      "Iteration:  3948 loss:  0.003937343135476112\n",
      "Iteration:  3949 loss:  0.004221086390316486\n",
      "Iteration:  3950 loss:  0.00408506253734231\n",
      "Iteration:  3951 loss:  0.004276069346815348\n",
      "Iteration:  3952 loss:  0.004168415442109108\n",
      "Iteration:  3953 loss:  0.0047691562213003635\n",
      "Iteration:  3954 loss:  0.005010720808058977\n",
      "Iteration:  3955 loss:  0.00401937635615468\n",
      "Iteration:  3956 loss:  0.004661332815885544\n",
      "Iteration:  3957 loss:  0.005181315820664167\n",
      "Iteration:  3958 loss:  0.004522270057350397\n",
      "Iteration:  3959 loss:  0.004449817817658186\n",
      "Iteration:  3960 loss:  0.004385617095977068\n",
      "Iteration:  3961 loss:  0.004739681258797646\n",
      "Iteration:  3962 loss:  0.0047060600481927395\n",
      "Iteration:  3963 loss:  0.004412971902638674\n",
      "Iteration:  3964 loss:  0.006198325660079718\n",
      "Iteration:  3965 loss:  0.005955342669039965\n",
      "Iteration:  3966 loss:  0.005329617764800787\n",
      "Iteration:  3967 loss:  0.005573027301579714\n",
      "Iteration:  3968 loss:  0.005652996711432934\n",
      "Iteration:  3969 loss:  0.005821781232953072\n",
      "Iteration:  3970 loss:  0.004592981655150652\n",
      "Iteration:  3971 loss:  0.005156143568456173\n",
      "Iteration:  3972 loss:  0.005979827139526606\n",
      "Iteration:  3973 loss:  0.004768685437738895\n",
      "Iteration:  3974 loss:  0.0051411851309239864\n",
      "Iteration:  3975 loss:  0.005301458295434713\n",
      "Iteration:  3976 loss:  0.004883438814431429\n",
      "Iteration:  3977 loss:  0.005538403522223234\n",
      "Iteration:  3978 loss:  0.005888377316296101\n",
      "Iteration:  3979 loss:  0.0052330633625388145\n",
      "Iteration:  3980 loss:  0.004962815437465906\n",
      "Iteration:  3981 loss:  0.00525555619969964\n",
      "Iteration:  3982 loss:  0.004924881272017956\n",
      "Iteration:  3983 loss:  0.004956561140716076\n",
      "Iteration:  3984 loss:  0.00467652827501297\n",
      "Iteration:  3985 loss:  0.005744433030486107\n",
      "Iteration:  3986 loss:  0.004091182257980108\n",
      "Iteration:  3987 loss:  0.004389611538499594\n",
      "Iteration:  3988 loss:  0.004698275588452816\n",
      "Iteration:  3989 loss:  0.004247464705258608\n",
      "Iteration:  3990 loss:  0.006304461974650621\n",
      "Iteration:  3991 loss:  0.006016524508595467\n",
      "Iteration:  3992 loss:  0.006041738670319319\n",
      "Iteration:  3993 loss:  0.005909027066081762\n",
      "Iteration:  3994 loss:  0.004754483234137297\n",
      "Iteration:  3995 loss:  0.005484988447278738\n",
      "Iteration:  3996 loss:  0.005229613743722439\n",
      "Iteration:  3997 loss:  0.005718596745282412\n",
      "Iteration:  3998 loss:  0.005501431878656149\n",
      "Iteration:  3999 loss:  0.004868594463914633\n",
      "Iteration:  4000 loss:  0.0042801713570952415\n",
      "Iteration:  4001 loss:  0.005410746671259403\n",
      "Iteration:  4002 loss:  0.0055543952621519566\n",
      "Iteration:  4003 loss:  0.005282255820930004\n",
      "Iteration:  4004 loss:  0.005121777765452862\n",
      "Iteration:  4005 loss:  0.004600053653120995\n",
      "Iteration:  4006 loss:  0.005949593614786863\n",
      "Iteration:  4007 loss:  0.00533582316711545\n",
      "Iteration:  4008 loss:  0.005990689154714346\n",
      "Iteration:  4009 loss:  0.005586160346865654\n",
      "Iteration:  4010 loss:  0.00542346341535449\n",
      "Iteration:  4011 loss:  0.005310581065714359\n",
      "Iteration:  4012 loss:  0.005885700229555368\n",
      "Iteration:  4013 loss:  0.005216291639953852\n",
      "Iteration:  4014 loss:  0.005831735208630562\n",
      "Iteration:  4015 loss:  0.005585420876741409\n",
      "Iteration:  4016 loss:  0.00508124241605401\n",
      "Iteration:  4017 loss:  0.005071983672678471\n",
      "Iteration:  4018 loss:  0.005524799227714539\n",
      "Iteration:  4019 loss:  0.005096231121569872\n",
      "Iteration:  4020 loss:  0.005383582785725594\n",
      "Iteration:  4021 loss:  0.005654124077409506\n",
      "Iteration:  4022 loss:  0.0060254791751503944\n",
      "Iteration:  4023 loss:  0.0056783403269946575\n",
      "Iteration:  4024 loss:  0.004744254983961582\n",
      "Iteration:  4025 loss:  0.004419272765517235\n",
      "Iteration:  4026 loss:  0.005667232442647219\n",
      "Iteration:  4027 loss:  0.005223486106842756\n",
      "Iteration:  4028 loss:  0.005913117900490761\n",
      "Iteration:  4029 loss:  0.005110159516334534\n",
      "Iteration:  4030 loss:  0.004985006060451269\n",
      "Iteration:  4031 loss:  0.0049893660470843315\n",
      "Iteration:  4032 loss:  0.004881942179054022\n",
      "Iteration:  4033 loss:  0.00542532280087471\n",
      "Iteration:  4034 loss:  0.004521806258708239\n",
      "Iteration:  4035 loss:  0.004726196173578501\n",
      "Iteration:  4036 loss:  0.004538792185485363\n",
      "Iteration:  4037 loss:  0.004580844193696976\n",
      "Iteration:  4038 loss:  0.00480295903980732\n",
      "Iteration:  4039 loss:  0.0047021047212183475\n",
      "Iteration:  4040 loss:  0.004843985196202993\n",
      "Iteration:  4041 loss:  0.004523292183876038\n",
      "Iteration:  4042 loss:  0.004985201638191938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4043 loss:  0.004542908631265163\n",
      "Iteration:  4044 loss:  0.004215562716126442\n",
      "Iteration:  4045 loss:  0.004341085907071829\n",
      "Iteration:  4046 loss:  0.0045316945761442184\n",
      "Iteration:  4047 loss:  0.004174832720309496\n",
      "Iteration:  4048 loss:  0.004677671007812023\n",
      "Iteration:  4049 loss:  0.004575848579406738\n",
      "Iteration:  4050 loss:  0.0042564840987324715\n",
      "Iteration:  4051 loss:  0.00505813118070364\n",
      "Iteration:  4052 loss:  0.004851360339671373\n",
      "Iteration:  4053 loss:  0.00493867602199316\n",
      "Iteration:  4054 loss:  0.003883349010720849\n",
      "Iteration:  4055 loss:  0.003930279985070229\n",
      "Iteration:  4056 loss:  0.00495968759059906\n",
      "Iteration:  4057 loss:  0.004057449288666248\n",
      "Iteration:  4058 loss:  0.004253176506608725\n",
      "Iteration:  4059 loss:  0.0039292918518185616\n",
      "Iteration:  4060 loss:  0.0055933608673512936\n",
      "Iteration:  4061 loss:  0.0050397105515003204\n",
      "Iteration:  4062 loss:  0.004827883560210466\n",
      "Iteration:  4063 loss:  0.004771143198013306\n",
      "Iteration:  4064 loss:  0.0054852343164384365\n",
      "Iteration:  4065 loss:  0.005784740671515465\n",
      "Iteration:  4066 loss:  0.005920466501265764\n",
      "Iteration:  4067 loss:  0.005095135420560837\n",
      "Iteration:  4068 loss:  0.005676711909472942\n",
      "Iteration:  4069 loss:  0.004819056019186974\n",
      "Iteration:  4070 loss:  0.005280055571347475\n",
      "Iteration:  4071 loss:  0.0043264892883598804\n",
      "Iteration:  4072 loss:  0.005330770742148161\n",
      "Iteration:  4073 loss:  0.0044797807931900024\n",
      "Iteration:  4074 loss:  0.004701633006334305\n",
      "Iteration:  4075 loss:  0.004876749590039253\n",
      "Iteration:  4076 loss:  0.005145299714058638\n",
      "Iteration:  4077 loss:  0.004070635419338942\n",
      "Iteration:  4078 loss:  0.004585558082908392\n",
      "Iteration:  4079 loss:  0.00448716850951314\n",
      "Iteration:  4080 loss:  0.005049510393291712\n",
      "Iteration:  4081 loss:  0.005110377445816994\n",
      "Iteration:  4082 loss:  0.004648440983146429\n",
      "Iteration:  4083 loss:  0.0056076073087751865\n",
      "Iteration:  4084 loss:  0.004722796380519867\n",
      "Iteration:  4085 loss:  0.0052133360877633095\n",
      "Iteration:  4086 loss:  0.005380334798246622\n",
      "Iteration:  4087 loss:  0.006337656639516354\n",
      "Iteration:  4088 loss:  0.005922091659158468\n",
      "Iteration:  4089 loss:  0.0061607034876942635\n",
      "Iteration:  4090 loss:  0.006237053778022528\n",
      "Iteration:  4091 loss:  0.005780505482107401\n",
      "Iteration:  4092 loss:  0.005692741367965937\n",
      "Iteration:  4093 loss:  0.005828718189150095\n",
      "Iteration:  4094 loss:  0.006381286308169365\n",
      "Iteration:  4095 loss:  0.004262000788003206\n",
      "Iteration:  4096 loss:  0.004288532305508852\n",
      "Iteration:  4097 loss:  0.0038965027779340744\n",
      "Iteration:  4098 loss:  0.004501667805016041\n",
      "Iteration:  4099 loss:  0.004284956026822329\n",
      "Iteration:  4100 loss:  0.004487429745495319\n",
      "Iteration:  4101 loss:  0.0036995839327573776\n",
      "Iteration:  4102 loss:  0.004398748744279146\n",
      "Iteration:  4103 loss:  0.004248905461281538\n",
      "Iteration:  4104 loss:  0.005403029266744852\n",
      "Iteration:  4105 loss:  0.005616192705929279\n",
      "Iteration:  4106 loss:  0.005095674190670252\n",
      "Iteration:  4107 loss:  0.005926732439547777\n",
      "Iteration:  4108 loss:  0.005339055322110653\n",
      "Iteration:  4109 loss:  0.005044067278504372\n",
      "Iteration:  4110 loss:  0.005264843814074993\n",
      "Iteration:  4111 loss:  0.004956560675054789\n",
      "Iteration:  4112 loss:  0.004437440074980259\n",
      "Iteration:  4113 loss:  0.004659409634768963\n",
      "Iteration:  4114 loss:  0.004402401857078075\n",
      "Iteration:  4115 loss:  0.004376303870230913\n",
      "Iteration:  4116 loss:  0.004219395574182272\n",
      "Iteration:  4117 loss:  0.004342372994869947\n",
      "Iteration:  4118 loss:  0.0040763732977211475\n",
      "Iteration:  4119 loss:  0.004278429783880711\n",
      "Iteration:  4120 loss:  0.0047041308134794235\n",
      "Iteration:  4121 loss:  0.005272630136460066\n",
      "Iteration:  4122 loss:  0.005124210845679045\n",
      "Iteration:  4123 loss:  0.00437761889770627\n",
      "Iteration:  4124 loss:  0.005177436862140894\n",
      "Iteration:  4125 loss:  0.005580287892371416\n",
      "Iteration:  4126 loss:  0.005121223162859678\n",
      "Iteration:  4127 loss:  0.005761196371167898\n",
      "Iteration:  4128 loss:  0.005201755091547966\n",
      "Iteration:  4129 loss:  0.00548858055844903\n",
      "Iteration:  4130 loss:  0.004030738491564989\n",
      "Iteration:  4131 loss:  0.005007757339626551\n",
      "Iteration:  4132 loss:  0.004484080243855715\n",
      "Iteration:  4133 loss:  0.005581643898040056\n",
      "Iteration:  4134 loss:  0.004894627723842859\n",
      "Iteration:  4135 loss:  0.004902590066194534\n",
      "Iteration:  4136 loss:  0.005371583625674248\n",
      "Iteration:  4137 loss:  0.004488252568989992\n",
      "Iteration:  4138 loss:  0.005401136353611946\n",
      "Iteration:  4139 loss:  0.005806198809295893\n",
      "Iteration:  4140 loss:  0.004974672105163336\n",
      "Iteration:  4141 loss:  0.005253947339951992\n",
      "Iteration:  4142 loss:  0.005292985122650862\n",
      "Iteration:  4143 loss:  0.005331144202500582\n",
      "Iteration:  4144 loss:  0.005264670588076115\n",
      "Iteration:  4145 loss:  0.005597981624305248\n",
      "Iteration:  4146 loss:  0.005205576308071613\n",
      "Iteration:  4147 loss:  0.0049733989872038364\n",
      "Iteration:  4148 loss:  0.004291174467653036\n",
      "Iteration:  4149 loss:  0.005816361866891384\n",
      "Iteration:  4150 loss:  0.004976361989974976\n",
      "Iteration:  4151 loss:  0.005071747116744518\n",
      "Iteration:  4152 loss:  0.005573640577495098\n",
      "Iteration:  4153 loss:  0.005409891717135906\n",
      "Iteration:  4154 loss:  0.004835773725062609\n",
      "Iteration:  4155 loss:  0.004282483831048012\n",
      "Iteration:  4156 loss:  0.005264179315418005\n",
      "Iteration:  4157 loss:  0.005846679676324129\n",
      "Iteration:  4158 loss:  0.004458198789507151\n",
      "Iteration:  4159 loss:  0.0052006966434419155\n",
      "Iteration:  4160 loss:  0.005277086514979601\n",
      "Iteration:  4161 loss:  0.004441495053470135\n",
      "Iteration:  4162 loss:  0.004987707827240229\n",
      "Iteration:  4163 loss:  0.004664462525397539\n",
      "Iteration:  4164 loss:  0.004876857623457909\n",
      "Iteration:  4165 loss:  0.0040029301308095455\n",
      "Iteration:  4166 loss:  0.004410761408507824\n",
      "Iteration:  4167 loss:  0.004585600923746824\n",
      "Iteration:  4168 loss:  0.004717623349279165\n",
      "Iteration:  4169 loss:  0.004314936697483063\n",
      "Iteration:  4170 loss:  0.004527405835688114\n",
      "Iteration:  4171 loss:  0.004353074356913567\n",
      "Iteration:  4172 loss:  0.004235740751028061\n",
      "Iteration:  4173 loss:  0.00514414394274354\n",
      "Iteration:  4174 loss:  0.005852325353771448\n",
      "Iteration:  4175 loss:  0.005265451967716217\n",
      "Iteration:  4176 loss:  0.00478872936218977\n",
      "Iteration:  4177 loss:  0.004756325855851173\n",
      "Iteration:  4178 loss:  0.00512143038213253\n",
      "Iteration:  4179 loss:  0.005031526554375887\n",
      "Iteration:  4180 loss:  0.005035695619881153\n",
      "Iteration:  4181 loss:  0.005454301368445158\n",
      "Iteration:  4182 loss:  0.005093129351735115\n",
      "Iteration:  4183 loss:  0.004642990417778492\n",
      "Iteration:  4184 loss:  0.005059169605374336\n",
      "Iteration:  4185 loss:  0.00520977983251214\n",
      "Iteration:  4186 loss:  0.004923761356621981\n",
      "Iteration:  4187 loss:  0.004554621875286102\n",
      "Iteration:  4188 loss:  0.00499683478847146\n",
      "Iteration:  4189 loss:  0.005226452369242907\n",
      "Iteration:  4190 loss:  0.004309193696826696\n",
      "Iteration:  4191 loss:  0.004566838499158621\n",
      "Iteration:  4192 loss:  0.004124099854379892\n",
      "Iteration:  4193 loss:  0.005603378172963858\n",
      "Iteration:  4194 loss:  0.005765732377767563\n",
      "Iteration:  4195 loss:  0.005223508924245834\n",
      "Iteration:  4196 loss:  0.0050612278282642365\n",
      "Iteration:  4197 loss:  0.005338480230420828\n",
      "Iteration:  4198 loss:  0.005447425413876772\n",
      "Iteration:  4199 loss:  0.004956732504069805\n",
      "Iteration:  4200 loss:  0.00585649348795414\n",
      "Iteration:  4201 loss:  0.006433176342397928\n",
      "Iteration:  4202 loss:  0.006407296750694513\n",
      "Iteration:  4203 loss:  0.005641703028231859\n",
      "Iteration:  4204 loss:  0.005503630265593529\n",
      "Iteration:  4205 loss:  0.005425991490483284\n",
      "Iteration:  4206 loss:  0.006244049873203039\n",
      "Iteration:  4207 loss:  0.005052600987255573\n",
      "Iteration:  4208 loss:  0.00545910419896245\n",
      "Iteration:  4209 loss:  0.005498901009559631\n",
      "Iteration:  4210 loss:  0.005025290884077549\n",
      "Iteration:  4211 loss:  0.0049723186530172825\n",
      "Iteration:  4212 loss:  0.005006521008908749\n",
      "Iteration:  4213 loss:  0.005162086337804794\n",
      "Iteration:  4214 loss:  0.004595206584781408\n",
      "Iteration:  4215 loss:  0.005345072131603956\n",
      "Iteration:  4216 loss:  0.005831997841596603\n",
      "Iteration:  4217 loss:  0.005193718243390322\n",
      "Iteration:  4218 loss:  0.005954330787062645\n",
      "Iteration:  4219 loss:  0.005707445554435253\n",
      "Iteration:  4220 loss:  0.005589944776147604\n",
      "Iteration:  4221 loss:  0.005773327313363552\n",
      "Iteration:  4222 loss:  0.0060590291395783424\n",
      "Iteration:  4223 loss:  0.00546058127656579\n",
      "Iteration:  4224 loss:  0.005427454132586718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4225 loss:  0.006055187899619341\n",
      "Iteration:  4226 loss:  0.006107772700488567\n",
      "Iteration:  4227 loss:  0.005319602321833372\n",
      "Iteration:  4228 loss:  0.0057730949483811855\n",
      "Iteration:  4229 loss:  0.004675562027841806\n",
      "Iteration:  4230 loss:  0.005017663352191448\n",
      "Iteration:  4231 loss:  0.005189391318708658\n",
      "Iteration:  4232 loss:  0.005154681857675314\n",
      "Iteration:  4233 loss:  0.005706885829567909\n",
      "Iteration:  4234 loss:  0.004632595926523209\n",
      "Iteration:  4235 loss:  0.005533288698643446\n",
      "Iteration:  4236 loss:  0.004718855954706669\n",
      "Iteration:  4237 loss:  0.004959032405167818\n",
      "Iteration:  4238 loss:  0.006189011037349701\n",
      "Iteration:  4239 loss:  0.0054322038777172565\n",
      "Iteration:  4240 loss:  0.004832196980714798\n",
      "Iteration:  4241 loss:  0.005536296404898167\n",
      "Iteration:  4242 loss:  0.004738365299999714\n",
      "Iteration:  4243 loss:  0.005298015661537647\n",
      "Iteration:  4244 loss:  0.004765352234244347\n",
      "Iteration:  4245 loss:  0.003694751299917698\n",
      "Iteration:  4246 loss:  0.004857095889747143\n",
      "Iteration:  4247 loss:  0.004230798687785864\n",
      "Iteration:  4248 loss:  0.0043328129686415195\n",
      "Iteration:  4249 loss:  0.0047013056464493275\n",
      "Iteration:  4250 loss:  0.0051999459974467754\n",
      "Iteration:  4251 loss:  0.004368667956441641\n",
      "Iteration:  4252 loss:  0.004763534292578697\n",
      "Iteration:  4253 loss:  0.004675674717873335\n",
      "Iteration:  4254 loss:  0.005585832521319389\n",
      "Iteration:  4255 loss:  0.005489958915859461\n",
      "Iteration:  4256 loss:  0.005376732442528009\n",
      "Iteration:  4257 loss:  0.004697027150541544\n",
      "Iteration:  4258 loss:  0.005355793051421642\n",
      "Iteration:  4259 loss:  0.004731909837573767\n",
      "Iteration:  4260 loss:  0.0049223932437598705\n",
      "Iteration:  4261 loss:  0.005215005949139595\n",
      "Iteration:  4262 loss:  0.004486232064664364\n",
      "Iteration:  4263 loss:  0.005354957655072212\n",
      "Iteration:  4264 loss:  0.004203821998089552\n",
      "Iteration:  4265 loss:  0.005228220485150814\n",
      "Iteration:  4266 loss:  0.005571719259023666\n",
      "Iteration:  4267 loss:  0.005868140142410994\n",
      "Iteration:  4268 loss:  0.005625506397336721\n",
      "Iteration:  4269 loss:  0.005128163378685713\n",
      "Iteration:  4270 loss:  0.005165076814591885\n",
      "Iteration:  4271 loss:  0.00516172731295228\n",
      "Iteration:  4272 loss:  0.00542299821972847\n",
      "Iteration:  4273 loss:  0.0039009570609778166\n",
      "Iteration:  4274 loss:  0.0045816972851753235\n",
      "Iteration:  4275 loss:  0.00515247555449605\n",
      "Iteration:  4276 loss:  0.003979661036282778\n",
      "Iteration:  4277 loss:  0.004265651572495699\n",
      "Iteration:  4278 loss:  0.005549231544137001\n",
      "Iteration:  4279 loss:  0.005677374079823494\n",
      "Iteration:  4280 loss:  0.0054000793024897575\n",
      "Iteration:  4281 loss:  0.004594162106513977\n",
      "Iteration:  4282 loss:  0.004356274381279945\n",
      "Iteration:  4283 loss:  0.004532984923571348\n",
      "Iteration:  4284 loss:  0.00457814522087574\n",
      "Iteration:  4285 loss:  0.004001403693109751\n",
      "Iteration:  4286 loss:  0.005006393883377314\n",
      "Iteration:  4287 loss:  0.004662735853344202\n",
      "Iteration:  4288 loss:  0.005009317770600319\n",
      "Iteration:  4289 loss:  0.005233357194811106\n",
      "Iteration:  4290 loss:  0.004519789945334196\n",
      "Iteration:  4291 loss:  0.005484648048877716\n",
      "Iteration:  4292 loss:  0.004796104039996862\n",
      "Iteration:  4293 loss:  0.005565292201936245\n",
      "Iteration:  4294 loss:  0.0052324626594781876\n",
      "Iteration:  4295 loss:  0.004946105182170868\n",
      "Iteration:  4296 loss:  0.005007185507565737\n",
      "Iteration:  4297 loss:  0.0051301149651408195\n",
      "Iteration:  4298 loss:  0.005663031712174416\n",
      "Iteration:  4299 loss:  0.004605636466294527\n",
      "Iteration:  4300 loss:  0.0047223931178450584\n",
      "Iteration:  4301 loss:  0.005681557580828667\n",
      "Iteration:  4302 loss:  0.004908804781734943\n",
      "Iteration:  4303 loss:  0.005019713658839464\n",
      "Iteration:  4304 loss:  0.004895773716270924\n",
      "Iteration:  4305 loss:  0.004593896679580212\n",
      "Iteration:  4306 loss:  0.0052041104063391685\n",
      "Iteration:  4307 loss:  0.004943473730236292\n",
      "Iteration:  4308 loss:  0.00484204338863492\n",
      "Iteration:  4309 loss:  0.004891747143119574\n",
      "Iteration:  4310 loss:  0.004527413286268711\n",
      "Iteration:  4311 loss:  0.004834440071135759\n",
      "Iteration:  4312 loss:  0.004388777539134026\n",
      "Iteration:  4313 loss:  0.004697551019489765\n",
      "Iteration:  4314 loss:  0.004680817946791649\n",
      "Iteration:  4315 loss:  0.004080873914062977\n",
      "Iteration:  4316 loss:  0.004779119975864887\n",
      "Iteration:  4317 loss:  0.004966297186911106\n",
      "Iteration:  4318 loss:  0.004340273793786764\n",
      "Iteration:  4319 loss:  0.005036619491875172\n",
      "Iteration:  4320 loss:  0.004552850034087896\n",
      "Iteration:  4321 loss:  0.004882554989308119\n",
      "Iteration:  4322 loss:  0.004518613219261169\n",
      "Iteration:  4323 loss:  0.004700715187937021\n",
      "Iteration:  4324 loss:  0.004290181212127209\n",
      "Iteration:  4325 loss:  0.0047686356119811535\n",
      "Iteration:  4326 loss:  0.0049718692898750305\n",
      "Iteration:  4327 loss:  0.0048103188164532185\n",
      "Iteration:  4328 loss:  0.004677393473684788\n",
      "Iteration:  4329 loss:  0.00474502332508564\n",
      "Iteration:  4330 loss:  0.003812462789937854\n",
      "Iteration:  4331 loss:  0.004308822564780712\n",
      "Iteration:  4332 loss:  0.004143992904573679\n",
      "Iteration:  4333 loss:  0.004706953652203083\n",
      "Iteration:  4334 loss:  0.004502695519477129\n",
      "Iteration:  4335 loss:  0.004534679464995861\n",
      "Iteration:  4336 loss:  0.0042120227590203285\n",
      "Iteration:  4337 loss:  0.004703367128968239\n",
      "Iteration:  4338 loss:  0.004304792732000351\n",
      "Iteration:  4339 loss:  0.004024831112474203\n",
      "Iteration:  4340 loss:  0.004792185500264168\n",
      "Iteration:  4341 loss:  0.004825107753276825\n",
      "Iteration:  4342 loss:  0.005310907959938049\n",
      "Iteration:  4343 loss:  0.005694075953215361\n",
      "Iteration:  4344 loss:  0.005940989125519991\n",
      "Iteration:  4345 loss:  0.005420834757387638\n",
      "Iteration:  4346 loss:  0.004964360035955906\n",
      "Iteration:  4347 loss:  0.004782607313245535\n",
      "Iteration:  4348 loss:  0.004670257680118084\n",
      "Iteration:  4349 loss:  0.004430662840604782\n",
      "Iteration:  4350 loss:  0.004432697780430317\n",
      "Iteration:  4351 loss:  0.0052173384465277195\n",
      "Iteration:  4352 loss:  0.004475586116313934\n",
      "Iteration:  4353 loss:  0.004827956203371286\n",
      "Iteration:  4354 loss:  0.005023305770009756\n",
      "Iteration:  4355 loss:  0.004438190255314112\n",
      "Iteration:  4356 loss:  0.0047967019490897655\n",
      "Iteration:  4357 loss:  0.004992066416889429\n",
      "Iteration:  4358 loss:  0.0062337531708180904\n",
      "Iteration:  4359 loss:  0.006020106840878725\n",
      "Iteration:  4360 loss:  0.00619090162217617\n",
      "Iteration:  4361 loss:  0.005346242338418961\n",
      "Iteration:  4362 loss:  0.00630914606153965\n",
      "Iteration:  4363 loss:  0.005519374273717403\n",
      "Iteration:  4364 loss:  0.005403765477240086\n",
      "Iteration:  4365 loss:  0.005974314175546169\n",
      "Iteration:  4366 loss:  0.006061809603124857\n",
      "Iteration:  4367 loss:  0.004857410676777363\n",
      "Iteration:  4368 loss:  0.005389233585447073\n",
      "Iteration:  4369 loss:  0.005063680466264486\n",
      "Iteration:  4370 loss:  0.006068048533052206\n",
      "Iteration:  4371 loss:  0.0058248103596270084\n",
      "Iteration:  4372 loss:  0.005302057601511478\n",
      "Iteration:  4373 loss:  0.004950704984366894\n",
      "Iteration:  4374 loss:  0.005541582591831684\n",
      "Iteration:  4375 loss:  0.005727061070501804\n",
      "Iteration:  4376 loss:  0.005386233329772949\n",
      "Iteration:  4377 loss:  0.005629040766507387\n",
      "Iteration:  4378 loss:  0.005998284090310335\n",
      "Iteration:  4379 loss:  0.005507465451955795\n",
      "Iteration:  4380 loss:  0.005987205076962709\n",
      "Iteration:  4381 loss:  0.004643938969820738\n",
      "Iteration:  4382 loss:  0.006013719830662012\n",
      "Iteration:  4383 loss:  0.004855890292674303\n",
      "Iteration:  4384 loss:  0.0043929573148489\n",
      "Iteration:  4385 loss:  0.0041827671229839325\n",
      "Iteration:  4386 loss:  0.004531701095402241\n",
      "Iteration:  4387 loss:  0.004484254866838455\n",
      "Iteration:  4388 loss:  0.004476040601730347\n",
      "Iteration:  4389 loss:  0.004562037531286478\n",
      "Iteration:  4390 loss:  0.004010620526969433\n",
      "Iteration:  4391 loss:  0.004360509570688009\n",
      "Iteration:  4392 loss:  0.0043770247139036655\n",
      "Iteration:  4393 loss:  0.004793268628418446\n",
      "Iteration:  4394 loss:  0.004675555042922497\n",
      "Iteration:  4395 loss:  0.004451278131455183\n",
      "Iteration:  4396 loss:  0.004897513426840305\n",
      "Iteration:  4397 loss:  0.003908826969563961\n",
      "Iteration:  4398 loss:  0.004356727469712496\n",
      "Iteration:  4399 loss:  0.004621945787221193\n",
      "Iteration:  4400 loss:  0.0042334445752203465\n",
      "Iteration:  4401 loss:  0.004714480601251125\n",
      "Iteration:  4402 loss:  0.00522630475461483\n",
      "Iteration:  4403 loss:  0.004789955448359251\n",
      "Iteration:  4404 loss:  0.00511708902195096\n",
      "Iteration:  4405 loss:  0.0050656464882195\n",
      "Iteration:  4406 loss:  0.004368075635284185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4407 loss:  0.0045592593960464\n",
      "Iteration:  4408 loss:  0.005535597912967205\n",
      "Iteration:  4409 loss:  0.005357277113944292\n",
      "Iteration:  4410 loss:  0.006319302599877119\n",
      "Iteration:  4411 loss:  0.005502045154571533\n",
      "Iteration:  4412 loss:  0.005318544805049896\n",
      "Iteration:  4413 loss:  0.006197442766278982\n",
      "Iteration:  4414 loss:  0.005883012432605028\n",
      "Iteration:  4415 loss:  0.00595033960416913\n",
      "Iteration:  4416 loss:  0.005321341101080179\n",
      "Iteration:  4417 loss:  0.006213814485818148\n",
      "Iteration:  4418 loss:  0.0062857335433363914\n",
      "Iteration:  4419 loss:  0.006679309066385031\n",
      "Iteration:  4420 loss:  0.006270224694162607\n",
      "Iteration:  4421 loss:  0.005077951587736607\n",
      "Iteration:  4422 loss:  0.005152011290192604\n",
      "Iteration:  4423 loss:  0.0048188683576881886\n",
      "Iteration:  4424 loss:  0.005540888290852308\n",
      "Iteration:  4425 loss:  0.005529033951461315\n",
      "Iteration:  4426 loss:  0.005521912127733231\n",
      "Iteration:  4427 loss:  0.005962807219475508\n",
      "Iteration:  4428 loss:  0.0054314397275447845\n",
      "Iteration:  4429 loss:  0.004891073796898127\n",
      "Iteration:  4430 loss:  0.005155001301318407\n",
      "Iteration:  4431 loss:  0.00530357426032424\n",
      "Iteration:  4432 loss:  0.004924277309328318\n",
      "Iteration:  4433 loss:  0.006236132234334946\n",
      "Iteration:  4434 loss:  0.004914905410259962\n",
      "Iteration:  4435 loss:  0.005168326199054718\n",
      "Iteration:  4436 loss:  0.004758505150675774\n",
      "Iteration:  4437 loss:  0.00544251361861825\n",
      "Iteration:  4438 loss:  0.0051596020348370075\n",
      "Iteration:  4439 loss:  0.004665075335651636\n",
      "Iteration:  4440 loss:  0.004322322551161051\n",
      "Iteration:  4441 loss:  0.005371858831495047\n",
      "Iteration:  4442 loss:  0.004129307810217142\n",
      "Iteration:  4443 loss:  0.004813740961253643\n",
      "Iteration:  4444 loss:  0.005144638009369373\n",
      "Iteration:  4445 loss:  0.005125443916767836\n",
      "Iteration:  4446 loss:  0.004943897016346455\n",
      "Iteration:  4447 loss:  0.00537114916369319\n",
      "Iteration:  4448 loss:  0.004560423083603382\n",
      "Iteration:  4449 loss:  0.005149783566594124\n",
      "Iteration:  4450 loss:  0.003906968981027603\n",
      "Iteration:  4451 loss:  0.004739423748105764\n",
      "Iteration:  4452 loss:  0.004702174570411444\n",
      "Iteration:  4453 loss:  0.004829802084714174\n",
      "Iteration:  4454 loss:  0.004750778432935476\n",
      "Iteration:  4455 loss:  0.005467637442052364\n",
      "Iteration:  4456 loss:  0.004313298966735601\n",
      "Iteration:  4457 loss:  0.0047896611504256725\n",
      "Iteration:  4458 loss:  0.004568933509290218\n",
      "Iteration:  4459 loss:  0.004254247061908245\n",
      "Iteration:  4460 loss:  0.004452978260815144\n",
      "Iteration:  4461 loss:  0.004745141137391329\n",
      "Iteration:  4462 loss:  0.004437800031155348\n",
      "Iteration:  4463 loss:  0.004617362283170223\n",
      "Iteration:  4464 loss:  0.0054548075422644615\n",
      "Iteration:  4465 loss:  0.005857693962752819\n",
      "Iteration:  4466 loss:  0.004797147586941719\n",
      "Iteration:  4467 loss:  0.005291674751788378\n",
      "Iteration:  4468 loss:  0.0051605310291051865\n",
      "Iteration:  4469 loss:  0.004950889386236668\n",
      "Iteration:  4470 loss:  0.005258454009890556\n",
      "Iteration:  4471 loss:  0.005008347798138857\n",
      "Iteration:  4472 loss:  0.003951249178498983\n",
      "Iteration:  4473 loss:  0.004506547935307026\n",
      "Iteration:  4474 loss:  0.003954285755753517\n",
      "Iteration:  4475 loss:  0.004601982422173023\n",
      "Iteration:  4476 loss:  0.00455282349139452\n",
      "Iteration:  4477 loss:  0.004706313833594322\n",
      "Iteration:  4478 loss:  0.004571783356368542\n",
      "Iteration:  4479 loss:  0.00530304666608572\n",
      "Iteration:  4480 loss:  0.004667604807764292\n",
      "Iteration:  4481 loss:  0.005599236115813255\n",
      "Iteration:  4482 loss:  0.004398280754685402\n",
      "Iteration:  4483 loss:  0.004857373423874378\n",
      "Iteration:  4484 loss:  0.0047393380664289\n",
      "Iteration:  4485 loss:  0.004514836706221104\n",
      "Iteration:  4486 loss:  0.004933176562190056\n",
      "Iteration:  4487 loss:  0.004494986031204462\n",
      "Iteration:  4488 loss:  0.005847787484526634\n",
      "Iteration:  4489 loss:  0.004997396841645241\n",
      "Iteration:  4490 loss:  0.004858062602579594\n",
      "Iteration:  4491 loss:  0.0052094850689172745\n",
      "Iteration:  4492 loss:  0.0045216139405965805\n",
      "Iteration:  4493 loss:  0.005054544657468796\n",
      "Iteration:  4494 loss:  0.00441703200340271\n",
      "Iteration:  4495 loss:  0.004897803533822298\n",
      "Iteration:  4496 loss:  0.004879355896264315\n",
      "Iteration:  4497 loss:  0.004843121860176325\n",
      "Iteration:  4498 loss:  0.00466793030500412\n",
      "Iteration:  4499 loss:  0.004356513265520334\n",
      "Iteration:  4500 loss:  0.004622749984264374\n",
      "Iteration:  4501 loss:  0.005806037224829197\n",
      "Iteration:  4502 loss:  0.0045622242614626884\n",
      "Iteration:  4503 loss:  0.004855041857808828\n",
      "Iteration:  4504 loss:  0.004418577067553997\n",
      "Iteration:  4505 loss:  0.005239774007350206\n",
      "Iteration:  4506 loss:  0.0044096531346440315\n",
      "Iteration:  4507 loss:  0.0052882772870361805\n",
      "Iteration:  4508 loss:  0.004178571980446577\n",
      "Iteration:  4509 loss:  0.0060786595568060875\n",
      "Iteration:  4510 loss:  0.005090904422104359\n",
      "Iteration:  4511 loss:  0.003966446965932846\n",
      "Iteration:  4512 loss:  0.0045515974052250385\n",
      "Iteration:  4513 loss:  0.004859159700572491\n",
      "Iteration:  4514 loss:  0.0047096689231693745\n",
      "Iteration:  4515 loss:  0.0042764125391840935\n",
      "Iteration:  4516 loss:  0.004743033554404974\n",
      "Iteration:  4517 loss:  0.004563410300761461\n",
      "Iteration:  4518 loss:  0.0038938114885240793\n",
      "Iteration:  4519 loss:  0.003943665884435177\n",
      "Iteration:  4520 loss:  0.0051233163103461266\n",
      "Iteration:  4521 loss:  0.004844374489039183\n",
      "Iteration:  4522 loss:  0.0042947810143232346\n",
      "Iteration:  4523 loss:  0.00523265590891242\n",
      "Iteration:  4524 loss:  0.00483842333778739\n",
      "Iteration:  4525 loss:  0.004984612576663494\n",
      "Iteration:  4526 loss:  0.004490980878472328\n",
      "Iteration:  4527 loss:  0.004329435061663389\n",
      "Iteration:  4528 loss:  0.004741230513900518\n",
      "Iteration:  4529 loss:  0.005045081954449415\n",
      "Iteration:  4530 loss:  0.005329046864062548\n",
      "Iteration:  4531 loss:  0.005093452986329794\n",
      "Iteration:  4532 loss:  0.004631855059415102\n",
      "Iteration:  4533 loss:  0.005342284217476845\n",
      "Iteration:  4534 loss:  0.004969827365130186\n",
      "Iteration:  4535 loss:  0.004526951815932989\n",
      "Iteration:  4536 loss:  0.005271601025015116\n",
      "Iteration:  4537 loss:  0.004808716010302305\n",
      "Iteration:  4538 loss:  0.004804364871233702\n",
      "Iteration:  4539 loss:  0.004810062237083912\n",
      "Iteration:  4540 loss:  0.004015409853309393\n",
      "Iteration:  4541 loss:  0.00721845543012023\n",
      "Iteration:  4542 loss:  0.006276303436607122\n",
      "Iteration:  4543 loss:  0.005585713777691126\n",
      "Iteration:  4544 loss:  0.006668503396213055\n",
      "Iteration:  4545 loss:  0.005401337519288063\n",
      "Iteration:  4546 loss:  0.005255136173218489\n",
      "Iteration:  4547 loss:  0.006303890608251095\n",
      "Iteration:  4548 loss:  0.006401839200407267\n",
      "Iteration:  4549 loss:  0.0053054941818118095\n",
      "Iteration:  4550 loss:  0.004752204287797213\n",
      "Iteration:  4551 loss:  0.005005008541047573\n",
      "Iteration:  4552 loss:  0.005070388782769442\n",
      "Iteration:  4553 loss:  0.004619266372174025\n",
      "Iteration:  4554 loss:  0.004548677243292332\n",
      "Iteration:  4555 loss:  0.004824640229344368\n",
      "Iteration:  4556 loss:  0.005132628604769707\n",
      "Iteration:  4557 loss:  0.004471150692552328\n",
      "Iteration:  4558 loss:  0.004979114979505539\n",
      "Iteration:  4559 loss:  0.005078175105154514\n",
      "Iteration:  4560 loss:  0.0049758548848330975\n",
      "Iteration:  4561 loss:  0.005672677885740995\n",
      "Iteration:  4562 loss:  0.004844252485781908\n",
      "Iteration:  4563 loss:  0.004589201882481575\n",
      "Iteration:  4564 loss:  0.005754783283919096\n",
      "Iteration:  4565 loss:  0.004758087452501059\n",
      "Iteration:  4566 loss:  0.005987148731946945\n",
      "Iteration:  4567 loss:  0.004898311570286751\n",
      "Iteration:  4568 loss:  0.005016634240746498\n",
      "Iteration:  4569 loss:  0.0052521186880767345\n",
      "Iteration:  4570 loss:  0.0058318632654845715\n",
      "Iteration:  4571 loss:  0.005090227350592613\n",
      "Iteration:  4572 loss:  0.0050535486079752445\n",
      "Iteration:  4573 loss:  0.004973779898136854\n",
      "Iteration:  4574 loss:  0.004298148211091757\n",
      "Iteration:  4575 loss:  0.00491015100851655\n",
      "Iteration:  4576 loss:  0.005694573745131493\n",
      "Iteration:  4577 loss:  0.00481171952560544\n",
      "Iteration:  4578 loss:  0.005798491649329662\n",
      "Iteration:  4579 loss:  0.004814946558326483\n",
      "Iteration:  4580 loss:  0.005270144436508417\n",
      "Iteration:  4581 loss:  0.004738607909530401\n",
      "Iteration:  4582 loss:  0.005257993470877409\n",
      "Iteration:  4583 loss:  0.005367668811231852\n",
      "Iteration:  4584 loss:  0.00602701073512435\n",
      "Iteration:  4585 loss:  0.004564858507364988\n",
      "Iteration:  4586 loss:  0.004008816555142403\n",
      "Iteration:  4587 loss:  0.004191506654024124\n",
      "Iteration:  4588 loss:  0.0043257735669612885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4589 loss:  0.004821478854864836\n",
      "Iteration:  4590 loss:  0.0045743971131742\n",
      "Iteration:  4591 loss:  0.004201579373329878\n",
      "Iteration:  4592 loss:  0.004815470427274704\n",
      "Iteration:  4593 loss:  0.0043905433267354965\n",
      "Iteration:  4594 loss:  0.004624311346560717\n",
      "Iteration:  4595 loss:  0.004968058317899704\n",
      "Iteration:  4596 loss:  0.004868600517511368\n",
      "Iteration:  4597 loss:  0.0049314945936203\n",
      "Iteration:  4598 loss:  0.004881641361862421\n",
      "Iteration:  4599 loss:  0.005337994080036879\n",
      "Iteration:  4600 loss:  0.004559411201626062\n",
      "Iteration:  4601 loss:  0.004526399075984955\n",
      "Iteration:  4602 loss:  0.004518681205809116\n",
      "Iteration:  4603 loss:  0.0036819246597588062\n",
      "Iteration:  4604 loss:  0.0047106449492275715\n",
      "Iteration:  4605 loss:  0.0049642049707472324\n",
      "Iteration:  4606 loss:  0.003970996476709843\n",
      "Iteration:  4607 loss:  0.004364712163805962\n",
      "Iteration:  4608 loss:  0.004090525675565004\n",
      "Iteration:  4609 loss:  0.0045296321623027325\n",
      "Iteration:  4610 loss:  0.0046492828987538815\n",
      "Iteration:  4611 loss:  0.004549321718513966\n",
      "Iteration:  4612 loss:  0.004758081398904324\n",
      "Iteration:  4613 loss:  0.005026024300605059\n",
      "Iteration:  4614 loss:  0.004584756679832935\n",
      "Iteration:  4615 loss:  0.004287922289222479\n",
      "Iteration:  4616 loss:  0.00442717457190156\n",
      "Iteration:  4617 loss:  0.0044921101070940495\n",
      "Iteration:  4618 loss:  0.004018498118966818\n",
      "Iteration:  4619 loss:  0.003990494646131992\n",
      "Iteration:  4620 loss:  0.004813024774193764\n",
      "Iteration:  4621 loss:  0.005170212592929602\n",
      "Iteration:  4622 loss:  0.005763696972280741\n",
      "Iteration:  4623 loss:  0.00539481732994318\n",
      "Iteration:  4624 loss:  0.0054755378514528275\n",
      "Iteration:  4625 loss:  0.005499984137713909\n",
      "Iteration:  4626 loss:  0.005395352374762297\n",
      "Iteration:  4627 loss:  0.005835684947669506\n",
      "Iteration:  4628 loss:  0.0047423625364899635\n",
      "Iteration:  4629 loss:  0.0054276324808597565\n",
      "Iteration:  4630 loss:  0.005670811515301466\n",
      "Iteration:  4631 loss:  0.005320305936038494\n",
      "Iteration:  4632 loss:  0.004893897566944361\n",
      "Iteration:  4633 loss:  0.005359042901545763\n",
      "Iteration:  4634 loss:  0.00573401153087616\n",
      "Iteration:  4635 loss:  0.005484800785779953\n",
      "Iteration:  4636 loss:  0.005134261213243008\n",
      "Iteration:  4637 loss:  0.004367097280919552\n",
      "Iteration:  4638 loss:  0.004539666697382927\n",
      "Iteration:  4639 loss:  0.004233427811414003\n",
      "Iteration:  4640 loss:  0.004674005322158337\n",
      "Iteration:  4641 loss:  0.004341913852840662\n",
      "Iteration:  4642 loss:  0.004859291482716799\n",
      "Iteration:  4643 loss:  0.004823973402380943\n",
      "Iteration:  4644 loss:  0.004233918618410826\n",
      "Iteration:  4645 loss:  0.0035921100061386824\n",
      "Iteration:  4646 loss:  0.0044504012912511826\n",
      "Iteration:  4647 loss:  0.0036796447820961475\n",
      "Iteration:  4648 loss:  0.004209604579955339\n",
      "Iteration:  4649 loss:  0.004003232344985008\n",
      "Iteration:  4650 loss:  0.004931086674332619\n",
      "Iteration:  4651 loss:  0.00435984181240201\n",
      "Iteration:  4652 loss:  0.004316664766520262\n",
      "Iteration:  4653 loss:  0.004479117225855589\n",
      "Iteration:  4654 loss:  0.004901951178908348\n",
      "Iteration:  4655 loss:  0.005413406528532505\n",
      "Iteration:  4656 loss:  0.005046674981713295\n",
      "Iteration:  4657 loss:  0.006268811412155628\n",
      "Iteration:  4658 loss:  0.005687512457370758\n",
      "Iteration:  4659 loss:  0.006098973099142313\n",
      "Iteration:  4660 loss:  0.0051853833720088005\n",
      "Iteration:  4661 loss:  0.0052194744348526\n",
      "Iteration:  4662 loss:  0.00478203222155571\n",
      "Iteration:  4663 loss:  0.005455951672047377\n",
      "Iteration:  4664 loss:  0.004894733428955078\n",
      "Iteration:  4665 loss:  0.0047822631895542145\n",
      "Iteration:  4666 loss:  0.004766353406012058\n",
      "Iteration:  4667 loss:  0.0053376746363937855\n",
      "Iteration:  4668 loss:  0.004679473582655191\n",
      "Iteration:  4669 loss:  0.005133054684847593\n",
      "Iteration:  4670 loss:  0.005176777485758066\n",
      "Iteration:  4671 loss:  0.0058569032698869705\n",
      "Iteration:  4672 loss:  0.0045034983195364475\n",
      "Iteration:  4673 loss:  0.004634548909962177\n",
      "Iteration:  4674 loss:  0.004517382476478815\n",
      "Iteration:  4675 loss:  0.00436363834887743\n",
      "Iteration:  4676 loss:  0.004258511587977409\n",
      "Iteration:  4677 loss:  0.004004296381026506\n",
      "Iteration:  4678 loss:  0.004435951355844736\n",
      "Iteration:  4679 loss:  0.004128209315240383\n",
      "Iteration:  4680 loss:  0.0044068810530006886\n",
      "Iteration:  4681 loss:  0.004181667231023312\n",
      "Iteration:  4682 loss:  0.005384881980717182\n",
      "Iteration:  4683 loss:  0.004970667418092489\n",
      "Iteration:  4684 loss:  0.004186847247183323\n",
      "Iteration:  4685 loss:  0.005384826101362705\n",
      "Iteration:  4686 loss:  0.005141619127243757\n",
      "Iteration:  4687 loss:  0.004635399207472801\n",
      "Iteration:  4688 loss:  0.005320318043231964\n",
      "Iteration:  4689 loss:  0.004407336935400963\n",
      "Iteration:  4690 loss:  0.005176115781068802\n",
      "Iteration:  4691 loss:  0.0046799578703939915\n",
      "Iteration:  4692 loss:  0.005255172494798899\n",
      "Iteration:  4693 loss:  0.004843810107558966\n",
      "Iteration:  4694 loss:  0.005107938777655363\n",
      "Iteration:  4695 loss:  0.0047954716719686985\n",
      "Iteration:  4696 loss:  0.004475560504943132\n",
      "Iteration:  4697 loss:  0.004357121419161558\n",
      "Iteration:  4698 loss:  0.005374533124268055\n",
      "Iteration:  4699 loss:  0.004682105965912342\n",
      "Iteration:  4700 loss:  0.00527174724265933\n",
      "Iteration:  4701 loss:  0.005392668303102255\n",
      "Iteration:  4702 loss:  0.005149373784661293\n",
      "Iteration:  4703 loss:  0.004548514261841774\n",
      "Iteration:  4704 loss:  0.005087133031338453\n",
      "Iteration:  4705 loss:  0.004815246909856796\n",
      "Iteration:  4706 loss:  0.004405570682138205\n",
      "Iteration:  4707 loss:  0.004867851734161377\n",
      "Iteration:  4708 loss:  0.004921163897961378\n",
      "Iteration:  4709 loss:  0.005337968003004789\n",
      "Iteration:  4710 loss:  0.004990421701222658\n",
      "Iteration:  4711 loss:  0.004937256686389446\n",
      "Iteration:  4712 loss:  0.00446662912145257\n",
      "Iteration:  4713 loss:  0.004938181489706039\n",
      "Iteration:  4714 loss:  0.004289260599762201\n",
      "Iteration:  4715 loss:  0.004199845250695944\n",
      "Iteration:  4716 loss:  0.005729861557483673\n",
      "Iteration:  4717 loss:  0.007586154621094465\n",
      "Iteration:  4718 loss:  0.007013409398496151\n",
      "Iteration:  4719 loss:  0.006785457953810692\n",
      "Iteration:  4720 loss:  0.0060506705194711685\n",
      "Iteration:  4721 loss:  0.007247808389365673\n",
      "Iteration:  4722 loss:  0.00608803192153573\n",
      "Iteration:  4723 loss:  0.0069105541333556175\n",
      "Iteration:  4724 loss:  0.0065643321722745895\n",
      "Iteration:  4725 loss:  0.00469869002699852\n",
      "Iteration:  4726 loss:  0.004972189199179411\n",
      "Iteration:  4727 loss:  0.003862376557663083\n",
      "Iteration:  4728 loss:  0.004359461832791567\n",
      "Iteration:  4729 loss:  0.0039481353014707565\n",
      "Iteration:  4730 loss:  0.0038906591944396496\n",
      "Iteration:  4731 loss:  0.004036933183670044\n",
      "Iteration:  4732 loss:  0.004670803900808096\n",
      "Iteration:  4733 loss:  0.004545517731457949\n",
      "Iteration:  4734 loss:  0.004890275653451681\n",
      "Iteration:  4735 loss:  0.005501379258930683\n",
      "Iteration:  4736 loss:  0.005762294866144657\n",
      "Iteration:  4737 loss:  0.005959102418273687\n",
      "Iteration:  4738 loss:  0.005461238790303469\n",
      "Iteration:  4739 loss:  0.005651201121509075\n",
      "Iteration:  4740 loss:  0.005528969224542379\n",
      "Iteration:  4741 loss:  0.00457843579351902\n",
      "Iteration:  4742 loss:  0.0060032205656170845\n",
      "Iteration:  4743 loss:  0.005746371112763882\n",
      "Iteration:  4744 loss:  0.006008600350469351\n",
      "Iteration:  4745 loss:  0.00537173030897975\n",
      "Iteration:  4746 loss:  0.004885731730610132\n",
      "Iteration:  4747 loss:  0.0055557661689817905\n",
      "Iteration:  4748 loss:  0.00490978267043829\n",
      "Iteration:  4749 loss:  0.004195903893560171\n",
      "Iteration:  4750 loss:  0.005914014298468828\n",
      "Iteration:  4751 loss:  0.005208407063037157\n",
      "Iteration:  4752 loss:  0.005146040115505457\n",
      "Iteration:  4753 loss:  0.005155159626156092\n",
      "Iteration:  4754 loss:  0.00470050098374486\n",
      "Iteration:  4755 loss:  0.004691148642450571\n",
      "Iteration:  4756 loss:  0.004902596119791269\n",
      "Iteration:  4757 loss:  0.004225386306643486\n",
      "Iteration:  4758 loss:  0.005645187571644783\n",
      "Iteration:  4759 loss:  0.005482157226651907\n",
      "Iteration:  4760 loss:  0.004506329074501991\n",
      "Iteration:  4761 loss:  0.00465575885027647\n",
      "Iteration:  4762 loss:  0.005559894256293774\n",
      "Iteration:  4763 loss:  0.004599764011800289\n",
      "Iteration:  4764 loss:  0.0050905561074614525\n",
      "Iteration:  4765 loss:  0.005164058413356543\n",
      "Iteration:  4766 loss:  0.004918810911476612\n",
      "Iteration:  4767 loss:  0.00512726092711091\n",
      "Iteration:  4768 loss:  0.004340515937656164\n",
      "Iteration:  4769 loss:  0.004430430009961128\n",
      "Iteration:  4770 loss:  0.0050206370651721954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4771 loss:  0.0050629135221242905\n",
      "Iteration:  4772 loss:  0.004385283682495356\n",
      "Iteration:  4773 loss:  0.004926000256091356\n",
      "Iteration:  4774 loss:  0.0049475980922579765\n",
      "Iteration:  4775 loss:  0.004754719324409962\n",
      "Iteration:  4776 loss:  0.0045379516668617725\n",
      "Iteration:  4777 loss:  0.005298808217048645\n",
      "Iteration:  4778 loss:  0.006793747190386057\n",
      "Iteration:  4779 loss:  0.00664155138656497\n",
      "Iteration:  4780 loss:  0.006234888453036547\n",
      "Iteration:  4781 loss:  0.007151870056986809\n",
      "Iteration:  4782 loss:  0.006856296677142382\n",
      "Iteration:  4783 loss:  0.00664988998323679\n",
      "Iteration:  4784 loss:  0.0055462149903178215\n",
      "Iteration:  4785 loss:  0.006011465564370155\n",
      "Iteration:  4786 loss:  0.004709659609943628\n",
      "Iteration:  4787 loss:  0.004668776877224445\n",
      "Iteration:  4788 loss:  0.003706202609464526\n",
      "Iteration:  4789 loss:  0.004637850448489189\n",
      "Iteration:  4790 loss:  0.004675485659390688\n",
      "Iteration:  4791 loss:  0.004149492830038071\n",
      "Iteration:  4792 loss:  0.00435960479080677\n",
      "Iteration:  4793 loss:  0.00459618866443634\n",
      "Iteration:  4794 loss:  0.004124857950955629\n",
      "Iteration:  4795 loss:  0.0049322498962283134\n",
      "Iteration:  4796 loss:  0.005145602859556675\n",
      "Iteration:  4797 loss:  0.005231703631579876\n",
      "Iteration:  4798 loss:  0.005185410846024752\n",
      "Iteration:  4799 loss:  0.004769760183990002\n",
      "Iteration:  4800 loss:  0.005784519016742706\n",
      "Iteration:  4801 loss:  0.005260838661342859\n",
      "Iteration:  4802 loss:  0.004771742504090071\n",
      "Iteration:  4803 loss:  0.004864755552262068\n",
      "Iteration:  4804 loss:  0.005165449343621731\n",
      "Iteration:  4805 loss:  0.005693585611879826\n",
      "Iteration:  4806 loss:  0.005269728135317564\n",
      "Iteration:  4807 loss:  0.004757846239954233\n",
      "Iteration:  4808 loss:  0.004372747149318457\n",
      "Iteration:  4809 loss:  0.004237612243741751\n",
      "Iteration:  4810 loss:  0.005450642667710781\n",
      "Iteration:  4811 loss:  0.004657629877328873\n",
      "Iteration:  4812 loss:  0.004338583443313837\n",
      "Iteration:  4813 loss:  0.004389173351228237\n",
      "Iteration:  4814 loss:  0.004176251124590635\n",
      "Iteration:  4815 loss:  0.004695776384323835\n",
      "Iteration:  4816 loss:  0.005387640558183193\n",
      "Iteration:  4817 loss:  0.0049035511910915375\n",
      "Iteration:  4818 loss:  0.003968978766351938\n",
      "Iteration:  4819 loss:  0.004229150712490082\n",
      "Iteration:  4820 loss:  0.004542005248367786\n",
      "Iteration:  4821 loss:  0.00513526052236557\n",
      "Iteration:  4822 loss:  0.003909193444997072\n",
      "Iteration:  4823 loss:  0.0051065608859062195\n",
      "Iteration:  4824 loss:  0.005276281386613846\n",
      "Iteration:  4825 loss:  0.0044199698604643345\n",
      "Iteration:  4826 loss:  0.0049317763186991215\n",
      "Iteration:  4827 loss:  0.004079361911863089\n",
      "Iteration:  4828 loss:  0.005469475872814655\n",
      "Iteration:  4829 loss:  0.0052244700491428375\n",
      "Iteration:  4830 loss:  0.005525610409677029\n",
      "Iteration:  4831 loss:  0.005125840660184622\n",
      "Iteration:  4832 loss:  0.005018334370106459\n",
      "Iteration:  4833 loss:  0.004946982953697443\n",
      "Iteration:  4834 loss:  0.004734445363283157\n",
      "Iteration:  4835 loss:  0.004608631134033203\n",
      "Iteration:  4836 loss:  0.004267536103725433\n",
      "Iteration:  4837 loss:  0.005145503208041191\n",
      "Iteration:  4838 loss:  0.004636919591575861\n",
      "Iteration:  4839 loss:  0.005114181898534298\n",
      "Iteration:  4840 loss:  0.0047847069799900055\n",
      "Iteration:  4841 loss:  0.0047654262743890285\n",
      "Iteration:  4842 loss:  0.0051228865049779415\n",
      "Iteration:  4843 loss:  0.0049249958246946335\n",
      "Iteration:  4844 loss:  0.004423284903168678\n",
      "Iteration:  4845 loss:  0.00504215806722641\n",
      "Iteration:  4846 loss:  0.0045938328839838505\n",
      "Iteration:  4847 loss:  0.005315697751939297\n",
      "Iteration:  4848 loss:  0.005142452195286751\n",
      "Iteration:  4849 loss:  0.004697573371231556\n",
      "Iteration:  4850 loss:  0.004969300236552954\n",
      "Iteration:  4851 loss:  0.005151113495230675\n",
      "Iteration:  4852 loss:  0.004754188004881144\n",
      "Iteration:  4853 loss:  0.004925520159304142\n",
      "Iteration:  4854 loss:  0.004057720769196749\n",
      "Iteration:  4855 loss:  0.005567407235503197\n",
      "Iteration:  4856 loss:  0.0049249860458076\n",
      "Iteration:  4857 loss:  0.004738103598356247\n",
      "Iteration:  4858 loss:  0.005752997472882271\n",
      "Iteration:  4859 loss:  0.0051223719492554665\n",
      "Iteration:  4860 loss:  0.005191890988498926\n",
      "Iteration:  4861 loss:  0.0043908110819756985\n",
      "Iteration:  4862 loss:  0.005037945229560137\n",
      "Iteration:  4863 loss:  0.004912806209176779\n",
      "Iteration:  4864 loss:  0.004825172945857048\n",
      "Iteration:  4865 loss:  0.00822825264185667\n",
      "Iteration:  4866 loss:  0.006168085616081953\n",
      "Iteration:  4867 loss:  0.00706197414547205\n",
      "Iteration:  4868 loss:  0.007214592769742012\n",
      "Iteration:  4869 loss:  0.005560251884162426\n",
      "Iteration:  4870 loss:  0.007514037657529116\n",
      "Iteration:  4871 loss:  0.006846200209110975\n",
      "Iteration:  4872 loss:  0.006979767698794603\n",
      "Iteration:  4873 loss:  0.0063348496332764626\n",
      "Iteration:  4874 loss:  0.005139994900673628\n",
      "Iteration:  4875 loss:  0.0054106381721794605\n",
      "Iteration:  4876 loss:  0.005153934005647898\n",
      "Iteration:  4877 loss:  0.004338792059570551\n",
      "Iteration:  4878 loss:  0.0047934167087078094\n",
      "Iteration:  4879 loss:  0.005143942777067423\n",
      "Iteration:  4880 loss:  0.00516207842156291\n",
      "Iteration:  4881 loss:  0.0046817767433822155\n",
      "Iteration:  4882 loss:  0.004773425869643688\n",
      "Iteration:  4883 loss:  0.00499572092667222\n",
      "Iteration:  4884 loss:  0.005009707994759083\n",
      "Iteration:  4885 loss:  0.004377252422273159\n",
      "Iteration:  4886 loss:  0.004565991461277008\n",
      "Iteration:  4887 loss:  0.004821515642106533\n",
      "Iteration:  4888 loss:  0.005505061708390713\n",
      "Iteration:  4889 loss:  0.004230157006531954\n",
      "Iteration:  4890 loss:  0.004476103000342846\n",
      "Iteration:  4891 loss:  0.005590524990111589\n",
      "Iteration:  4892 loss:  0.005895845126360655\n",
      "Iteration:  4893 loss:  0.005551518406718969\n",
      "Iteration:  4894 loss:  0.005634231958538294\n",
      "Iteration:  4895 loss:  0.00628413213416934\n",
      "Iteration:  4896 loss:  0.004988626576960087\n",
      "Iteration:  4897 loss:  0.00596737302839756\n",
      "Iteration:  4898 loss:  0.005168852861970663\n",
      "Iteration:  4899 loss:  0.004186294041574001\n",
      "\n",
      "Model test error: tf.Tensor(0.014753693249760842, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "batch_size = 128\n",
    "batches = []\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "cossim = tf.keras.losses.CosineSimilarity()\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 1e-2)\n",
    "model = MyModel()\n",
    "for graph in range(0, len(training_pairs), batch_size):\n",
    "      batches.append(training_pairs[graph:graph+batch_size])\n",
    "        \n",
    "        \n",
    "iteration = 0\n",
    "for batch in batches:      \n",
    "    losses = 0  \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        for graph_pair in batch:\n",
    "            data = transfer_to_torch(graph_pair)\n",
    "  \n",
    "            test = model(data, training=True)\n",
    "           # print(test)\n",
    "            losses += mse(data[\"target\"], test)\n",
    "            \n",
    "    grads = tape.gradient(losses, model.trainable_weights)\n",
    "    #print(model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    loss = losses.numpy()    \n",
    "    print (\"Iteration: \", iteration, \"loss: \", loss/len(batch))  \n",
    "    iteration+=1\n",
    "\n",
    "    \n",
    "test_scores = []\n",
    "test_gt_ged = load_generated_graphs(dataset, file_name='ged_matrix_test')\n",
    "for graph_1 in testing_graphs:\n",
    "    for graph_2 in training_graphs:\n",
    "        if((graph_1.graph['gid'], graph_2.graph['gid']) in test_gt_ged):\n",
    "            curr_graph_pair = {'graph_pair': [graph_1, graph_2], 'ged':test_gt_ged[(graph_1.graph['gid'], graph_2.graph['gid'])]}\n",
    "            data = transfer_to_torch(curr_graph_pair, type_specified=False)\n",
    "            prediction = model(data,training=False)\n",
    "            prediction = tf.math.exp(tf.math.reduce_sum(tf.math.log(prediction))).reshape(1, -1)\n",
    "            current_error = mse(prediction, data[\"gt_ged\"])\n",
    "            test_scores.append(current_error)\n",
    "\n",
    "\n",
    "    for graph_2 in testing_graphs:\n",
    "        if((graph_1.graph['gid'], graph_2.graph['gid']) in test_gt_ged):\n",
    "            curr_graph_pair = {'graph_pair': [graph_1, graph_2], 'ged':test_gt_ged[(graph_1.graph['gid'], graph_2.graph['gid'])]}\n",
    "            data = transfer_to_torch(curr_graph_pair, type_specified=False)\n",
    "            prediction = model(data,training=False)\n",
    "            prediction = tf.math.exp(tf.math.reduce_sum(tf.math.log(prediction))).reshape(1, -1)\n",
    "            current_error = mse(prediction, data[\"gt_ged\"])\n",
    "            test_scores.append(current_error)\n",
    "\n",
    "model_error = sum(test_scores) / len(test_scores)\n",
    "print(\"\\nModel test error: \" + str(model_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
